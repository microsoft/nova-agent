Directory structure:
â””â”€â”€ mahmoodlab-trident/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ DETAILS.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ run_batch_of_slides.py
    â”œâ”€â”€ run_single_slide.py
    â”œâ”€â”€ .readthedocs.yaml
    â”œâ”€â”€ _readme/
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ Makefile
    â”‚   â”œâ”€â”€ api.rst
    â”‚   â”œâ”€â”€ citation.rst
    â”‚   â”œâ”€â”€ conf.py
    â”‚   â”œâ”€â”€ faq.rst
    â”‚   â”œâ”€â”€ index.rst
    â”‚   â”œâ”€â”€ installation.rst
    â”‚   â”œâ”€â”€ make.bat
    â”‚   â”œâ”€â”€ quickstart.rst
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ tutorials.rst
    â”‚   â”œâ”€â”€ _static/
    â”‚   â””â”€â”€ cli_helpers/
    â”‚       â””â”€â”€ cli_generate.py
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ test_encoder_same_local_hf.py
    â”‚   â”œâ”€â”€ test_openslidewsi.py
    â”‚   â”œâ”€â”€ test_patch_encoders.py
    â”‚   â”œâ”€â”€ test_processor.py
    â”‚   â”œâ”€â”€ test_segmentation_models.py
    â”‚   â””â”€â”€ test_slide_encoders.py
    â”œâ”€â”€ trident/
    â”‚   â”œâ”€â”€ Concurrency.py
    â”‚   â”œâ”€â”€ Converter.py
    â”‚   â”œâ”€â”€ IO.py
    â”‚   â”œâ”€â”€ Maintenance.py
    â”‚   â”œâ”€â”€ Processor.py
    â”‚   â”œâ”€â”€ Visualization.py
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ patch_encoder_models/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ load.py
    â”‚   â”‚   â”œâ”€â”€ local_ckpts.json
    â”‚   â”‚   â”œâ”€â”€ model_zoo/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ conchv1_5/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ conchv1_5.py
    â”‚   â”‚   â”‚   â””â”€â”€ ctranspath/
    â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚       â””â”€â”€ ctran.py
    â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ constants.py
    â”‚   â”‚       â””â”€â”€ transform_utils.py
    â”‚   â”œâ”€â”€ segmentation_models/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ load.py
    â”‚   â”‚   â””â”€â”€ local_ckpts.json
    â”‚   â”œâ”€â”€ slide_encoder_models/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ load.py
    â”‚   â”‚   â”œâ”€â”€ local_ckpts.json
    â”‚   â”‚   â””â”€â”€ model_zoo/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â””â”€â”€ reusable_blocks/
    â”‚   â”‚           â”œâ”€â”€ ABMIL.py
    â”‚   â”‚           â””â”€â”€ __init__.py
    â”‚   â””â”€â”€ wsi_objects/
    â”‚       â”œâ”€â”€ CuCIMWSI.py
    â”‚       â”œâ”€â”€ ImageWSI.py
    â”‚       â”œâ”€â”€ OpenSlideWSI.py
    â”‚       â”œâ”€â”€ WSI.py
    â”‚       â”œâ”€â”€ WSIFactory.py
    â”‚       â”œâ”€â”€ WSIPatcher.py
    â”‚       â”œâ”€â”€ WSIPatcherDataset.py
    â”‚       â””â”€â”€ __init__.py
    â”œâ”€â”€ tutorials/
    â”‚   â”œâ”€â”€ 1-Step-by-Step-Patch-Feature-Extraction-with-Trident.ipynb
    â”‚   â”œâ”€â”€ 2-Using-Trident-With-Your-Custom-Patch-Encoder.ipynb
    â”‚   â””â”€â”€ 3-Training-a-WSI-Classification-Model-with-ABMIL-and-Heatmaps.ipynb
    â””â”€â”€ .github/
        â”œâ”€â”€ ISSUE_TEMPLATE/
        â”‚   â”œâ”€â”€ bug_report.yml
        â”‚   â”œâ”€â”€ feature_request.yml
        â”‚   â””â”€â”€ question.yml
        â””â”€â”€ workflows/
            â””â”€â”€ ci.yml

================================================
File: README.md
================================================
# ðŸ”±   Trident

 [arXiv](https://arxiv.org/pdf/2502.06750) | [Blog](https://www.linkedin.com/pulse/announcing-new-open-source-tools-accelerate-ai-pathology-andrew-zhang-loape/?trackingId=pDkifo54SRuJ2QeGiGcXpQ%3D%3D) | [Cite](https://github.com/mahmoodlab/trident?tab=readme-ov-file#reference)
 | [Documentation](https://trident-docs.readthedocs.io/en/latest/) | [License](https://github.com/mahmoodlab/trident?tab=License-1-ov-file)
 
Trident is a toolkit for large-scale whole-slide image processing.
This project was developed by the [Mahmood Lab](https://faisal.ai/) at Harvard Medical School and Brigham and Women's Hospital. This work was funded by NIH NIGMS R35GM138216.

> [!NOTE]
> Contributions are welcome! Please report any issues. You may also contribute by opening a pull request.

### Key Features:

<img align="right" src="_readme/trident_crop.jpg" width="250px" />

- **Tissue Segmentation**: Extract tissue from background (H&E, IHC, etc.).
- **Patch Extraction**: Extract tissue patches of any size and magnification.
- **Patch Feature Extraction**: Extract patch embeddings from 20+ foundation models, including [UNI](https://www.nature.com/articles/s41591-024-02857-3), [Virchow](https://www.nature.com/articles/s41591-024-03141-0), [H-Optimus-0](https://github.com/bioptimus/releases/tree/main/models/h-optimus/v0) and more...
- **Slide Feature Extraction**: Extract slide embeddings from 5+ slide foundation models, including [Threads](https://arxiv.org/abs/2501.16652) (coming soon!), [Titan](https://arxiv.org/abs/2411.19666), and [GigaPath](https://www.nature.com/articles/s41586-024-07441-w). 

### Updates:
- 05.25: New batch-wise WSI caching for scalable processing on limited SSD space + nested WSI search (`--search_nested`).
- 04.25: Native support for PIL.Image and CuCIM (use `wsi = load_wsi(xxx.svs)`). Support for seg + patch encoding without Internet.
- 04.25: Remove artifacts/penmarks from the tissue segmentation with `--remove_artifacts` and `--remove_penmarks`. 
- 02.25: New image converter from `czi`, `png`, etc to `tiff`.
- 02.25: Support for [GrandQC](https://www.nature.com/articles/s41467-024-54769-y) tissue vs. background segmentation.
- 02.25: Support for [Madeleine](https://github.com/mahmoodlab/MADELEINE/tree/main), [Hibou](https://github.com/HistAI/hibou), [Lunit](https://huggingface.co/1aurent/vit_small_patch8_224.lunit_dino), [Kaiko](https://huggingface.co/histai/hibou-L), and [H-Optimus-1](https://huggingface.co/bioptimus/H-optimus-1) models.

### ðŸ”¨ 1. **Installation**:
- Create an environment: `conda create -n "trident" python=3.10`, and activate it `conda activate trident`.
- Cloning: `git clone https://github.com/mahmoodlab/trident.git && cd trident`.
- Local installation: `pip install -e .`.

Additional packages may be required to load some pretrained models. Follow error messages for instructions.

### ðŸ”¨ 2. **Running Trident**:

**Already familiar with WSI processing?** Perform segmentation, patching, and UNI feature extraction from a directory of WSIs with:

```
python run_batch_of_slides.py --task all --wsi_dir ./wsis --job_dir ./trident_processed --patch_encoder uni_v1 --mag 20 --patch_size 256
```

**Feeling cautious?**

Run this command to perform all processing steps for a **single** slide:
```
python run_single_slide.py --slide_path ./wsis/xxxx.svs --job_dir ./trident_processed --patch_encoder uni_v1 --mag 20 --patch_size 256
```

**Or follow step-by-step instructions:**

**Step 1: Tissue Segmentation:** Segments tissue vs. background from a dir of WSIs
 - **Command**:
   ```bash
   python run_batch_of_slides.py --task seg --wsi_dir ./wsis --job_dir ./trident_processed --gpu 0 --segmenter hest
   ```
   - `--task seg`: Specifies that you want to do tissue segmentation.
   - `--wsi_dir ./wsis`: Path to dir with your WSIs.
   - `--job_dir ./trident_processed`: Output dir for processed results.
   - `--gpu 0`: Uses GPU with index 0.
   - `--segmenter`: Segmentation model. Defaults to `hest`. Switch to `grandqc` for fast H&E segmentation. Add the option `--remove_artifacts` for additional artifact clean up.
 - **Outputs**:
   - WSI thumbnails in `./trident_processed/thumbnails`.
   - WSI thumbnails with tissue contours in `./trident_processed/contours`.
   - GeoJSON files containing tissue contours in `./trident_processed/contours_geojson`. These can be opened in [QuPath](https://qupath.github.io/) for editing/quality control, if necessary.

 **Step 2: Tissue Patching:** Extracts patches from segmented tissue regions at a specific magnification.
 - **Command**:
   ```bash
   python run_batch_of_slides.py --task coords --wsi_dir ./wsis --job_dir ./trident_processed --mag 20 --patch_size 256 --overlap 0
   ```
   - `--task coords`: Specifies that you want to do patching.
   - `--wsi_dir wsis`: Path to the dir with your WSIs.
   - `--job_dir ./trident_processed`: Output dir for processed results.
   - `--mag 20`: Extracts patches at 20x magnification.
   - `--patch_size 256`: Each patch is 256x256 pixels.
   - `--overlap 0`: Patches overlap by 0 pixels (**always** an absolute number in pixels, e.g., `--overlap 128` for 50% overlap for 256x256 patches.
 - **Outputs**:
   - Patch coordinates as h5 files in `./trident_processed/20x_256px/patches`.
   - WSI thumbnails annotated with patch borders in `./trident_processed/20x_256px/visualization`.

 **Step 3a: Patch Feature Extraction:** Extracts features from tissue patches using a specified encoder
 - **Command**:
   ```bash
   python run_batch_of_slides.py --task feat --wsi_dir ./wsis --job_dir ./trident_processed --patch_encoder uni_v1 --mag 20 --patch_size 256 
   ```
   - `--task feat`: Specifies that you want to do feature extraction.
   - `--wsi_dir wsis`: Path to the dir with your WSIs.
   - `--job_dir ./trident_processed`: Output dir for processed results.
   - `--patch_encoder uni_v1`: Uses the `UNI` patch encoder. See below for list of supported models. 
   - `--mag 20`: Features are extracted from patches at 20x magnification.
   - `--patch_size 256`: Patches are 256x256 pixels in size.
 - **Outputs**: 
   - Features are saved as h5 files in `./trident_processed/20x_256px/features_uni_v1`. (Shape: `(n_patches, feature_dim)`)

Trident supports 21 patch encoders, loaded via a patch [`encoder_factory`](https://github.com/mahmoodlab/trident/blob/main/trident/patch_encoder_models/load.py#L14). Models requiring specific installations will return error messages with additional instructions. Gated models on HuggingFace require access requests.

| Patch Encoder         | Embedding Dim | Args                                                             | Link |
|-----------------------|---------------:|------------------------------------------------------------------|------|
| **UNI**               | 1024           | `--patch_encoder uni_v1 --patch_size 256 --mag 20`               | [MahmoodLab/UNI](https://huggingface.co/MahmoodLab/UNI) |
| **UNI2-h**             | 1536           | `--patch_encoder uni_v2 --patch_size 256 --mag 20`               | [MahmoodLab/UNI2-h](https://huggingface.co/MahmoodLab/UNI2-h) |
| **CONCH**             | 512            | `--patch_encoder conch_v1 --patch_size 512 --mag 20`             | [MahmoodLab/CONCH](https://huggingface.co/MahmoodLab/CONCH) |
| **CONCHv1.5**         | 768            | `--patch_encoder conch_v15 --patch_size 512 --mag 20`            | [MahmoodLab/conchv1_5](https://huggingface.co/MahmoodLab/conchv1_5) |
| **Virchow**           | 2560           | `--patch_encoder virchow --patch_size 224 --mag 20`              | [paige-ai/Virchow](https://huggingface.co/paige-ai/Virchow) |
| **Virchow2**          | 2560           | `--patch_encoder virchow2 --patch_size 224 --mag 20`             | [paige-ai/Virchow2](https://huggingface.co/paige-ai/Virchow2) |
| **Phikon**            | 768            | `--patch_encoder phikon --patch_size 224 --mag 20`               | [owkin/phikon](https://huggingface.co/owkin/phikon) |
| **Phikon-v2**         | 1024           | `--patch_encoder phikon_v2 --patch_size 224 --mag 20`            | [owkin/phikon-v2](https://huggingface.co/owkin/phikon-v2/) |
| **Prov-Gigapath**     | 1536           | `--patch_encoder gigapath --patch_size 256 --mag 20`             | [prov-gigapath](https://huggingface.co/prov-gigapath/prov-gigapath) |
| **H-Optimus-0**       | 1536           | `--patch_encoder hoptimus0 --patch_size 224 --mag 20`            | [bioptimus/H-optimus-0](https://huggingface.co/bioptimus/H-optimus-0) |
| **H-Optimus-1**       | 1536           | `--patch_encoder hoptimus1 --patch_size 224 --mag 20`            | [bioptimus/H-optimus-1](https://huggingface.co/bioptimus/H-optimus-1) |
| **MUSK**              | 1024           | `--patch_encoder musk --patch_size 384 --mag 20`                 | [xiangjx/musk](https://huggingface.co/xiangjx/musk) |
| **Midnight-12k**      | 3072           | `--patch_encoder midnight12k --patch_size 224 --mag 20`          | [kaiko-ai/midnight](https://huggingface.co/kaiko-ai/midnight) |
| **Kaiko**             | 384/768/1024   | `--patch_encoder {kaiko-vits8, kaiko-vits16, kaiko-vitb8, kaiko-vitb16, kaiko-vitl14} --patch_size 256 --mag 20` | [1aurent/kaikoai-models-66636c99d8e1e34bc6dcf795](https://huggingface.co/collections/1aurent/kaikoai-models-66636c99d8e1e34bc6dcf795) |
| **Lunit**             | 384            | `--patch_encoder lunit-vits8 --patch_size 224 --mag 20`          | [1aurent/vit_small_patch8_224.lunit_dino](https://huggingface.co/1aurent/vit_small_patch8_224.lunit_dino) |
| **Hibou**             | 1024           | `--patch_encoder hibou_l --patch_size 224 --mag 20`              | [histai/hibou-L](https://huggingface.co/histai/hibou-L) |
| **CTransPath-CHIEF**  | 768            | `--patch_encoder ctranspath --patch_size 256 --mag 10`           | â€” |
| **ResNet50**          | 1024           | `--patch_encoder resnet50 --patch_size 256 --mag 20`             | â€” |

**Step 3b: Slide Feature Extraction:** Extracts slide embeddings using a slide encoder. Will also automatically extract the right patch embeddings. 
 - **Command**:
   ```bash
   python run_batch_of_slides.py --task feat --wsi_dir ./wsis --job_dir ./trident_processed --slide_encoder titan --mag 20 --patch_size 512 
   ```
   - `--task feat`: Specifies that you want to do feature extraction.
   - `--wsi_dir wsis`: Path to the dir containing WSIs.
   - `--job_dir ./trident_processed`: Output dir for processed results.
   - `--slide_encoder titan`: Uses the `Titan` slide encoder. See below for supported models.
   - `--mag 20`: Features are extracted from patches at 20x magnification.
   - `--patch_size 512`: Patches are 512x512 pixels in size.
 - **Outputs**: 
   - Features are saved as h5 files in `./trident_processed/20x_256px/slide_features_titan`. (Shape: `(feature_dim)`)

Trident supports 5 slide encoders, loaded via a slide-level [`encoder_factory`](https://github.com/mahmoodlab/trident/blob/main/trident/slide_encoder_models/load.py#L14). Models requiring specific installations will return error messages with additional instructions. Gated models on HuggingFace require access requests.

| Slide Encoder | Patch Encoder | Args | Link |
|---------------|----------------|------|------|
| **Threads** | conch_v15 | `--slide_encoder threads --patch_size 512 --mag 20` | *(Coming Soon!)* |
| **Titan** | conch_v15 | `--slide_encoder titan --patch_size 512 --mag 20` | [MahmoodLab/TITAN](https://huggingface.co/MahmoodLab/TITAN) |
| **PRISM** | virchow | `--slide_encoder prism --patch_size 224 --mag 20` | [paige-ai/Prism](https://huggingface.co/paige-ai/Prism) |
| **CHIEF** | ctranspath | `--slide_encoder chief --patch_size 256 --mag 10` | [CHIEF](https://github.com/hms-dbmi/CHIEF) |
| **GigaPath** | gigapath | `--slide_encoder gigapath --patch_size 256 --mag 20` | [prov-gigapath](https://huggingface.co/prov-gigapath/prov-gigapath) |
| **Madeleine** | conch_v1 | `--slide_encoder madeleine --patch_size 256 --mag 10` | [MahmoodLab/madeleine](https://huggingface.co/MahmoodLab/madeleine) |

> [!NOTE]
> If your task includes multiple slides per patient, you can generate patient-level embeddings by: (1) processing each slide independently and taking their average slide embedding (late fusion) or (2) pooling all patches together and processing that as a single "pseudo-slide" (early fusion). For an implementation of both fusion strategies, please check out our sister repository [Patho-Bench](https://github.com/mahmoodlab/Patho-Bench).

Please see our [tutorials](https://github.com/mahmoodlab/trident/tree/main/tutorials) for more support as well as a [detailed readme](https://github.com/mahmoodlab/trident/blob/main/DETAILS.md) for additional features.

### ðŸ™‹ FAQ
- **Q**: How do I extract patch embeddings from legacy patch coordinates extracted with [CLAM](https://github.com/mahmoodlab/CLAM)?
   - **A**:
      ```bash
      python run_batch_of_slides.py --task feat --wsi_dir ..wsis --job_dir legacy_dir --patch_encoder uni_v1 --mag 20 --patch_size 256 --coords_dir extracted_mag20x_patch256_fp/
      ```
- **Q**: How do I keep patches corresponding to holes in the tissue?
   - **A**: In `run_batch_of_slides`, this behavior is default. Set `--remove_holes` to exclude patches on top of holes.

- **Q**: I see weird messages when building models using timm. What is happening?
   - **A**: Make sure `timm==0.9.16` is installed. `timm==1.X.X` creates issues with most models. 

- **Q**: How can I use `run_single_slide.py` and `run_batch_of_slides.py` in other repos with minimal work?
  - **A**: Make sure `trident` is installed using `pip install -e .`. Then, both scripts are exposed and can be integrated into any Python code, e.g., as

```python
import sys 
from run_single_slide import main

sys.argv = [
    "run_single_slide",
    '--slide_path', "output/wsis/394140.svs",
    "--job_dir", 'output/',
    "--mag", "20",
    "--patch_size", '256'
]

main()
```

- **Q**: I am not satisfied with the tissue vs background segmentation. What can I do?
   - **A**: Trident uses GeoJSON to store and load segmentations. This format is natively supported by [QuPath](https://qupath.github.io/). You can load the Trident segmentation into QuPath, modify it using QuPath's annotation tools, and save the updated segmentation back to GeoJSON.
   - **A**: You can try another segmentation model by specifying `segmenter --grandqc`.

- **Q**: I want to process a custom list of WSIs. Can I do it? Also, most of my WSIs don't have the micron per pixel (mpp) stored. Can I pass it?
   - **A**: Yes using the `--custom_list_of_wsis` argument. Provide a list of WSI names in a CSV (with slide extension, `wsi`). Optionally, provide the mpp (field `mpp`)
 
 - **Q**: Do I need to install any additional packages to use Trident?
   - **A**: Most pretrained models require additional dependencies (e.g., the CTransPath patch encoder requires `pip install timm_ctp`). When you load a model using Trident, it will tell you what dependencies are missing and how to install them. 

## License and Terms of Use

â“’ Mahmood Lab. This repository is released under the [CC-BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/deed.en) license and may only be used for non-commercial, academic research purposes with proper attribution. Any commercial use, sale, or other monetization of this repository is prohibited and requires prior approval. By downloading any pretrained encoder, you agree to follow the model's respective license.

## Acknowledgements

The project was built on top of amazing repositories such as [Timm](https://github.com/huggingface/pytorch-image-models/), [HuggingFace](https://huggingface.co/docs/datasets/en/index), and open-source contributions from the community. We thank the authors and developers for their contribution. 

## Issues

- The preferred mode of communication is via GitHub issues.
- If GitHub issues are inappropriate, email gjaume@bwh.harvard.edu and andrewzh@mit.edu.
- Immediate response to minor issues may not be available.

## Funding
This work was funded by NIH NIGMS [R35GM138216](https://reporter.nih.gov/search/sWDcU5IfAUCabqoThQ26GQ/project-details/10029418).

## How to cite

If you find our work useful in your research or if you use parts of this code, please consider citing our papers:

```
@article{zhang2025standardizing,
  title={Accelerating Data Processing and Benchmarking of AI Models for Pathology},
  author={Zhang, Andrew and Jaume, Guillaume and Vaidya, Anurag and Ding, Tong and Mahmood, Faisal},
  journal={arXiv preprint arXiv:2502.06750},
  year={2025}
}

@article{vaidya2025molecular,
  title={Molecular-driven Foundation Model for Oncologic Pathology},
  author={Vaidya, Anurag and Zhang, Andrew and Jaume, Guillaume and Song, Andrew H and Ding, Tong and Wagner, Sophia J and Lu, Ming Y and Doucet, Paul and Robertson, Harry and Almagro-Perez, Cristina and others},
  journal={arXiv preprint arXiv:2501.16652},
  year={2025}
}

```

<img src="_readme/joint_logo.png"> 



================================================
File: DETAILS.md
================================================
## Quality Control

trident outputs a variety of files for quality control. It is recommended that you review these files after each step to ensure that the results are as expected.

1. Segmentation contours are saved in the `./<job_dir>/contours` directory. These are thumbnails of the WSI with the tissue contours drawn in green.

<img src="_readme/contours.jpg" alt="WSI thumbnail with the tissue contours drawn in green." height="150px">

2. Patch annotations are saved in the `./<job_dir>/<patch_dir>/visualization` directory. These are thumbnails of the WSI with the patch borders drawn in red.

<img src="_readme/viz.jpg" alt="Patches drawn on top of the original WSI." height="150px">

## Custom Pipelines

Trident provides two simple `encoder_factory` functions for loading many patch and slide encoders through a unified API. You can import `encoder_factory` and load pretrained foundation model encoders into your own pipeline for inference or finetuning.

### Patch Encoders
```python
from trident.patch_encoder_models.load import encoder_factory
encoder = encoder_factory("uni_v1") # Or any other encoder name
# Attributes:
print(encoder.enc_name)         # Model name 
print(encoder.eval_transforms)  # PyTorch transforms to process the input image
print(encoder.precision)        # Recommended precision to run the model
```

### Slide Encoders
```python
from trident.slide_encoder_models.load import encoder_factory
encoder = encoder_factory("titan") # Or any other encoder name
# Attributes:
print(encoder.enc_name)         # Model name
print(encoder.precision)        # Recommended precision to run the model
```

Some encoders take optional keyword arguments. For example, the `conch_v1` encoder can be run with or without the projection head. These keyword arguments can be passed directly to encoder_factory:

```python
encoder = encoder_factory("conch_v1", with_proj=True)
```

## Need for Speed
Trident offers two optional ways to meet those conference deadlines on short notice: caching and multiprocessing.

### Caching
If your WSIs are on a cloud directory, it may be beneficial to copy them to a local directory before feature extraction. This is because the time it takes to read a WSI from a remote location is often longer than the time it takes to process the WSI locally. To do this, you can specify a path for `wsi_cache` when initializing Trident (see `run_trident.py`). If you do not specify `wsi_cache`, Trident will process the WSIs directly from `wsi_source_dir`. Caching is only recommended if you plan to do feature extraction; otherwise, the benefit of caching is typically outweighed by the I/O cost of copying the WSIs.

Here is an example workflow using caching:
1. Run segmentation and patching normally, without caching.
2. We will use the cache for feature extraction. To copy WSIs to the cache, run this command:
```bash
python run_batch_of_slides.py --task cache --job_dir ./trident_processed --wsi_dir wsis --wsi_cache cache_dir
```
3. While the WSIs are being transferred, you can start a feature extraction job in a separate terminal window, pointing to the same `wsi_cache` directory. For example:
```bash
python run_batch_of_slides.py --task feat --wsi_dir wsis --job_dir ./trident_processed --patch_encoder uni_v1 --mag 20 --patch_size 256 --wsi_cache cache_dir
```
This instance will automatically use the cached WSIs if they are available, or skip to the next WSI if they are not.

If you are running low on storage, you can set `clear_cache` to `True` in the feature extraction job. This will delete the cached WSIs as they are processed. Note that this assumes the caching job can run faster than the feature extraction job. Otherwise, the feature extraction job will skip slides because the caching job has not yet finished transferring them. If you find this is happening, you can rerun the feature extraction job once the caching job has finished.

> [!WARNING]
> Be careful when setting `clear_cache` to `True`. Make sure `wsi_cache` is set to the correct directory. Otherwise, you may accidentally delete your original copy of the raw WSIs.

### Multiprocessing
Trident supports flexible multiprocessing, so you can run many instances of caching, segmentation, patching, or feature extraction in parallel and they will automatically avoid conflicts by "leapfrogging" each other. Before processing a slide, Trident creates a lockfile of the form `{slide_name}.lock`. If another Trident instance tries to process the same slide, it will see the lock and skip to the next slide.

Here is an example workflow using multiprocessing:
1. Open a terminal window (it is highly recommended you use [tmux](https://github.com/tmux/tmux/wiki) so that processes continue running even if your computer sleeps) and start a segmentation job:
```bash
python run_batch_of_slides.py --task seg --wsi_dir ./wsis --job_dir ./trident_processed --gpu 0
```
2. Open another terminal window (or another tmux pane) and run the exact same command.
3. Repeat Step 2 one or more times to spawn additional processes depending on how powerful your computer is. Be careful not to overload your machine, see Tips below:

#### Tips:
- Running multiple instances in parallel does not necessarily speed up processing, because of CPU or I/O bottlenecks. For example, on my machine I find that running more than 3 feature extraction instances in parallel causes things to lock up. Running multiple instances of the same task is probably only helpful if your bottleneck is the GPU. Keep track of your CPU load using `htop`.
- If you are concurrently running two consecutive tasks (e.g., patching and feature extraction), make sure that the upstream task is faster than the downstream task. Otherwise, the downstream task will end up skipping slides because the upstream task has not yet finished processing them. In case this happens, just rerun the downstream task once the upstream task has finished.

## Under the Hood

### Segmentation
Segmentation is performed by a deep learning model. 2 options: HEST segmenter, a [DeepLabV3 model](https://arxiv.org/abs/1706.05587v3) finetuned specifically to segment tissue from background in WSIs (`--segmenter hest`), or GrandQC segmenter, published in [Nat. Comm](https://www.nature.com/articles/s41467-024-54769-y) which can be used with `--segmenter grandqc`. 

### Patching
Trident's patching module is deterministic and extracts a set of patch coordinates given a tissue mask. Patches are extracted at a particular size (`patch_size`) and particular magnification (`mag`). Trident will attempt to read the base resolution of the WSI and calculate the appropriate downsample factor to extract patches at the desired magnification. Patches can either be nonoverlapping (`overlap == 0`) or overlapping (`overlap > 0`), where `overlap` refers to the absolute overlap in pixels. Trident keeps all patches that contain at least one pixel of tissue.

### Feature Extraction
Given a set of WSIs and patch coordinates, Trident's feature extraction module extracts features from the patches using a pretrained image encoder and associated transforms. Internally, Trident extracts patches from each WSI in batches and applies the provided transformations to them, then feeds them through the image encoder. The resulting patch features (shape (`num_patches`, `feature_dim`)) can be saved as h5 or pt files.

The `batch_size` parameter can be used to limit the number of patches processed in parallel. It is recommended to set `batch_size` as high as possible without running out of VRAM. The provided image encoder has only the following requirements: (1) must subclass `nn.Module`, (2) must have a `forward` method which takes a tensor of shape (b c h w) and returns a tensor of shape (b f). The image encoder must also have a `transforms` attribute, which should resize the image to the size expected by the image encoder and apply any other necessary transformations (including normalization and conversion to tensor). 



================================================
File: LICENSE
================================================
# Attribution-NonCommercial-NoDerivatives 4.0 International

> *Creative Commons Corporation (â€œCreative Commonsâ€) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an â€œas-isâ€ basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.*
>
> ### Using Creative Commons Public Licenses
>
> Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.
>
> * __Considerations for licensors:__ Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. [More considerations for licensors](http://wiki.creativecommons.org/Considerations_for_licensors_and_licensees#Considerations_for_licensors).
>
> * __Considerations for the public:__ By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensorâ€™s permission is not necessary for any reasonâ€“for example, because of any applicable exception or limitation to copyrightâ€“then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. [More considerations for the public](http://wiki.creativecommons.org/Considerations_for_licensors_and_licensees#Considerations_for_licensees).

## Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License

By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.

### Section 1 â€“ Definitions.

a. __Adapted Material__ means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.

b. __Copyright and Similar Rights__ means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.

e. __Effective Technological Measures__ means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.

f. __Exceptions and Limitations__ means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.

h. __Licensed Material__ means the artistic or literary work, database, or other material to which the Licensor applied this Public License.

i. __Licensed Rights__ means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.

h. __Licensor__ means the individual(s) or entity(ies) granting rights under this Public License.

i. __NonCommercial__ means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.

j. __Share__ means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.

k. __Sui Generis Database Rights__ means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.

l. __You__ means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.

### Section 2 â€“ Scope.

a. ___License grant.___

   1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:

        A. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and

        B. produce and reproduce, but not Share, Adapted Material for NonCommercial purposes only.

   2. __Exceptions and Limitations.__ For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.

   3. __Term.__ The term of this Public License is specified in Section 6(a).

   4. __Media and formats; technical modifications allowed.__ The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.

   5. __Downstream recipients.__

        A. __Offer from the Licensor â€“ Licensed Material.__ Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.

        B. __No downstream restrictions.__ You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.

   6. __No endorsement.__ Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).

b. ___Other rights.___

   1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.

   2. Patent and trademark rights are not licensed under this Public License.

   3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.

### Section 3 â€“ License Conditions.

Your exercise of the Licensed Rights is expressly made subject to the following conditions.

a. ___Attribution.___

   1. If You Share the Licensed Material, You must:

      A. retain the following if it is supplied by the Licensor with the Licensed Material:

         i. identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);

         ii. a copyright notice;

         iii. a notice that refers to this Public License;

         iv. a notice that refers to the disclaimer of warranties;

         v. a URI or hyperlink to the Licensed Material to the extent reasonably practicable;

      B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and

      C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.
 
        For the avoidance of doubt, You do not have permission under this Public License to Share Adapted Material.

   2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.

   3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.

### Section 4 â€“ Sui Generis Database Rights.

Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:

a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only and provided You do not Share Adapted Material;

b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and

c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.

For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.

### Section 5 â€“ Disclaimer of Warranties and Limitation of Liability.

a. __Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.__

b. __To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.__

c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.

### Section 6 â€“ Term and Termination.

a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.

b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:

   1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or

   2. upon express reinstatement by the Licensor.

   For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.

c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.

d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License.

### Section 7 â€“ Other Terms and Conditions.

a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.

b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.

### Section 8 â€“ Interpretation.

a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.

b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.

c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.

d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.

> Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the â€œLicensor.â€ Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at [creativecommons.org/policies](http://creativecommons.org/policies), Creative Commons does not authorize the use of the trademark â€œCreative Commonsâ€ or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.
>
> Creative Commons may be contacted at [creativecommons.org](http://creativecommons.org).



================================================
File: pyproject.toml
================================================
[tool.poetry]
name = "trident"
version = "0.2.0"
description = "A package for preprocessing whole-slide images."
authors = [
    "Andrew Zhang <andrewzh@mit.edu>",
    "Guillaume Jaume <gjaume@bwh.harvard.edu>",
    "Paul Doucet <homedoucetpaul@gmail.com>"
]
license = "CC BY-NC-ND 4.0"  # Specify your package's license if different.
repository = "https://github.com/mahmoodlab/TRIDENT"

[tool.poetry.dependencies]
python = "^3.10"  # Specify the Python version compatibility.
ipywidgets = "*"
torch = "*"
transformers = "*"
tqdm = "*"
h5py = "*"
matplotlib = "*"
segmentation-models-pytorch = "*"
opencv-python = "*"
openslide-python = "*"
Pillow = "*"
timm = "0.9.16"
einops_exts = "*"
geopandas = "*"
huggingface_hub = "*"
openslide-bin = "*"
scipy = "*"

[tool.poetry.dev-dependencies]
# Optional development dependencies 

[tool.poetry.scripts]
run_batch_of_slides = "run_batch_of_slides:main"
run_single_slide = "run_single_slide:main"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry.package]
include = [
    { format = "file", path = "trident/slide_encoder_models/local_ckpts.json" },
    { format = "file", path = "trident/patch_encoder_models/local_ckpts.json" },
    { format = "file", path = "trident/segmentation_models/local_ckpts.json" },
]
include_package_data = true



================================================
File: run_batch_of_slides.py
================================================
"""
Example usage:

```
python run_batch_of_slides.py --task all --wsi_dir output/wsis --job_dir output --patch_encoder uni_v1 --mag 20 --patch_size 256
```

"""
import os
import argparse
import torch

from trident import Processor 


def build_parser():
    """
    Parse command-line arguments for the Trident processing script.
    """
    parser = argparse.ArgumentParser(description='Run Trident')

    # Generic arguments 
    parser.add_argument('--gpu', type=int, default=0, help='GPU index to use for processing tasks.')
    parser.add_argument('--task', type=str, default='seg', 
                        choices=['seg', 'coords', 'feat', 'all'], 
                        help='Task to run: seg (segmentation), coords (save tissue coordinates), img (save tissue images), feat (extract features).')
    parser.add_argument('--job_dir', type=str, required=True, help='Directory to store outputs.')
    parser.add_argument('--skip_errors', action='store_true', default=False, 
                        help='Skip errored slides and continue processing.')
    parser.add_argument('--max_workers', type=int, default=None, help='Maximum number of workers. Set to 0 to use main process.')
    parser.add_argument('--batch_size', type=int, default=64, 
                        help="Batch size used for segmentation and feature extraction. Will be override by"
                        "`seg_batch_size` and `feat_batch_size` if you want to use different ones. Defaults to 64.")

    # Caching argument for fast WSI processing
    parser.add_argument(
        '--wsi_cache', type=str, default=None,
        help='Path to a local cache (e.g., SSD) used to speed up access to WSIs stored on slower drives (e.g., HDD).'
    )
    parser.add_argument(
        '--cache_batch_size', type=int, default=32,
        help='Maximum number of slides to cache locally at once. Helps control disk usage.'
    )

    # Slide-related arguments
    parser.add_argument('--wsi_dir', type=str, required=True, 
                        help='Directory containing WSI files (no nesting allowed).')
    parser.add_argument('--wsi_ext', type=str, nargs='+', default=None, 
                        help='List of allowed file extensions for WSI files.')
    parser.add_argument('--custom_mpp_keys', type=str, nargs='+', default=None,
                    help='Custom keys used to store the resolution as MPP (micron per pixel) in your list of whole-slide image.')
    parser.add_argument('--custom_list_of_wsis', type=str, default=None,
                    help='Custom list of WSIs specified in a csv file.')
    parser.add_argument('--reader_type', type=str, choices=['openslide', 'image', 'cucim'], default=None,
                    help='Force the use of a specific WSI image reader. Options are ["openslide", "image", "cucim"]. Defaults to None (auto-determine which reader to use).')
    parser.add_argument("--search_nested", action="store_true",
                        help=("If set, recursively search for whole-slide images (WSIs) within all subdirectories of "
                              "`wsi_source`. Uses `os.walk` to include slides from nested folders. "
                              "This allows processing of datasets organized in hierarchical structures. "
                              "Defaults to False (only top-level slides are included)."))
    # Segmentation arguments 
    parser.add_argument('--segmenter', type=str, default='hest', 
                        choices=['hest', 'grandqc'], 
                        help='Type of tissue vs background segmenter. Options are HEST or GrandQC.')
    parser.add_argument('--seg_conf_thresh', type=float, default=0.5, 
                    help='Confidence threshold to apply to binarize segmentation predictions. Lower this threhsold to retain more tissue. Defaults to 0.5. Try 0.4 as 2nd option.')
    parser.add_argument('--remove_holes', action='store_true', default=False, 
                        help='Do you want to remove holes?')
    parser.add_argument('--remove_artifacts', action='store_true', default=False, 
                        help='Do you want to run an additional model to remove artifacts (including penmarks, blurs, stains, etc.)?')
    parser.add_argument('--remove_penmarks', action='store_true', default=False, 
                        help='Do you want to run an additional model to remove penmarks?')
    parser.add_argument('--seg_batch_size', type=int, default=None, 
                        help='Batch size for segmentation. Defaults to None (use `batch_size` argument instead).')
    
    # Patching arguments
    parser.add_argument('--mag', type=int, choices=[5, 10, 20, 40, 80], default=20, 
                        help='Magnification for coords/features extraction.')
    parser.add_argument('--patch_size', type=int, default=512, 
                        help='Patch size for coords/image extraction.')
    parser.add_argument('--overlap', type=int, default=0, 
                        help='Absolute overlap for patching in pixels. Defaults to 0.')
    parser.add_argument('--min_tissue_proportion', type=float, default=0., 
                        help='Minimum proportion of the patch under tissue to be kept. Between 0. and 1.0. Defaults to 0.')
    parser.add_argument('--coords_dir', type=str, default=None, 
                        help='Directory to save/restore tissue coordinates.')
    # Feature extraction arguments 
    parser.add_argument('--patch_encoder', type=str, default='conch_v15', 
                        choices=['conch_v1', 'uni_v1', 'uni_v2', 'ctranspath', 'phikon', 
                                 'resnet50', 'gigapath', 'virchow', 'virchow2', 
                                 'hoptimus0', 'hoptimus1', 'phikon_v2', 'conch_v15', 'musk', 'hibou_l',
                                 'kaiko-vits8', 'kaiko-vits16', 'kaiko-vitb8', 'kaiko-vitb16',
                                 'kaiko-vitl14', 'lunit-vits8', 'midnight12k'],
                        help='Patch encoder to use')
    parser.add_argument(
        '--patch_encoder_ckpt_path', type=str, default=None,
        help=(
            "Optional local path to a patch encoder checkpoint (.pt, .pth, .bin, or .safetensors). "
            "This is only needed in offline environments (e.g., compute clusters without internet). "
            "If not provided, models are downloaded automatically from Hugging Face. "
            "You can also specify local paths via the model registry at "
            "`./trident/patch_encoder_models/local_ckpts.json`."
        )
    )
    parser.add_argument('--slide_encoder', type=str, default=None, 
                        choices=['threads', 'titan', 'prism', 'gigapath', 'chief', 'madeleine',
                                 'mean-virchow', 'mean-virchow2', 'mean-conch_v1', 'mean-conch_v15', 'mean-ctranspath',
                                 'mean-gigapath', 'mean-resnet50', 'mean-hoptimus0', 'mean-phikon', 'mean-phikon_v2',
                                 'mean-musk', 'mean-uni_v1', 'mean-uni_v2',  
                                 ], 
                        help='Slide encoder to use')
    parser.add_argument('--feat_batch_size', type=int, default=None, 
                        help='Batch size for feature extraction. Defaults to None (use `batch_size` argument instead).')
    return parser


def parse_arguments():
    return build_parser().parse_args()


def generate_help_text() -> str:
    """
    Generate the command-line help text for documentation purposes.
    
    Returns:
        str: The full help message string from the argument parser.
    """
    parser = build_parser()
    return parser.format_help()


def initialize_processor(args):
    """
    Initialize the Trident Processor with arguments set in `run_batch_of_slides`.
    """
    return Processor(
        job_dir=args.job_dir,
        wsi_source=args.wsi_dir,
        wsi_ext=args.wsi_ext,
        wsi_cache=args.wsi_cache,
        skip_errors=args.skip_errors,
        custom_mpp_keys=args.custom_mpp_keys,
        custom_list_of_wsis=args.custom_list_of_wsis,
        max_workers=args.max_workers,
        reader_type=args.reader_type,
        search_nested=args.search_nested,
    )


def run_task(processor, args):
    """
    Execute the specified task using the Trident Processor.
    """

    if args.task == 'seg':
        from trident.segmentation_models.load import segmentation_model_factory

        # instantiate segmentation model and artifact remover if requested by user
        segmentation_model = segmentation_model_factory(
            args.segmenter,
            confidence_thresh=args.seg_conf_thresh,
        )
        if args.remove_artifacts or args.remove_penmarks:
            artifact_remover_model = segmentation_model_factory(
                'grandqc_artifact',
                remove_penmarks_only=args.remove_penmarks and not args.remove_artifacts
            )
        else:
            artifact_remover_model = None

        # run segmentation 
        processor.run_segmentation_job(
            segmentation_model,
            seg_mag=segmentation_model.target_mag,
            holes_are_tissue= not args.remove_holes,
            artifact_remover_model=artifact_remover_model,
            batch_size=args.seg_batch_size if args.seg_batch_size is not None else args.batch_size,
            device=f'cuda:{args.gpu}',
        )
    elif args.task == 'coords':
        processor.run_patching_job(
            target_magnification=args.mag,
            patch_size=args.patch_size,
            overlap=args.overlap,
            saveto=args.coords_dir,
            min_tissue_proportion=args.min_tissue_proportion
        )
    elif args.task == 'feat':
        if args.slide_encoder is None: 
            from trident.patch_encoder_models.load import encoder_factory
            encoder = encoder_factory(args.patch_encoder, weights_path=args.patch_encoder_ckpt_path)
            processor.run_patch_feature_extraction_job(
                coords_dir=args.coords_dir or f'{args.mag}x_{args.patch_size}px_{args.overlap}px_overlap',
                patch_encoder=encoder,
                device=f'cuda:{args.gpu}',
                saveas='h5',
                batch_limit=args.feat_batch_size if args.feat_batch_size is not None else args.batch_size,
            )
        else:
            from trident.slide_encoder_models.load import encoder_factory
            encoder = encoder_factory(args.slide_encoder)
            processor.run_slide_feature_extraction_job(
                slide_encoder=encoder,
                coords_dir=args.coords_dir or f'{args.mag}x_{args.patch_size}px_{args.overlap}px_overlap',
                device=f'cuda:{args.gpu}',
                saveas='h5',
                batch_limit=args.feat_batch_size if args.feat_batch_size is not None else args.batch_size,
            )
    else:
        raise ValueError(f'Invalid task: {args.task}')


def main():

    args = parse_arguments()
    args.device = f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu'

    if args.wsi_cache:
        # === Parallel pipeline with caching ===

        from queue import Queue
        from threading import Thread

        from trident.Concurrency import batch_producer, batch_consumer, cache_batch
        from trident.IO import collect_valid_slides

        queue = Queue(maxsize=1)
        valid_slides = collect_valid_slides(
            wsi_dir=args.wsi_dir,
            custom_list_path=args.custom_list_of_wsis,
            wsi_ext=args.wsi_ext,
            search_nested=args.search_nested,
            max_workers=args.max_workers
        )
        print(f"[MAIN] Found {len(valid_slides)} valid slides in {args.wsi_dir}.")

        warm = valid_slides[:args.cache_batch_size]
        warmup_dir = os.path.join(args.wsi_cache, "batch_0")
        print(f"[MAIN] Warmup caching batch: {warmup_dir}")
        cache_batch(warm, warmup_dir)
        queue.put(0)

        def processor_factory(wsi_dir: str) -> Processor:
            local_args = argparse.Namespace(**vars(args))
            local_args.wsi_dir = wsi_dir
            local_args.wsi_cache = None
            local_args.custom_list_of_wsis = None
            local_args.search_nested = False
            return initialize_processor(local_args)

        def run_task_fn(processor: Processor, task_name: str):
            args.task = task_name
            run_task(processor, args)

        producer = Thread(target=batch_producer, args=(
            queue, valid_slides, args.cache_batch_size, args.cache_batch_size, args.wsi_cache
        ))

        consumer = Thread(target=batch_consumer, args=(
            queue, args.task, args.wsi_cache, processor_factory, run_task_fn
        ))

        print("[MAIN] Starting producer and consumer threads.")
        producer.start()
        consumer.start()
        producer.join()
        consumer.join()
    else:
        # === Sequential mode ===
        processor = initialize_processor(args)
        tasks = ['seg', 'coords', 'feat'] if args.task == 'all' else [args.task]
        for task_name in tasks:
            args.task = task_name
            run_task(processor, args)


if __name__ == "__main__":
    main()



================================================
File: run_single_slide.py
================================================
"""
Example usage:

```
python run_single_slide.py --slide_path output/wsis/394140.svs --job_dir output/ --mag 20 --patch_size 256
```

"""
import argparse
import os

from trident import load_wsi
from trident.segmentation_models import segmentation_model_factory
from trident.patch_encoder_models import encoder_factory


def parse_arguments():
    """
    Parse command-line arguments for processing a single WSI.
    """
    parser = argparse.ArgumentParser(description="Process a WSI from A to Z.")
    parser.add_argument("--gpu", type=int, default=0, help="GPU index to use for processing tasks")
    parser.add_argument("--slide_path", type=str, required=True, help="Path to the WSI file to process")
    parser.add_argument("--job_dir", type=str, required=True, help="Directory to store outputs")
    parser.add_argument('--patch_encoder', type=str, default='conch_v15', 
                        choices=['conch_v1', 'uni_v1', 'uni_v2', 'ctranspath', 'phikon', 
                                 'resnet50', 'gigapath', 'virchow', 'virchow2', 
                                 'hoptimus0', 'hoptimus1', 'phikon_v2', 'conch_v15', 'musk', 'hibou_l',
                                 'kaiko-vits8', 'kaiko-vits16', 'kaiko-vitb8', 'kaiko-vitb16',
                                 'kaiko-vitl14', 'lunit-vits8'],
                        help='Patch encoder to use')
    parser.add_argument("--mag", type=int, choices=[5, 10, 20, 40], default=20,
                        help="Magnification at which patches/features are extracted")
    parser.add_argument("--patch_size", type=int, default=256, help="Patch size at which coords/features are extracted")
    parser.add_argument('--segmenter', type=str, default='hest', 
                        choices=['hest', 'grandqc',], 
                        help='Type of tissue vs background segmenter. Options are HEST or GrandQC.')
    parser.add_argument('--seg_conf_thresh', type=float, default=0.5, 
                    help='Confidence threshold to apply to binarize segmentation predictions. Lower this threhsold to retain more tissue. Defaults to 0.5. Try 0.4 as 2nd option.')
    parser.add_argument('--custom_mpp_keys', type=str, nargs='+', default=None,
                    help='Custom keys used to store the resolution as MPP (micron per pixel) in your list of whole-slide image.')
    parser.add_argument('--overlap', type=int, default=0, 
                        help='Absolute overlap for patching in pixels. Defaults to 0. ')
    parser.add_argument('--batch_size', type=int, default=32, 
                        help='Batch size for feature extraction. Defaults to 32.')
    return parser.parse_args()


def process_slide(args):
    """
    Process a single WSI by performing segmentation, patch extraction, and feature extraction sequentially.
    """

    # Initialize the WSI
    print(f"Processing slide: {args.slide_path}")
    slide = load_wsi(slide_path=args.slide_path, lazy_init=False, custom_mpp_keys=args.custom_mpp_keys)

    # Step 1: Tissue Segmentation
    print("Running tissue segmentation...")
    segmentation_model = segmentation_model_factory(
        model_name=args.segmenter,
        confidence_thresh=args.seg_conf_thresh,
    )
    slide.segment_tissue(
        segmentation_model=segmentation_model,
        target_mag=segmentation_model.target_mag,
        job_dir=args.job_dir,
        device=f"cuda:{args.gpu}"
    )
    print(f"Tissue segmentation completed. Results saved to {args.job_dir}contours_geojson and {args.job_dir}contours")

    # Step 2: Tissue Coordinate Extraction (Patching)
    print("Extracting tissue coordinates...")
    save_coords = os.path.join(args.job_dir, f'{args.mag}x_{args.patch_size}px_{args.overlap}px_overlap')

    coords_path = slide.extract_tissue_coords(
        target_mag=args.mag,
        patch_size=args.patch_size,
        save_coords=save_coords
    )
    print(f"Tissue coordinates extracted and saved to {coords_path}.")

    # Step 3: Visualize patching
    viz_coords_path = slide.visualize_coords(
        coords_path=coords_path,
        save_patch_viz=os.path.join(save_coords, 'visualization'),
    )
    print(f"Tissue coordinates extracted and saved to {viz_coords_path}.")

    # Step 4: Feature Extraction
    print("Extracting features from patches...")
    encoder = encoder_factory(args.patch_encoder)
    encoder.eval()
    encoder.to(f"cuda:{args.gpu}")
    features_path = features_dir = os.path.join(save_coords, "features_{}".format(args.patch_encoder))
    slide.extract_patch_features(
        patch_encoder=encoder,
        coords_path=os.path.join(save_coords, 'patches', f'{slide.name}_patches.h5'),
        save_features=features_dir,
        device=f"cuda:{args.gpu}",
        batch_limit=args.batch_size
    )
    print(f"Feature extraction completed. Results saved to {features_path}")


def main():
    args = parse_arguments()
    process_slide(args)


if __name__ == "__main__":
    main()



================================================
File: .readthedocs.yaml
================================================
version: 2

build:
  os: "ubuntu-20.04"
  tools:
    python: "3.10"

python:
  install:
    - requirements: docs/requirements.txt
    - method: pip
      path: .

sphinx:
  configuration: docs/conf.py




================================================
File: docs/Makefile
================================================
# Minimal makefile for Sphinx documentation
#

# You can set these variables from the command line, and also
# from the environment for the first two.
SPHINXOPTS    ?=
SPHINXBUILD   ?= sphinx-build
SOURCEDIR     = .
BUILDDIR      = _build

# Put it first so that "make" without argument is like "make help".
help:
	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

.PHONY: help Makefile

# Catch-all target: route all unknown targets to Sphinx using the new
# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
%: Makefile
	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)



================================================
File: docs/api.rst
================================================
API Reference
=============

This section documents the **public API** of TRIDENT. 

.. contents::
   :local:
   :depth: 2


Trident
-------

Core of TRIDENT with `Processor` and WSI building.

.. automodule:: trident
   :members:
   :undoc-members:
   :inherited-members:
   :show-inheritance:


Segmentation Models
-------------------

Semantic segmentation models for tissue vs. background detection and filtering.

.. automodule:: trident.segmentation_models
   :members:
   :undoc-members:


Patch Encoders
--------------

Factory for loading patch-level encoder models.

.. list-table:: 
   :header-rows: 1
   :widths: 18 10 40 32

   * - Patch Encoder
     - Dim
     - Args
     - Link
   * - **UNI**
     - 1024
     - ``--patch_encoder uni_v1 --patch_size 256 --mag 20``
     - `MahmoodLab/UNI <https://huggingface.co/MahmoodLab/UNI>`__
   * - **UNI2-h**
     - 1536
     - ``--patch_encoder uni_v2 --patch_size 256 --mag 20``
     - `MahmoodLab/UNI2-h <https://huggingface.co/MahmoodLab/UNI2-h>`__
   * - **CONCH**
     - 512
     - ``--patch_encoder conch_v1 --patch_size 512 --mag 20``
     - `MahmoodLab/CONCH <https://huggingface.co/MahmoodLab/CONCH>`__
   * - **CONCHv1.5**
     - 768
     - ``--patch_encoder conch_v15 --patch_size 512 --mag 20``
     - `MahmoodLab/conchv1_5 <https://huggingface.co/MahmoodLab/conchv1_5>`__
   * - **Virchow**
     - 2560
     - ``--patch_encoder virchow --patch_size 224 --mag 20``
     - `paige-ai/Virchow <https://huggingface.co/paige-ai/Virchow>`__
   * - **Virchow2**
     - 2560
     - ``--patch_encoder virchow2 --patch_size 224 --mag 20``
     - `paige-ai/Virchow2 <https://huggingface.co/paige-ai/Virchow2>`__
   * - **Phikon**
     - 768
     - ``--patch_encoder phikon --patch_size 224 --mag 20``
     - `owkin/phikon <https://huggingface.co/owkin/phikon>`__
   * - **Phikon-v2**
     - 1024
     - ``--patch_encoder phikon_v2 --patch_size 224 --mag 20``
     - `owkin/phikon-v2 <https://huggingface.co/owkin/phikon-v2/>`__
   * - **Prov-Gigapath**
     - 1536
     - ``--patch_encoder gigapath --patch_size 256 --mag 20``
     - `prov-gigapath <https://huggingface.co/prov-gigapath/prov-gigapath>`__
   * - **H-Optimus-0**
     - 1536
     - ``--patch_encoder hoptimus0 --patch_size 224 --mag 20``
     - `bioptimus/H-optimus-0 <https://huggingface.co/bioptimus/H-optimus-0>`__
   * - **H-Optimus-1**
     - 1536
     - ``--patch_encoder hoptimus1 --patch_size 224 --mag 20``
     - `bioptimus/H-optimus-1 <https://huggingface.co/bioptimus/H-optimus-1>`__
   * - **MUSK**
     - 1024
     - ``--patch_encoder musk --patch_size 384 --mag 20``
     - `xiangjx/musk <https://huggingface.co/xiangjx/musk>`__
   * - **Midnight-12k**
     - 3072
     - ``--patch_encoder midnight12k --patch_size 224 --mag 20``
     - `kaiko-ai/midnight <https://huggingface.co/kaiko-ai/midnight>`__
   * - **Kaiko**
     - 384/768/1024
     - ``--patch_encoder kaiko-vit* --patch_size 256 --mag 20``
     - `Kaiko Collection <https://huggingface.co/collections/1aurent/kaikoai-models-66636c99d8e1e34bc6dcf795>`__
   * - **Lunit**
     - 384
     - ``--patch_encoder lunit-vits8 --patch_size 224 --mag 20``
     - `1aurent/lunit <https://huggingface.co/1aurent/vit_small_patch8_224.lunit_dino>`__
   * - **Hibou**
     - 1024
     - ``--patch_encoder hibou_l --patch_size 224 --mag 20``
     - `histai/hibou-L <https://huggingface.co/histai/hibou-L>`__
   * - **CTransPath-CHIEF**
     - 768
     - ``--patch_encoder ctranspath --patch_size 256 --mag 10``
     - â€”
   * - **ResNet50**
     - 1024
     - ``--patch_encoder resnet50 --patch_size 256 --mag 20``
     - â€”

.. automodule:: trident.patch_encoder_models
   :members:
   :undoc-members:


Slide Encoders
--------------

Factory for slide-level encoder models.

.. list-table:: 
   :header-rows: 1
   :widths: 20 20 40 32

   * - Slide Encoder
     - Patch Encoder
     - Args
     - Link
   * - **Threads**
     - conch_v15
     - ``--slide_encoder threads --patch_size 512 --mag 20``
     - *(Coming Soon!)*
   * - **Titan**
     - conch_v15
     - ``--slide_encoder titan --patch_size 512 --mag 20``
     - `MahmoodLab/TITAN <https://huggingface.co/MahmoodLab/TITAN>`__
   * - **PRISM**
     - virchow
     - ``--slide_encoder prism --patch_size 224 --mag 20``
     - `paige-ai/Prism <https://huggingface.co/paige-ai/Prism>`__
   * - **CHIEF**
     - ctranspath
     - ``--slide_encoder chief --patch_size 256 --mag 10``
     - `CHIEF <https://github.com/hms-dbmi/CHIEF>`__
   * - **GigaPath**
     - gigapath
     - ``--slide_encoder gigapath --patch_size 256 --mag 20``
     - `prov-gigapath <https://huggingface.co/prov-gigapath/prov-gigapath>`__
   * - **Madeleine**
     - conch_v1
     - ``--slide_encoder madeleine --patch_size 256 --mag 10``
     - `MahmoodLab/madeleine <https://huggingface.co/MahmoodLab/madeleine>`__

.. automodule:: trident.slide_encoder_models
   :members:
   :undoc-members:



================================================
File: docs/citation.rst
================================================
Citation & License
==================

If you use TRIDENT in your work, please cite:

.. code-block:: bibtex

   @article{zhang2025standardizing,
     title={Accelerating Data Processing and Benchmarking of AI Models for Pathology},
     author={Zhang, Andrew and Jaume, Guillaume and Vaidya, Anurag and Ding, Tong and Mahmood, Faisal},
     journal={arXiv preprint arXiv:2502.06750},
     year={2025}
   }

   @article{vaidya2025molecular,
     title={Molecular-driven Foundation Model for Oncologic Pathology},
     author={Vaidya, Anurag and Zhang, Andrew and Jaume, Guillaume and ...},
     journal={arXiv preprint arXiv:2501.16652},
     year={2025}
   }

License
-------
Released under CC-BY-NC-ND 4.0. Academic use only.

Funding
-------
Supported by NIH NIGMS R35GM138216.


================================================
File: docs/conf.py
================================================
# Configuration file for the Sphinx documentation builder.
#
# This file only contains a selection of the most common options. For a full
# list see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
import subprocess
import sys
sys.path.insert(0, os.path.abspath('./../'))


# -- Project information -----------------------------------------------------

project = 'TRIDENT'
copyright = '2025, Guillaume Jaume'
author = 'Guillaume Jaume'

# The full version, including alpha/beta/rc tags
release = 'v0.1.1'

# HTML style
html_theme = 'sphinx_rtd_theme'
html_static_path = ['_static']
html_logo = '_static/lab_logo.svg'
html_theme_options = {
    "sidebar_hide_name": True,
}

# -- General configuration ---------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.autosummary',
    'sphinx.ext.napoleon',  # For NumPy or Google-style docstrings
    "sphinx_design",  
]
autosummary_generate = True

autoclass_content = 'both'  # Shows class-level and __init__ docstring
napoleon_include_init_with_doc = True  # for Google/NumPy-style docstrings

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

html_context = {
    "display_github": True,
    "github_user": "guillaumejaume",
    "github_repo": "TRIDENT",
    "github_version": "docs",
    "conf_py_path": "/docs/",
}

# === Auto-generate CLI help files before building docs ===

def run_cli_generate():
    print("Auto-generating CLI help text...")
    root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    cli_generate_script = os.path.join(root_dir, 'docs', 'cli_helpers', 'cli_generate.py')
    output_help_txt = os.path.join(root_dir, 'docs', 'generated', 'run_batch_of_slides_help.txt')

    os.makedirs(os.path.dirname(output_help_txt), exist_ok=True)

    with open(output_help_txt, 'w') as f:
        subprocess.run(["python", cli_generate_script], stdout=f, check=True)

run_cli_generate()



================================================
File: docs/faq.rst
================================================
Frequently Asked Questions
==========================

.. dropdown:: **How do I extract embeddings from legacy CLAM coordinates?**

   Use the `--coords_dir` flag to pass CLAM-style patch coordinates:

   .. code-block:: bash

      python run_batch_of_slides.py --task feat --wsi_dir wsis --job_dir legacy_dir --coords_dir extracted_coords --patch_encoder uni_v1


.. dropdown:: **My WSIs have no micron-per-pixel (MPP) or magnification metadata. What should I do?**

   PNGs and JPEGs do not store MPP metadata in the file itself. If you're working with such formats, passing a CSV via `--custom_list_of_wsis` is **required**. This CSV should include at least two columns: `wsi` and `mpp`.

   Example:

   .. code-block:: csv

      wsi,mpp
      TCGA-AJ-A8CV-01Z-00-DX1_1.png,0.25
      TCGA-AJ-A8CV-01Z-00-DX1_2.png,0.25
      TCGA-AJ-A8CV-01Z-00-DX1_3.png,0.25

   If you're using OpenSlide-readable formats (e.g., `.svs`, `.tiff`), this CSV is optionalâ€”but you can still use it to:

   - Restrict processing to a specific subset of slides
   - Override incorrect or missing MPP metadata


.. dropdown:: **I want to skip patches on holes.**

   By default, TRIDENT includes all tissue patches (including holes). Use `--remove_holes` to exclude them. Not recommended, as "holes" often help define the tissue microenvironment.

.. dropdown:: **I donâ€™t have enough local SSD storage and my WSIs are on a slow remote disk. How can I accelerate processing?**

   When WSIs are stored on slow network or external drives, processing can be very slow. Use `--wsi_cache ./cache --cache_batch_size 32` to enable local caching. WSIs will be copied in batches to a local SSD, processed in parallel, and automatically cleaned up after use. This significantly reduces I/O bottlenecks.

.. dropdown:: **My WSIs are in multiple subfolders. How can I process them all?**

   By default, only the top-level directory is scanned. Use `--search_nested` to recursively search for WSIs in all nested folders and include them in processing.

.. dropdown:: **I work on a cluster without Internet access. How can I use models offline?**

   You can use local checkpoint files by editing the model registry files in Trident. This allows you to cache or pre-download all necessary models for both segmentation and patch encoding.

   **1. Segmentation Models**

   Update the segmentation model registry at:
   `trident/segmentation_models/local_ckpts.json`

   Example:

   .. code-block:: json

      {
        "hest": "./ckpts/trident/deeplabv3_seg_v4.ckpt",
        "grandqc": "./ckpts/trident/Tissue_Detection_MPP10.pth",
        "grandqc_artifact": "./ckpts/trident/GrandQC_MPP1_state_dict.pth"
      }

   **2. Patch Encoder Models**

   Update the patch encoder model registry at:
   `trident/patch_encoder_models/local_ckpts.json`

   Example:

   .. code-block:: json

      {
        "conch_v1": "./ckpts/conch_patch_encoder/pytorch_model.bin",
        "uni_v1": "./ckpts/uni_patch_encoder/pytorch_model.bin",
        "uni_v2": "./ckpts/uni2_patch_encoder/pytorch_model.bin",
        "ctranspath": "./ckpts/ctranspath_patch_encoder/CHIEF_CTransPath.pth",
        "phikon": "./ckpts/phikon_patch_encoder/pytorch_model.bin",
        "resnet50": "./ckpts/resnet_patch_encoder/pytorch_model.bin",
        "gigapath": "./ckpts/gigapath_patch_encoder/pytorch_model.bin",
        "virchow": "./ckpts/virchow_patch_encoder/pytorch_model.bin",
        "virchow2": "./ckpts/virchow2_patch_encoder/pytorch_model.bin",
        "hoptimus0": "./ckpts/hoptimus0_patch_encoder/pytorch_model.bin",
        "hoptimus1": "./ckpts/hoptimus1_patch_encoder/pytorch_model.bin",
        "phikon_v2": "./ckpts/phikon-v2_patch_encoder/model.safetensors",
        "kaiko-vitb8": "./ckpts/kaiko_vitb8_patch_encoder/model.safetensors",
        "kaiko-vitb16": "./ckpts/kaiko_vitb16_patch_encoder/model.safetensors",
        "kaiko-vits8": "./ckpts/kaiko_vits8_patch_encoder/model.safetensors",
        "kaiko-vits16": "./ckpts/kaiko_vits16_patch_encoder/model.safetensors",
        "kaiko-vitl14": "./ckpts/kaiko_vitl14_patch_encoder/model.safetensors",
        "lunit-vits8": "./ckpts/lunit_patch_encoder/model.safetensors",
        "conch_v15": "./ckpts/conchv1_5_patch_encoder/pytorch_model_vision.bin"
      }

   **3. Alternative Option**

   You can also directly pass a local checkpoint path at runtime using the `--patch_encoder_ckpt_path` argument in `run_batch_of_slides.py`.

   **4. Optional: Pre-download All Models in Advance**

   Full credit to @haydenych. If you'd like to automatically download all model weights in advance (e.g., from a connected machine), use the following:

   .. code-block:: bash

      XDG_CACHE_HOME="<YOUR_CACHE_DIR>" HF_TOKEN="<YOUR_HUGGINGFACE_TOKEN>" python run_predownload_weights.py

   This will fetch all segmentation, patch encoder, and slide encoder weights supported in Trident.

   To run downstream tasks using the cached models:

   .. code-block:: bash

      XDG_CACHE_HOME="<YOUR_CACHE_DIR>" python run_single_slide.py ...
      XDG_CACHE_HOME="<YOUR_CACHE_DIR>" python run_batch_of_slides.py ...

   Example `run_predownload_weights.py` script (can be adapted based on needs):

   .. code-block:: python

      from trident.segmentation_models import segmentation_model_factory
      from trident.patch_encoder_models.load import encoder_factory as patch_encoder_model_factory
      from trident.slide_encoder_models.load import encoder_factory as slide_encoder_model_factory

      segmentation_models = ["hest", "grandqc", "grandqc_artifact"]
      for model in segmentation_models:
          try:
              segmentation_model_factory(model)
          except Exception as e:
              print(f"Failed to download weights for {model}: {e}")

      patch_encoder_models = [
          "conch_v1", "uni_v1", "uni_v2", "ctranspath", "phikon", "resnet50", "gigapath",
          "virchow", "virchow2", "hoptimus0", "hoptimus1", "phikon_v2", "conch_v15",
          "musk", "hibou_l", "kaiko-vits8", "kaiko-vits16", "kaiko-vitb8", "kaiko-vitb16",
          "kaiko-vitl14", "lunit-vits8"
      ]
      for model in patch_encoder_models:
          try:
              patch_encoder_model_factory(model)
          except Exception as e:
              print(f"Failed to download weights for {model}: {e}")

      slide_encoder_models = [
          "threads", "titan", "prism", "gigapath", "chief", "madeleine", "mean-virchow",
          "mean-virchow2", "mean-conch_v1", "mean-conch_v15", "mean-ctranspath", "mean-gigapath",
          "mean-resnet50", "mean-hoptimus0", "mean-phikon", "mean-phikon_v2", "mean-musk",
          "mean-uni_v1", "mean-uni_v2"
      ]
      for model in slide_encoder_models:
          try:
              slide_encoder_model_factory(model)
          except Exception as e:
              print(f"Failed to download weights for {model}: {e}")


================================================
File: docs/index.rst
================================================
.. image:: _static/trident_crop.jpg
   :align: right
   :width: 220px

Welcome to **TRIDENT**!
======================================

**TRIDENT** is a scalable toolkit for **large-scale whole-slide image (WSI) processing**, developed at the `Mahmood Lab <https://mahmoodlab.org>`_ at **Harvard Medical School** and **Brigham and Women's Hospital**.

ðŸš€ **What TRIDENT offers:**

- **Tissue vs. background segmentation** for H&E, IHC, special stains, and artifact removal
- **Patch-level feature extraction** using 20+ foundation models
- **Slide-level feature extraction** via 5+ pretrained model backbones
- Native support for **OpenSlide**, **CuCIM**, and **PIL-compatible** formats

Explore the **end-to-end pipeline**, from segmentation to slide-level representation â€” all powered by the latest **foundation models** for computational pathology.

---

.. toctree::
   :maxdepth: 2
   :caption: ðŸ“š Contents

   installation
   quickstart
   tutorials
   api
   faq
   citation

.. note::
   ðŸ§ª This project is supported by **NIH NIGMS R35GM138216** and is under active development by the Mahmood Lab.



================================================
File: docs/installation.rst
================================================
Installation
============

Create a fresh environment:

.. code-block:: bash

   conda create -n "trident" python=3.10
   conda activate trident

Clone the repository:

.. code-block:: bash

   git clone https://github.com/mahmoodlab/trident.git && cd trident

Install the package locally:

.. code-block:: bash

   pip install -e .

.. warning::
   Some pretrained models require additional dependencies. TRIDENT will guide you via error messages when needed.


================================================
File: docs/make.bat
================================================
@ECHO OFF

pushd %~dp0

REM Command file for Sphinx documentation

if "%SPHINXBUILD%" == "" (
	set SPHINXBUILD=sphinx-build
)
set SOURCEDIR=.
set BUILDDIR=_build

if "%1" == "" goto help

%SPHINXBUILD% >NUL 2>NUL
if errorlevel 9009 (
	echo.
	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
	echo.installed, then set the SPHINXBUILD environment variable to point
	echo.to the full path of the 'sphinx-build' executable. Alternatively you
	echo.may add the Sphinx directory to PATH.
	echo.
	echo.If you don't have Sphinx installed, grab it from
	echo.https://www.sphinx-doc.org/
	exit /b 1
)

%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%
goto end

:help
%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%

:end
popd



================================================
File: docs/quickstart.rst
================================================
Quickstart
==================

Trident provides user-facing command-line scripts for processing large batches of whole-slide images (WSIs).

This page explains how to quickly get started, and provides detailed help for available options.

---

Processing a batch of slides
----------

To process a batch of WSIs through segmentation, patch extraction, and feature extraction in one go,  
run the following command:

.. code-block:: bash

    python run_batch_of_slides.py --task all --wsi_dir output/wsis --job_dir output --patch_encoder uni_v1 --mag 20 --patch_size 256

- `--wsi_dir`: Folder containing your whole-slide images (.svs, .tiff, etc.)
- `--job_dir`: Folder where all outputs (masks, coordinates, features) will be stored
- `--patch_encoder`: Pre-trained encoder to use (e.g., `uni_v1`, `conch_v15`, etc.)
- `--mag`: Target magnification level for patches (e.g., 20x)
- `--patch_size`: Size of patches to extract (e.g., 256px)

This will:
1. Segment tissue areas in the slides.
2. Extract patch coordinates over the tissue.
3. Extract patch-level features using the specified encoder.

Typical Examples
-----------------

**Segmentation Only**  
(Segment tissue regions and save binary masks.)

.. code-block:: bash

    python run_batch_of_slides.py --task seg --wsi_dir input_wsis --job_dir output

**Patch Extraction Only**  
(Extract patch coordinates from tissue regions.)

.. code-block:: bash

    python run_batch_of_slides.py --task coords --wsi_dir input_wsis --job_dir output --mag 20 --patch_size 256

**Feature Extraction Only**  
(Extract features from patches using a patch encoder.)

.. code-block:: bash

    python run_batch_of_slides.py --task feat --wsi_dir input_wsis --job_dir output --patch_encoder uni_v1 --mag 20 --patch_size 256

---

Help Text
---------

The full list of available arguments and options is shown below.

.. note::

   .. raw:: html

      <div style="padding: 12px; background-color: #f9fafb; border: 1px solid #e0e0e0; border-radius: 8px; box-shadow: 2px 2px 8px rgba(0,0,0,0.05);">

      <h3 style="margin-top: 0px; margin-bottom: 10px;">
         ðŸ› ï¸ <b>Command-line Argument Reference</b>
      </h3>

   .. literalinclude:: generated/run_batch_of_slides_help.txt
      :language: text

   .. raw:: html

      </div>

Notes
-----

- If you use `--task all`, it will run segmentation, patch extraction, and feature extraction sequentially.
- For feature extraction, you can choose either a **patch encoder** (e.g., `uni_v1`) or a **slide encoder** (e.g., `threads`).
- You can cache WSIs locally using `--wsi_cache` for faster processing on networked filesystems.
- You can control the number of parallel workers with `--max_workers`.

For more advanced settings (artifact removal, segmentation confidence thresholds, slide readers),  
refer to the full help text above.




================================================
File: docs/requirements.txt
================================================
sphinx
sphinx_design
sphinx_rtd_theme  



================================================
File: docs/tutorials.rst
================================================
Tutorials
=========

Browse our interactive guides:

- `1-Step-by-Step-Patch-Feature-Extraction-with-Trident.ipynb <https://github.com/mahmoodlab/TRIDENT/blob/main/tutorials/1-Step-by-Step-Patch-Feature-Extraction-with-Trident.ipynb>`_: Guided-whole slide imahe processing. 
- `2-Using-Trident-With-Your-Custom-Patch-Encoder.ipynb <https://github.com/mahmoodlab/TRIDENT/blob/main/tutorials/2-Using-Trident-With-Your-Custom-Patch-Encoder.ipynb>`_: Using Trident with a custom patch encoder. 
- `3-Training-a-WSI-Classification-Model-with-ABMIL-and-Heatmaps.ipynb <https://github.com/mahmoodlab/TRIDENT/blob/main/tutorials/3-Training-a-WSI-Classification-Model-with-ABMIL-and-Heatmaps.ipynb>`_: Training an ABMIL model with attention heatmaps.




================================================
File: docs/cli_helpers/cli_generate.py
================================================
# cli_generate.py

import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from run_batch_of_slides import generate_help_text


if __name__ == "__main__":
    help_text = generate_help_text()
    print(help_text)



================================================
File: tests/test_encoder_same_local_hf.py
================================================
import torch
import numpy as np 
from PIL import Image
import unittest
import json
from pathlib import Path

try:
    import lovely_tensors; lovely_tensors.monkey_patch()
except:
    pass

import sys; sys.path.append('../')
from trident.patch_encoder_models import *


class TestEncoderConsistency(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        cls.dummy_image = Image.fromarray(np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8))

    def _load_encoder(self, encoder_name, source, weights_path=None, **kwargs):
        print(f"  ðŸ”§ Loading {encoder_name} ({source})")
        encoder = encoder_factory(encoder_name, weights_path=weights_path, **kwargs)
        encoder = encoder.to(self.device)
        encoder.eval()
        return encoder

    def _run_forward(self, encoder, encoder_name, source):
        with torch.inference_mode(), torch.amp.autocast('cuda', dtype=encoder.precision):
            dummy_input = encoder.eval_transforms(self.dummy_image).to(self.device).unsqueeze(dim=0)
            output = encoder(dummy_input)
        print(f"  ðŸ“ Output shape from {source}: {tuple(output.shape)}")
        return output

    def _compare_architecture(self, enc1, enc2):
        keys1 = set(enc1.state_dict().keys())
        keys2 = set(enc2.state_dict().keys())
        if keys1 != keys2:
            print("\033[1;33mâš ï¸ Architecture mismatch in keys:\033[0m")
            print("  Only in default :", keys1 - keys2)
            print("  Only in local   :", keys2 - keys1)
            return False
        return True

    def _compare_weights(self, enc1, enc2):
        diffs = []
        for k in enc1.state_dict().keys():
            w1 = enc1.state_dict()[k]
            w2 = enc2.state_dict()[k]
            if not torch.allclose(w1, w2, atol=1e-5, rtol=1e-4):
                abs_diff = (w1 - w2).abs()
                max_diff = abs_diff.max().item()
                mean_diff = abs_diff.mean().item()
                diffs.append((k, max_diff, mean_diff))
        if diffs:
            print("\033[1;33mâš ï¸ Weight differences found:\033[0m")
            for k, max_d, mean_d in sorted(diffs, key=lambda x: -x[1])[:10]:
                print(f"    ðŸ” {k:<50} max diff: {max_d:.4e}, mean diff: {mean_d:.4e}")
            return False
        return True


def generate_encoder_test(encoder_name, weights_path, **kwargs):
    def test(self):
        header = f"ðŸ§ª TEST: {encoder_name}"
        if kwargs:
            kwarg_str = ', '.join(f"{k}={v}" for k, v in kwargs.items())
            header += f" ({kwarg_str})"
        print(f"\n\033[1;36m{'=' * len(header)}\n{header}\n{'=' * len(header)}\033[0m")

        # Load models
        enc_default = self._load_encoder(encoder_name, source="default", **kwargs)
        enc_local = self._load_encoder(encoder_name, source="local checkpoint", weights_path=weights_path, **kwargs)

        # # Compare architecture
        # arch_match = self._compare_architecture(enc_default, enc_local)
        # self.assertTrue(arch_match, f"Architecture mismatch in {encoder_name}")

        # # Compare weights
        # weights_match = self._compare_weights(enc_default, enc_local)
        # self.assertTrue(weights_match, f"Weight mismatch in {encoder_name}")

        # Run inference
        out_default = self._run_forward(enc_default, encoder_name, source="default")
        out_local = self._run_forward(enc_local, encoder_name, source="local checkpoint")

        if torch.allclose(out_default, out_local, atol=1e-5, rtol=1e-4):
            print(f"\033[1;32mâœ… Outputs match for {encoder_name}\033[0m")
        else:
            diff = (out_default - out_local).abs().max().item()
            print(f"\033[1;31mâŒ Outputs do NOT match (max abs diff = {diff:.4e})\033[0m")
            self.fail(f"Output mismatch for {encoder_name} with kwargs={kwargs}")
    return test


# Dynamically register tests before unittest.main()
def register_tests():
    ckpt_path = Path('../trident/patch_encoder_models/local_ckpts_guillaume.json')
    with open(ckpt_path) as f:
        local_ckpts = json.load(f)

    # local ckpt not supported
    local_ckpts.pop('musk')
    local_ckpts.pop('custom_encoder')
    local_ckpts.pop('hibou_l')

    for encoder_name, path in local_ckpts.items():
        test_name = f"test_{encoder_name}"
        test_fn = generate_encoder_test(encoder_name, path)
        setattr(TestEncoderConsistency, test_name, test_fn)


register_tests()

if __name__ == '__main__':
    unittest.main()



================================================
File: tests/test_openslidewsi.py
================================================
import os
import unittest
import torch  # Check for CUDA availability

import sys; sys.path.append('../')
from trident import load_wsi
from trident.segmentation_models import segmentation_model_factory
from trident.patch_encoder_models import encoder_factory

from huggingface_hub import snapshot_download


"""
Test the methods of the OpenSlideWSI object, i.e. bypassing the Processor class.
This is useful if you want to use the OpenSlideWSI class in a custom pipeline.
"""

class TestOpenSlideWSI(unittest.TestCase):
    HF_REPO = "MahmoodLab/unit-testing"
    TEST_SLIDE_FILENAMES = [
        "394140.svs",
        "TCGA-AN-A0XW-01Z-00-DX1.811E11E7-FA67-46BB-9BC6-1FD0106B789D.svs",
        "TCGA-B6-A0IJ-01Z-00-DX1.BF2E062F-06DA-4CA8-86C4-36674C035CAA.svs"
    ]
    TEST_OUTPUT_DIR = "test_single_slide_processing/"
    TEST_PATCH_ENCODER = "uni_v1"
    TEST_MAG = 20
    TEST_PATCH_SIZE = 256

    # Dynamically determine device
    TEST_DEVICE = f"cuda:0" if torch.cuda.is_available() else "cpu"

    @classmethod
    def setUpClass(cls):
        """
        Set up the test environment by downloading slides and creating directories.
        """
        os.makedirs(cls.TEST_OUTPUT_DIR, exist_ok=True)
        cls.local_wsi_dir = snapshot_download(
            repo_id=cls.HF_REPO,
            repo_type='dataset',
            local_dir=os.path.join(cls.TEST_OUTPUT_DIR, 'wsis'),
            allow_patterns=['*']
        )

    def test_integration(self):
        """
        Test all processing methods of OpenSlideWSI end-to-end with real slides.
        """
        for slide_filename in self.TEST_SLIDE_FILENAMES:
            with self.subTest(slide=slide_filename):
                slide_path = os.path.join(self.local_wsi_dir, slide_filename)
                slide = load_wsi(slide_path=slide_path, lazy_init=False)

                # Step 1: Tissue segmentation
                segmentation_model = segmentation_model_factory("hest")
                slide.segment_tissue(segmentation_model=segmentation_model, target_mag=10, job_dir=self.TEST_OUTPUT_DIR, device=self.TEST_DEVICE)

                # Step 2: Tissue coordinate extraction
                coords_path = slide.extract_tissue_coords(
                    target_mag=self.TEST_MAG,
                    patch_size=self.TEST_PATCH_SIZE,
                    save_coords=self.TEST_OUTPUT_DIR
                )

                # Step 3: Visualization
                viz_coords_path = slide.visualize_coords(
                    coords_path=coords_path,
                    save_patch_viz=os.path.join(self.TEST_OUTPUT_DIR, "visualization")
                )

                # Step 4: Feature extraction
                encoder = encoder_factory(self.TEST_PATCH_ENCODER)
                encoder.eval()
                encoder.to(self.TEST_DEVICE)
                features_dir = os.path.join(self.TEST_OUTPUT_DIR, f"features_{self.TEST_PATCH_ENCODER}")
                slide.extract_patch_features(
                    patch_encoder=encoder,
                    coords_path=coords_path,
                    save_features=features_dir,
                    device=self.TEST_DEVICE
                )

                # Verify outputs
                self.assertTrue(os.path.exists(os.path.join(self.TEST_OUTPUT_DIR, "contours_geojson")), "GDF contours were not saved.")
                self.assertTrue(os.path.exists(os.path.join(self.TEST_OUTPUT_DIR, "contours")), "Contours were not saved.")
                self.assertTrue(os.path.exists(coords_path), "Tissue coordinates file was not saved.")
                self.assertTrue(os.path.exists(viz_coords_path), "Visualization file was not saved.")
                self.assertTrue(os.path.exists(features_dir), "Feature extraction results were not saved.")

        expected_file_count = len(self.TEST_SLIDE_FILENAMES)
        output_dirs = [
            "visualization",
            "thumbnails",
            "patches",
            "contours",
            "contours_geojson",
            f"features_{self.TEST_PATCH_ENCODER}"
        ]

        for output_dir in output_dirs:
            dir_path = os.path.join(self.TEST_OUTPUT_DIR, output_dir)
            self.assertTrue(os.path.exists(dir_path), f"Directory '{output_dir}' does not exist.")
            files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]
            self.assertEqual(
                len(files), expected_file_count,
                f"Expected {expected_file_count} files in '{output_dir}', but found {len(files)}."
            )

if __name__ == "__main__":
    unittest.main()



================================================
File: tests/test_patch_encoders.py
================================================
import torch
import numpy as np 
from PIL import Image
import unittest
try:
    import lovely_tensors; lovely_tensors.monkey_patch()
except:
    pass

import sys; sys.path.append('../')
from trident.patch_encoder_models import * 

"""
Test forward pass of patch encoders
"""

class TestPatchEncoders(unittest.TestCase):

    def setUp(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.dummy_image = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)
        self.dummy_image = Image.fromarray(self.dummy_image)

    def _test_encoder_forward(self, encoder_name, **kwargs):
        print("\033[95m" + f"Testing {encoder_name} forward pass" + "\033[0m")
        if kwargs:
            print("\033[92m" + f"    With kwargs: {kwargs}" + "\033[0m")
        encoder = encoder_factory(encoder_name, **kwargs)
        encoder = encoder.to(self.device)
        encoder.eval()

        with torch.inference_mode(), torch.amp.autocast('cuda', dtype=encoder.precision):
            dummy_input = encoder.eval_transforms(self.dummy_image).to(self.device).unsqueeze(dim=0)
            output = encoder(dummy_input)

        self.assertIsNotNone(output)
        self.assertIsInstance(output, torch.Tensor)
        print("\033[94m"+ f"    {encoder_name} forward pass success with output {output}" + "\033[0m")

    def test_conch_v1_forward(self):
        self._test_encoder_forward('conch_v1', with_proj = True, normalize = True)
        self._test_encoder_forward('conch_v1', with_proj = False, normalize = True)
        self._test_encoder_forward('conch_v1', with_proj = True, normalize = False)
        self._test_encoder_forward('conch_v1', with_proj = False, normalize = False)
        
    def test_conch_v15_forward(self):
        self._test_encoder_forward('conch_v15')

    def test_uni_v1_forward(self):
        self._test_encoder_forward('uni_v1')
        
    def test_uni_v2_forward(self):
        self._test_encoder_forward('uni_v2')

    def test_ctranspath_forward(self):
        self._test_encoder_forward('ctranspath')

    def test_phikon_forward(self):
        self._test_encoder_forward('phikon')
    
    def test_phikon_v2_forward(self):
        self._test_encoder_forward('phikon_v2')

    def test_resnet50_forward(self):
        self._test_encoder_forward('resnet50')

    def test_gigapath_forward(self):
        self._test_encoder_forward('gigapath')

    def test_virchow_forward(self):
        self._test_encoder_forward('virchow')

    def test_virchow2_forward(self):
        self._test_encoder_forward('virchow2')

    def test_hoptimus0_forward(self):
        self._test_encoder_forward('hoptimus0')

    def test_hoptimus1_forward(self):
        self._test_encoder_forward('hoptimus1')

    def test_musk_forward(self):
        self._test_encoder_forward('musk')
    
    def test_hibou_l_forward(self):
        self._test_encoder_forward('hibou_l')
    
    def test_kaiko_forward(self):
        self._test_encoder_forward('kaiko-vits8')
        self._test_encoder_forward('kaiko-vits16')
        self._test_encoder_forward('kaiko-vitb8')
        self._test_encoder_forward('kaiko-vitb16')
        self._test_encoder_forward('kaiko-vitl14')
        
    def test_lunitvits8_forward(self):
        self._test_encoder_forward('lunit-vits8')
    
    def test_midnight12k_forward(self):
        self._test_encoder_forward('midnight12k')
        self._test_encoder_forward('midnight12k', return_type="cls+mean")

if __name__ == '__main__':
    unittest.main()


================================================
File: tests/test_processor.py
================================================
import os
import unittest
import sys
sys.path.append('../')
from trident import Processor
from trident.segmentation_models import segmentation_model_factory
from trident.patch_encoder_models import encoder_factory

from huggingface_hub import snapshot_download

"""
Unit tests for the Processor class, which is responsible for handling segmentation, patching, and feature extraction on whole slide images (WSIs).

Attributes:
    HF_REPO (str): The Hugging Face repository ID for downloading test data.
    TEST_OUTPUT_DIR (str): The directory where test outputs will be stored.
    TEST_WSI_EXT (list): List of file extensions for WSIs.
    TEST_GPU_INDEX (int): The index of the GPU to be used for testing.
    TEST_PATCH_ENCODER (str): The name of the patch encoder model to be used.
    TEST_MAG (int): The target magnification for patching.
    TEST_PATCH_SIZE (int): The size of the patches to be extracted.
    TEST_OVERLAP (int): The overlap between patches.

Methods:
    setUpClass(cls): Set up the test environment by creating directories and preparing mock data.
    setUp(self): Initialize the Processor instance for each test.
    test_tissue_processing(self): Test the processing tasks including segmentation, patching, and feature extraction.
    tearDown(self): Clean up after each test.
    tearDownClass(cls): Remove the test output directory after all tests are done.
"""

class TestProcessor(unittest.TestCase):
    HF_REPO = "MahmoodLab/unit-testing"
    TEST_OUTPUT_DIR = "test_processor_output/"
    TEST_WSI_EXT = [".svs", ".tif"]
    TEST_GPU_INDEX = 0
    TEST_PATCH_ENCODER = "uni_v1"
    TEST_MAG = 20
    TEST_PATCH_SIZE = 256
    TEST_OVERLAP = 0

    @classmethod
    def setUpClass(cls):
        """
        Set up the test environment by creating directories and preparing mock data.
        """
        os.makedirs(cls.TEST_OUTPUT_DIR, exist_ok=True)
        cls.local_wsi_dir = snapshot_download(
            repo_id=cls.HF_REPO,
            repo_type='dataset',
            local_dir=os.path.join(cls.TEST_OUTPUT_DIR, 'wsis'),
            allow_patterns=['*svs']
        )

    def setUp(self):
        """
        Initialize the Processor instance for each test.
        """
        self.custom_list_of_wsis = snapshot_download(
            repo_id=self.HF_REPO,
            repo_type='dataset',
            local_dir=os.path.join(self.TEST_OUTPUT_DIR),
            allow_patterns=['*csv']
        )

    def test_processor_with_wsis(self):
        """
        Test the process constructor when processing a custom list of WSIs.
        """

        self.processor = Processor(
            job_dir=self.TEST_OUTPUT_DIR,
            wsi_source=os.path.join(TestProcessor.TEST_OUTPUT_DIR),
            wsi_ext=self.TEST_WSI_EXT,
            custom_list_of_wsis=os.path.join(self.custom_list_of_wsis, 'valid_list_of_wsis.csv')
        )

    def test_tissue_processing(self):
        """
        Test the processing tasks end-to-end including segmentation, patching, and feature extraction on a set of real WSIs.
        """

        self.processor = Processor(
            job_dir=self.TEST_OUTPUT_DIR,
            wsi_source=os.path.join(TestProcessor.TEST_OUTPUT_DIR, 'wsis'),
            wsi_ext=self.TEST_WSI_EXT
        )

        segmentation_model = segmentation_model_factory('hest')
        self.processor.run_segmentation_job(
            segmentation_model=segmentation_model,
            seg_mag=5,
            device=f'cuda:{self.TEST_GPU_INDEX}'
        )
        output_dirs = ["contours", "contours_geojson"]
        for dir_name in output_dirs:
            dir_path = os.path.join(self.TEST_OUTPUT_DIR, dir_name)
            self.assertTrue(os.path.exists(dir_path), f"Segmentation output directory '{dir_name}' does not exist.")

        self.processor.run_patching_job(
            target_magnification=self.TEST_MAG,
            patch_size=self.TEST_PATCH_SIZE
        )
        coords_dir = f"{self.TEST_MAG}x_{self.TEST_PATCH_SIZE}px_{self.TEST_OVERLAP}px_overlap"

        encoder = encoder_factory(self.TEST_PATCH_ENCODER)
        encoder.eval()
        encoder.to(f"cuda:{self.TEST_GPU_INDEX}")
        self.processor.run_patch_feature_extraction_job(
            coords_dir=coords_dir,
            patch_encoder=encoder,
            device=f'cuda:{self.TEST_GPU_INDEX}',
            saveas="h5"
        )
        features_dir = os.path.join(self.TEST_OUTPUT_DIR, coords_dir, f"features_{self.TEST_PATCH_ENCODER}")
        self.assertTrue(len(os.listdir(features_dir)) > 0, "Feature extraction results are missing.")

    def tearDown(self):
        pass

    @classmethod
    def tearDownClass(cls):
        """
        Remove the test output directory.
        """
        pass
        # if os.path.exists(cls.TEST_OUTPUT_DIR):
        #     import shutil
        #     shutil.rmtree(cls.TEST_OUTPUT_DIR)

if __name__ == "__main__":
    unittest.main()



================================================
File: tests/test_segmentation_models.py
================================================
import torch
import numpy as np 
from PIL import Image
import unittest

try:
    import lovely_tensors; lovely_tensors.monkey_patch()
except:
    pass

import sys; sys.path.append('../')
from trident.segmentation_models import segmentation_model_factory 

"""
Test forward pass of the segmentation model(s).
"""

class TestSegmentationModels(unittest.TestCase):

    def setUp(self):
        pass

    def _test_forward(self, encoder_name):
        print("\033[95m" + f"Testing {encoder_name} forward pass" + "\033[0m")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        encoder = segmentation_model_factory(encoder_name).to(device)

        self.dummy_image = np.random.randint(0, 256, (encoder.input_size, encoder.input_size, 3), dtype=np.uint8)
        self.dummy_image = Image.fromarray(self.dummy_image)

        with torch.inference_mode():
            dummy_input = encoder.eval_transforms(self.dummy_image).unsqueeze(dim=0).to(device)
            output = encoder(dummy_input)

        self.assertIsNotNone(output)
        self.assertIsInstance(output, torch.Tensor)
        print("\033[94m"+ f"    {encoder_name} forward pass success with output {output}" + "\033[0m")

    def test_hest(self):
        self._test_forward('hest')
        
    def test_grandqc(self):
        self._test_forward('grandqc')

if __name__ == '__main__':
    unittest.main()



================================================
File: tests/test_slide_encoders.py
================================================
import unittest
import torch

import sys; sys.path.append('../')
from trident.slide_encoder_models import *

"""
Test the forward pass of the slide encoders.
"""

class TestSlideEncoders(unittest.TestCase):

    def setUp(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def _test_encoder_forward(self, encoder, batch, expected_precision):
        print("\033[95m" + f"Testing {encoder.__class__.__name__} forward pass" + "\033[0m")
        encoder = encoder.to(self.device)
        encoder.eval()
        self.assertEqual(encoder.precision, expected_precision)
        self.assertTrue(hasattr(encoder, 'model'))

        with torch.inference_mode(), torch.amp.autocast('cuda', dtype=encoder.precision):
            output = encoder.forward(batch, device=self.device)

        self.assertIsNotNone(output)
        self.assertIsInstance(output, torch.Tensor)
        self.assertTrue(output.shape[-1] == encoder.embedding_dim)
        print("\033[94m"+ f"    {encoder.__class__.__name__} forward pass success with output shape {output.shape}" + "\033[0m")

    def test_prism_encoder_initialization(self):
        sample_batch = {
            'features': torch.randn(1, 100, 2560),
            'coords': torch.randn(1, 100, 2),
        }
        self._test_encoder_forward(PRISMSlideEncoder(), sample_batch, torch.float16)

    def test_chief_encoder_initialization(self):
        sample_batch = {
            'features': torch.randn(1, 100, 768),
        }
        self._test_encoder_forward(CHIEFSlideEncoder(), sample_batch, torch.float32)

    def test_titan_encoder_initialization(self):
        sample_batch = {
            'features': torch.randn(1, 100, 768),
            'coords': torch.randint(0, 4096, (1, 100, 2)),
            'attributes': {'patch_size_level0': 512}
        }
        self._test_encoder_forward(TitanSlideEncoder(), sample_batch, torch.float16)
        
    def test_gigapath_encoder_initialization(self):
        sample_batch = {
            'features': torch.randn(1, 100, 1536),
            'coords': torch.randn(1, 100, 2),
            'attributes': {'patch_size_level0': 224}
        }
        self._test_encoder_forward(GigaPathSlideEncoder(), sample_batch, torch.float16)

    def test_slide_encoder_factory_with_valid_names(self):
        print("\033[95m" + "Testing Slide Encoder Factory with valid names" + "\033[0m")
        # Test factory method for valid model names
        for model_name, expected_class in [
            ('mean-conch_v15', MeanSlideEncoder),
            ('mean-blahblah', MeanSlideEncoder),
            ('prism', PRISMSlideEncoder),
            ('chief', CHIEFSlideEncoder),
            ('gigapath', GigaPathSlideEncoder),
            ('titan', TitanSlideEncoder),
            ('madeleine', MadeleineSlideEncoder),
        ]:
            encoder = encoder_factory(model_name)
            self.assertIsInstance(encoder, expected_class)

    def test_madeleine_encoder_initialization(self):
        sample_batch = {
            'features': torch.randn(1, 100, 512),
        }
        self._test_encoder_forward(MadeleineSlideEncoder(), sample_batch, torch.bfloat16)

    def test_slide_encoder_factory_invalid_name(self):
        print("\033[95m" + "Testing Slide Encoder Factory with invalid names" + "\033[0m")
        with self.assertRaises(ValueError):
            encoder_factory('invalid-model')


if __name__ == "__main__":
    unittest.main()



================================================
File: trident/Concurrency.py
================================================
import os
import gc
import torch
import shutil
from typing import List, Callable
from queue import Queue



def cache_batch(wsis: List[str], dest_dir: str) -> List[str]:
    """
    Copies WSIs to a local cache directory. Handles .mrxs subdirectories if present.

    Returns:
        List[str]: Paths to copied WSIs.
    """
    os.makedirs(dest_dir, exist_ok=True)
    copied = []

    for wsi in wsis:
        dest_path = os.path.join(dest_dir, os.path.basename(wsi))
        shutil.copy(wsi, dest_path)
        copied.append(dest_path)

        if wsi.endswith('.mrxs'):
            mrxs_dir = os.path.splitext(wsi)[0]
            if os.path.exists(mrxs_dir):
                dest_mrxs_dir = os.path.join(dest_dir, os.path.basename(mrxs_dir))
                shutil.copytree(mrxs_dir, dest_mrxs_dir)

    return copied


def batch_producer(
    queue: Queue,
    valid_slides: List[str],
    start_idx: int,
    batch_size: int,
    cache_dir: str,
) -> None:
    """
    Produces and caches batches of slides. Sends batch IDs to a queue for downstream processing.

    Args:
        queue (Queue): Queue to communicate with the consumer.
        valid_slides (List[str]): List of valid WSI paths.
        start_idx (int): Index in `valid_slides` to start batching from.
        batch_size (int): Number of slides per batch.
        cache_dir (str): Root directory where batches will be cached.
    """
    for i in range(start_idx, len(valid_slides), batch_size):
        batch_paths = valid_slides[i:i + batch_size]
        batch_id = i // batch_size
        ssd_batch_dir = os.path.join(cache_dir, f"batch_{batch_id}")
        print(f"[PRODUCER] Caching batch {batch_id}: {ssd_batch_dir}")
        cache_batch(batch_paths, ssd_batch_dir)
        queue.put(batch_id)

    queue.put(None)  # Sentinel to signal completion


def batch_consumer(
    queue: Queue,
    task: str,
    cache_dir: str,
    processor_factory: Callable[[str], object],
    run_task_fn: Callable[[object, str], None],
) -> None:
    """
    Consumes cached batches from the queue, processes them, and optionally clears cache.

    Args:
        queue (Queue): Queue from the producer.
        task (str): Task name ('seg', 'coords', 'feat', or 'all').
        cache_dir (str): Directory containing cached batches.
        processor_factory (Callable): Function that creates a processor given a WSI dir.
        run_task_fn (Callable): Function to run a task given a processor and task name.
    """

    while True:
        batch_id = queue.get()
        if batch_id is None:
            queue.task_done()
            break

        ssd_batch_dir = os.path.join(cache_dir, f"batch_{batch_id}")
        print(f"[CONSUMER] Processing batch {batch_id}: {ssd_batch_dir}")

        processor = processor_factory(ssd_batch_dir)

        try:
            if task == 'all':
                for subtask in ['seg', 'coords', 'feat']:
                    run_task_fn(processor, subtask)
            else:
                run_task_fn(processor, task)
        finally:
            # release all WSI and processor resources
            if hasattr(processor, "release"):
                processor.release()
            del processor
            gc.collect()
            torch.cuda.empty_cache()

            print(f"[CONSUMER] Clearing cache for batch {batch_id}")
            shutil.rmtree(ssd_batch_dir, ignore_errors=True)
            queue.task_done()



================================================
File: trident/Converter.py
================================================
from PIL import Image
import numpy as np
import os
import pandas as pd
from tqdm import tqdm

Image.MAX_IMAGE_PIXELS = None

# Bioformats
BIOFORMAT_EXTENSIONS = {
    '.tif', '.tiff', '.ndpi', '.svs', '.lif', '.ims', '.vsi', '.bif', '.btf',
    '.mrxs', '.scn', '.ome.tiff', '.ome.tif', '.h5', '.hdf', '.hdf5', '.he5',
    '.dicom', '.dcm', '.ome.xml', '.zvi', '.pcoraw', '.jp2', '.qptiff', '.nrrd', '.ome.btf', '.fg7'
}

# PIL
PIL_EXTENSIONS = {'.png', '.jpg', '.jpeg'}

# OpenSlide
OPENSLIDE_EXTENSIONS = {'.svs', '.tif', '.dcm', '.vms', '.vmu', '.ndpi', '.scn', '.mrxs', '.tiff', '.svslide', '.bif', '.czi'}

# Combined with CZI 
SUPPORTED_EXTENSIONS = BIOFORMAT_EXTENSIONS | PIL_EXTENSIONS | {'.czi'}



class AnyToTiffConverter:
    """
    A class to convert images to TIFF format with options for resizing and pyramidal tiling.
    
    Attributes:
        job_dir (str): Directory to save converted images.
        bigtiff (bool): Flag to enable the creation of BigTIFF files.
    """
    def __init__(self, job_dir: str, bigtiff: bool = False):
        """
        Initializes the Converter with a job directory and BigTIFF support.

        Args:
            job_dir (str): The directory where converted images will be saved.
            bigtiff (bool): Enable or disable BigTIFF file creation.
        """
        self.job_dir = job_dir
        self.bigtiff = bigtiff
        os.makedirs(job_dir, exist_ok=True)

    def process_file(self, input_file: str, mpp: float, zoom: float) -> None:
        """
        Process a single image file to convert it into TIFF format.

        Args:
            input_file (str): Path to the input image file.
            mpp (float): Microns per pixel value for the output image.
            zoom (float): Zoom factor for image resizing, e.g., 0.5 is reducing the image by a factor.
        """
        try:
            img_name = os.path.splitext(os.path.basename(input_file))[0]
            img = self._read_image(input_file, zoom)
            self._save_tiff(img, img_name, mpp * (1/zoom))
        except Exception as e:
            print(f"Error processing {input_file}: {e}")

    def _read_image(self, file_path: str, zoom: float = 1) -> np.ndarray:
        """
        Read and resize an image from the given path.

        Args:
            file_path (str): Path to the image file.
            zoom (float): Zoom factor for resizing, e.g., 0.5 is reducing the image by a factor.

        Returns:
            np.ndarray: Array representing the resized image.
        """
        if file_path.endswith('.czi'):
            try:
                import pylibCZIrw.czi as pyczi
            except ImportError:
                raise ImportError("pylibCZIrw is required for CZI files. Install it with pip install pylibCZIrw.")
            with pyczi.open_czi(file_path) as czidoc:
                return czidoc.read(zoom=zoom)
        if file_path.lower().endswith(tuple(BIOFORMAT_EXTENSIONS)):
            try:
                from valis_hest.slide_io import BioFormatsSlideReader
            except ImportError:
                raise ImportError("Install valis_hest with `pip install valis_hest` and JVM with `sudo apt-get install maven`.")
            reader = BioFormatsSlideReader(file_path) 
            reader.create_metadata()
            img = reader.slide2image(level=int(1/zoom)-1)  # @TODO: Assumes each level 2x small than the higher one.
            return img
        else:
            with Image.open(file_path) as img:
                new_size = (int(img.width * zoom), int(img.height * zoom))
                img_resized = img.resize(new_size, Image.Resampling.LANCZOS)
                return np.array(img_resized)

    def _get_mpp(self, mpp_data: pd.DataFrame, input_file: str) -> float:
        """
        Retrieve the MPP (Microns per Pixel) value for a specific file from a DataFrame.

        Args:
            mpp_data (pd.DataFrame): DataFrame containing MPP values.
            input_file (str): Filename to search for in the DataFrame.

        Returns:
            float: MPP value for the file.
        """
        filename = os.path.basename(input_file)
        mpp_row = mpp_data.loc[mpp_data['wsi'] == filename, 'mpp']
        if mpp_row.empty:
            raise ValueError(f"No MPP found for {filename} in CSV.")
        return float(mpp_row.values[0])

    def _save_tiff(self, img: np.ndarray, img_name: str, mpp: float) -> None:
        """
        Save an image as a pyramidal TIFF image.

        Args:
            img (np.ndarray): Image data to save as a numpy array.
            img_name (str): Image name (without extensions). 
            mpp (float): Microns per pixel value of the output TIFF image.
        """
        save_path = os.path.join(self.job_dir, f"{img_name}.tiff")
        try:
            import pyvips
        except ImportError:
            raise ImportError("pyvips is required for saving pyramidal TIFFs. Install it with pip install pyvips.")
        pyvips_img = pyvips.Image.new_from_array(img)
        pyvips_img.tiffsave(
            save_path,
            bigtiff=self.bigtiff,
            pyramid=True,
            tile=True,
            tile_width=256,
            tile_height=256,
            compression='jpeg',
            resunit=pyvips.enums.ForeignTiffResunit.CM,
            xres=1. / (mpp * 1e-4),
            yres=1. / (mpp * 1e-4)
        )

    def process_all(self, input_dir: str, mpp_csv: str, downscale_by: int = 1) -> None:
        """
        Process all eligible image files in a directory to convert them to pyramidal TIFF.

        Args:
            input_dir (str): Directory containing image files to process.
            mpp_csv (str): Path to a CSV file with 2 field: "wsi" with fnames with extensions and "mpp" with the micron per pixel values.
            downscale_by (int): Factor to downscale images by, e.g., to save a 40x image into a 20x one, set downscale_by to 2. 
        """
        files = [f for f in os.listdir(input_dir) if f.lower().endswith(tuple(SUPPORTED_EXTENSIONS))]
        mpp_df = pd.read_csv(mpp_csv)
        for filename in tqdm(files, desc="Processing images"):
            img_path = os.path.join(input_dir, filename)
            mpp = self._get_mpp(mpp_df, img_path)
            try:
                with Image.open(img_path) as img:
                    size = img.size
            except Exception:
                size = "Unknown"
            tqdm.write(f"Processing {filename} | Size: {size}")
            self.process_file(img_path, mpp, zoom=1/downscale_by)

        #clean up 
        try:
            from valis_hest import slide_io
            slide_io.kill_jvm() 
        except:
            pass


if __name__ == "__main__":

    # Example usage. Still experimental. Coverage could be improved.
    converter = AnyToTiffConverter(job_dir='./pyramidal_tiff', bigtiff=False)

    # Convert all images in the dir "../pngs" with mpp specified in to_process.csv. TIFF are saved at the original pixel res.
    converter.process_all(input_dir='../wsis/', mpp_csv='../pngs/to_process.csv', downscale_by=1)

    # Example of to_process.csv specifying the mpp of all WSIs in the dir "../wsis"
    # wsi,mpp
    # 3756144.svs,0.25
    # 4290019.svs,0.25
    # 619709.svs,0.25


================================================
File: trident/IO.py
================================================
from __future__ import annotations

import torch
import socket
import os
import json
from typing import List, Optional, Union, Tuple
import h5py
import numpy as np
import cv2
import pandas as pd
from geopandas import gpd
from shapely import Polygon


ENV_TRIDENT_HOME = "TRIDENT_HOME"
ENV_XDG_CACHE_HOME = "XDG_CACHE_HOME"
DEFAULT_CACHE_DIR = "~/.cache"
_cache_dir: Optional[str] = None


def collect_valid_slides(
    wsi_dir: str,
    custom_list_path: Optional[str] = None,
    wsi_ext: Optional[List[str]] = None,
    search_nested: bool = False,
    max_workers: int = 8,
    return_relative_paths: bool = False
) -> Union[List[str], Tuple[List[str], List[str]]]:
    """
    Retrieve all valid WSI file paths from a directory, optionally filtered by a custom list.

    Args:
        wsi_dir (str): Path to the directory containing WSIs.
        custom_list_path (Optional[str]): Path to a CSV file with 'wsi' column of relative slide paths.
        wsi_ext (Optional[List[str]]): Allowed file extensions.
        search_nested (bool): Whether to search subdirectories.
        max_workers (int): Threads to use when checking file existence.
        return_relative_paths (bool): Whether to also return relative paths.

    Returns:
        List[str]: Full paths to valid WSIs.
        OR
        Tuple[List[str], List[str]]: (full paths, relative paths)
    
    Raises:
        ValueError: If custom CSV is invalid or files not found.
    """
    valid_rel_paths: List[str] = []

    if custom_list_path is not None:
        from concurrent.futures import ThreadPoolExecutor

        wsi_df = pd.read_csv(custom_list_path)
        if 'wsi' not in wsi_df.columns:
            raise ValueError("CSV must contain a column named 'wsi'.")

        rel_paths = wsi_df['wsi'].dropna().astype(str).tolist()
        if not rel_paths:
            raise ValueError(f"No valid slides found in the custom list at {custom_list_path}.")

        def exists_fn(rel_path: str) -> bool:
            return os.path.exists(os.path.join(wsi_dir, rel_path))

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(exists_fn, rel_paths))

        for rel_path, exists in zip(rel_paths, results):
            if not exists:
                raise ValueError(
                    f"Slide '{rel_path}' not found in '{wsi_dir}'. "
                    "If the folder is nested, ensure 'wsi' column contains relative paths."
                )

        valid_rel_paths = rel_paths

    else:
        if wsi_ext is None:
            from trident.Converter import PIL_EXTENSIONS, OPENSLIDE_EXTENSIONS
            wsi_ext = list(PIL_EXTENSIONS) + list(OPENSLIDE_EXTENSIONS)

        wsi_ext = [ext.lower() for ext in wsi_ext]

        def matches_ext(filename: str) -> bool:
            return any(filename.lower().endswith(ext) for ext in wsi_ext)

        if search_nested:
            for root, _, files in os.walk(wsi_dir):
                for f in files:
                    if matches_ext(f):
                        rel_path = os.path.relpath(os.path.join(root, f), wsi_dir)
                        valid_rel_paths.append(rel_path)
        else:
            valid_rel_paths = [
                f for f in os.listdir(wsi_dir)
                if matches_ext(f)
            ]

        valid_rel_paths.sort()

    full_paths = [os.path.join(wsi_dir, rel) for rel in valid_rel_paths]

    return (full_paths, valid_rel_paths) if return_relative_paths else full_paths


def get_dir() -> str:
    r"""
    Get Trident cache directory used for storing downloaded models & weights.
    If :func:`~trident.hub.set_dir` is not called, default path is ``$TRIDENT_HOME`` where
    environment variable ``$TRIDENT_HOME`` defaults to ``$XDG_CACHE_HOME/torch``.
    ``$XDG_CACHE_HOME`` follows the X Design Group specification of the Linux
    filesystem layout, with a default value ``~/.cache`` if the environment
    variable is not set.
    """

    if _cache_dir is not None:
        return _cache_dir
    return _get_trident_home()


def set_dir(d: Union[str, os.PathLike]) -> None:
    r"""
    Optionally set the Trident cache directory used to save downloaded models & weights.
    Args:
        d (str): path to a local folder to save downloaded models & weights.
    """
    global _cache_dir
    _cache_dir = os.path.expanduser(d)


def _get_trident_home():
    trident_home = os.path.expanduser(
        os.getenv(
            ENV_TRIDENT_HOME,
            os.path.join(os.getenv(ENV_XDG_CACHE_HOME, DEFAULT_CACHE_DIR), "trident"),
        )
    )
    return trident_home


def has_internet_connection(timeout=3.0) -> bool:
    endpoint = os.environ.get("HF_ENDPOINT", "huggingface.co")
    
    if endpoint.startswith(("http://", "https://")):
        from urllib.parse import urlparse
        endpoint = urlparse(endpoint).netloc
    
    try:
        # Fast socket-level check
        socket.create_connection((endpoint, 443), timeout=timeout)
        return True
    except OSError:
        pass

    try:
        # Fallback HTTP-level check (if requests is available)
        import requests
        url = f"https://{endpoint}" if not endpoint.startswith(("http://", "https://")) else endpoint
        r = requests.head(url, timeout=timeout)
        return r.status_code < 500
    except Exception:
        return False


def get_weights_path(model_type, encoder_name):
    """
    Retrieve the path to the weights file for a given model name.
    This function looks up the path to the weights file in a local checkpoint
    registry (local_ckpts.json). If the path in the registry is absolute, it
    returns that path. If the path is relative, it joins the relative path with
    the provided weights_root directory.
    Args:
        weights_root (str): The root directory where weights files are stored.
        name (str): The name of the model whose weights path is to be retrieved.
    Returns:
        str: The absolute path to the weights file.
    """

    assert model_type in ['patch', 'slide', 'seg'], f"Encoder type must be 'patch' or 'slide' or 'seg', not '{model_type}'"

    if model_type == 'patch' or model_type == 'slide':
        root = os.path.join(os.path.dirname(__file__), f"{model_type}_encoder_models")
    else:
        root = os.path.join(os.path.dirname(__file__), "segmentation_models")

    registry_path = os.path.join(root, "local_ckpts.json")
    with open(registry_path, "r") as f:
        registry = json.load(f)

    path = registry.get(encoder_name)    
    if path:
        path = path if os.path.isabs(path) else os.path.abspath(os.path.join(root, 'model_zoo', path)) # Make path absolute
        if not os.path.exists(path):
            path = ""

    return path


def create_lock(path, suffix = None):
    """
    The `create_lock` function creates a lock file to signal that a particular file or process 
    is currently being worked on. This is especially useful in multiprocessing or distributed 
    systems to avoid conflicts or multiple processes working on the same resource.

    Parameters:
    -----------
    path : str
        The path to the file or resource being locked.
    suffix : str, optional
        An additional suffix to append to the lock file name. This allows for creating distinct 
        lock files for similar resources. Defaults to None.

    Returns:
    --------
    None
        The function creates a `.lock` file in the specified path and does not return anything.

    Example:
    --------
    >>> create_lock("/path/to/resource")
    >>> # Creates a file named "/path/to/resource.lock" to indicate the resource is locked.
    """
    if suffix is not None:
        path = f"{path}_{suffix}"
    lock_file = f"{path}.lock"
    with open(lock_file, 'w') as f:
        f.write("")

#####################

def remove_lock(path, suffix = None):
    """
    The `remove_lock` function removes a lock file, signaling that the file or process 
    is no longer in use and is available for other operations.

    Parameters:
    -----------
    path : str
        The path to the file or resource whose lock needs to be removed.
    suffix : str, optional
        An additional suffix to identify the lock file. Defaults to None.

    Returns:
    --------
    None
        The function deletes the `.lock` file associated with the resource.

    Example:
    --------
    >>> remove_lock("/path/to/resource")
    >>> # Removes the file "/path/to/resource.lock", indicating the resource is unlocked.
    """
    if suffix is not None:
        path = f"{path}_{suffix}"
    lock_file = f"{path}.lock"
    os.remove(lock_file)

#####################

def is_locked(path, suffix = None):
    """
    The `is_locked` function checks if a resource is currently locked by verifying 
    the existence of a `.lock` file.

    Parameters:
    -----------
    path : str
        The path to the file or resource to check for a lock.
    suffix : str, optional
        An additional suffix to identify the lock file. Defaults to None.

    Returns:
    --------
    bool
        True if the `.lock` file exists, indicating the resource is locked. False otherwise.

    Example:
    --------
    >>> is_locked("/path/to/resource")
    False
    >>> create_lock("/path/to/resource")
    >>> is_locked("/path/to/resource")
    True
    """
    if suffix is not None:
        path = f"{path}_{suffix}"
    return os.path.exists(f"{path}.lock")


###########################################################################
def update_log(path_to_log, key, message):
    """
    The `update_log` function appends or updates a message in a log file. It is useful for tracking 
    progress or recording errors during a long-running process.

    Parameters:
    -----------
    path_to_log : str
        The path to the log file where messages will be written.
    key : str
        A unique identifier for the log entry, such as a slide name or file ID.
    message : str
        The message to log, such as a status update or error message.

    Returns:
    --------
    None
        The function writes to the log file in-place.

    Example:
    --------
    >>> update_log("processing.log", "slide1", "Processing completed")
    >>> # Appends or updates "slide1: Processing completed" in the log file.
    """    
    # Create log if it doesn't exist
    if not os.path.exists(path_to_log):
        with open(path_to_log, 'w') as f:
            f.write(f'{key}: {message}\n')
            return
        
    # If slide id already in log, delete the message and add the new one
    if os.path.exists(path_to_log):
        with open(path_to_log, 'r') as f:
            lines = f.readlines()
        with open(path_to_log, 'w') as f:
            for line in lines:
                if not line.split(':')[0] == key:
                    f.write(line)
            f.write(f'{key}: {message}\n')
        return
    
################################################################################

def save_h5(save_path, assets, attributes = None, mode = 'w'):
    """
    The `save_h5` function saves a dictionary of assets to an HDF5 file. This is commonly used to store 
    large datasets or hierarchical data structures in a compact and organized format.

    Parameters:
    -----------
    save_path : str
        The path where the HDF5 file will be saved.
    assets : dict
        A dictionary containing the data to save. Keys represent dataset names, and values are NumPy arrays.
    attributes : dict, optional
        A dictionary mapping dataset names to additional metadata (attributes) to save alongside the data. Defaults to None.
    mode : str, optional
        The file mode for opening the HDF5 file. Options include 'w' (write) and 'a' (append). Defaults to 'w'.

    Returns:
    --------
    None
        The function writes data and attributes to the specified HDF5 file.

    Example:
    --------
    >>> assets = {'data': np.array([1, 2, 3]), 'labels': np.array([0, 1, 1])}
    >>> attributes = {'data': {'description': 'Numerical data'}}
    >>> save_h5("output.h5", assets, attributes)
    >>> # Saves datasets and attributes to "output.h5".
    """

    with h5py.File(save_path, mode) as file:
        for key, val in assets.items():
            data_shape = val.shape
            if key not in file:
                data_type = val.dtype
                chunk_shape = (1, ) + data_shape[1:]
                maxshape = (None, ) + data_shape[1:]
                dset = file.create_dataset(key, shape=data_shape, maxshape=maxshape, chunks=chunk_shape, dtype=data_type)
                dset[:] = val
                if attributes is not None:
                    if key in attributes.keys():
                        for attr_key, attr_val in attributes[key].items():
                            try:
                                # Serialize if the attribute value is a dictionary
                                if isinstance(attr_val, dict):
                                    attr_val = json.dumps(attr_val)
                                # Serialize Nones
                                elif attr_val is None:
                                    attr_val = 'None'
                                dset.attrs[attr_key] = attr_val
                            except:
                                raise Exception(f'WARNING: Could not save attribute {attr_key} with value {attr_val} for asset {key}')
                                
            else:
                dset = file[key]
                dset.resize(len(dset) + data_shape[0], axis=0)
                dset[-data_shape[0]:] = val

################################################################################

class JSONsaver(json.JSONEncoder):
    """
    The `JSONsaver` class extends the `json.JSONEncoder` to handle objects that are typically 
    unserializable by the standard JSON encoder. It provides support for custom types, including 
    NumPy arrays, ranges, PyTorch data types, and callable objects.

    This class is particularly useful when saving complex configurations or datasets to JSON files, 
    ensuring that all objects are serialized correctly or replaced with representative strings.

    Methods:
    --------
    default(obj):
        Overrides the default serialization behavior to handle custom types.

    Parameters:
    -----------
    json.JSONEncoder : class
        Inherits from Python's built-in `json.JSONEncoder`.

    Example:
    --------
    >>> data = {
    ...     "array": np.array([1.2, 3.4]),
    ...     "range": range(10),
    ...     "torch_dtype": torch.float32,
    ...     "lambda_func": lambda x: x**2
    ... }
    >>> with open("output.json", "w") as f:
    ...     json.dump(data, f, cls=JSONsaver)
    >>> # Successfully saves all objects to "output.json".
    """
    def default(self, obj):
        if isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, range):
            return list(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.bool_):
            return str(obj)
        elif obj in [torch.float16, torch.float32, torch.bfloat16]:
            return str(obj)
        elif callable(obj):
            if hasattr(obj, '__name__'):
                if obj.__name__ == '<lambda>':
                    return f'CALLABLE.{id(obj)}' # Unique identifier for lambda functions
                else:   
                    return f'CALLABLE.{obj.__name__}'
            else:
                return f'CALLABLE.{str(obj)}'
        else:
            print(f"[WARNING] Could not serialize object {obj}")
            return super().default(obj)
        

def read_coords(coords_path):
    """
    The `read_coords` function reads patch coordinates from an HDF5 file, along with any user-defined 
    attributes stored during the patching process. This function is essential for workflows that rely 
    on spatial metadata, such as patch-based analysis in computational pathology.

    Parameters:
    -----------
    coords_path : str
        The path to the HDF5 file containing patch coordinates and attributes.

    Returns:
    --------
    attrs : dict
        A dictionary of user-defined attributes stored during patching.
    coords : np.array
        An array of patch coordinates at level 0.

    Example:
    --------
    >>> attrs, coords = read_coords("patch_coords.h5")
    >>> print(attrs)
    {'patch_size': 256, 'target_mag': 20}
    >>> print(coords)
    [[0, 0], [0, 256], [256, 0], ...]
    """
    with h5py.File(coords_path, 'r') as f:
        attrs = dict(f['coords'].attrs)
        coords = f['coords'][:]
    return attrs, coords


def read_coords_legacy(coords_path):
    """
    The `read_coords_legacy` function reads legacy patch coordinates from an HDF5 file. This function 
    is designed for compatibility with older patching tools such as CLAM or Fishing-Rod, which used 
    a different structure for storing patching metadata.

    Parameters:
    -----------
    coords_path : str
        The path to the HDF5 file containing legacy patch coordinates and metadata.

    Returns:
    --------
    patch_size : int
        The target patch size at the desired magnification.
    patch_level : int
        The patch level used when reading the slide.
    custom_downsample : int
        Any additional downsampling applied to the patches.
    coords : np.array
        An array of patch coordinates.

    Example:
    --------
    >>> patch_size, patch_level, custom_downsample, coords = read_coords_legacy("legacy_coords.h5")
    >>> print(patch_size, patch_level, custom_downsample)
    256, 1, 2
    >>> print(coords)
    [[0, 0], [256, 0], [0, 256], ...]
    """
    with h5py.File(coords_path, 'r') as f:
        patch_size = f['coords'].attrs['patch_size']
        patch_level = f['coords'].attrs['patch_level']
        custom_downsample = f['coords'].attrs.get('custom_downsample', 1)
        coords = f['coords'][:]
    return patch_size, patch_level, custom_downsample, coords


def mask_to_gdf(
    mask: np.ndarray,
    keep_ids: List[int] = [],
    exclude_ids: List[int] = [],
    max_nb_holes: int = 0,
    min_contour_area: float = 1000,
    pixel_size: float = 1,
    contour_scale: float = 1.0
) -> gpd.GeoDataFrame:
    """
    Convert a binary mask into a GeoDataFrame of polygons representing detected regions.

    This function processes a binary mask to identify contours, filter them based on specified parameters,
    and scale them to the desired dimensions. The output is a GeoDataFrame where each row corresponds 
    to a detected region, with polygons representing the tissue contours and their associated holes.

    Args:
        mask (np.ndarray): The binary mask to process, where non-zero regions represent areas of interest.
        keep_ids (List[int], optional): A list of contour indices to keep. Defaults to an empty list (keep all).
        exclude_ids (List[int], optional): A list of contour indices to exclude. Defaults to an empty list.
        max_nb_holes (int, optional): The maximum number of holes to retain for each contour. 
            Use 0 to retain no holes. Defaults to 0.
        min_contour_area (float, optional): Minimum area (in pixels) for a contour to be retained. Defaults to 1000.
        pixel_size (float, optional): Pixel size of level 0. Defaults to 1.
        contour_scale (float, optional): Scaling factor for the output polygons. Defaults to 1.0.

    Returns:
        gpd.GeoDataFrame: A GeoDataFrame containing polygons for the detected regions. The GeoDataFrame
        includes a `tissue_id` column (integer ID for each region) and a `geometry` column (polygons).

    Raises:
        Exception: If no valid contours are detected in the mask.

    Example:
        >>> mask = np.array([[0, 1, 1], [0, 0, 1], [1, 1, 1]], dtype=np.uint8)
        >>> gdf = mask_to_gdf(mask, min_contour_area=500, pixel_size=0.5)
        >>> print(gdf)

    Notes:
        - The function internally downsamples the input mask for efficiency before finding contours.
        - The resulting polygons are scaled back to the original resolution using the `contour_scale` parameter.
        - Holes in contours are also detected and included in the resulting polygons.
    """

    TARGET_EDGE_SIZE = 2000
    scale = TARGET_EDGE_SIZE / mask.shape[0]

    downscaled_mask = cv2.resize(mask, (round(mask.shape[1] * scale), round(mask.shape[0] * scale)))

    # Find and filter contours
    mode = cv2.RETR_TREE if max_nb_holes == 0 else cv2.RETR_CCOMP
    contours, hierarchy = cv2.findContours(downscaled_mask, mode, cv2.CHAIN_APPROX_NONE)

    if hierarchy is None:
        hierarchy = np.array([])
    else:
        hierarchy = np.squeeze(hierarchy, axis=(0,))[:, 2:]

    filter_params = {
        'filter_color_mode': 'none',
        'max_n_holes': max_nb_holes,
        'a_t': min_contour_area * pixel_size ** 2,
        'min_hole_area': 4000 * pixel_size ** 2
    }

    if filter_params: 
        foreground_contours, hole_contours = filter_contours(contours, hierarchy, filter_params, pixel_size)  # Necessary for filtering out artifacts

    if len(foreground_contours) == 0:
        print(f"[Warning] No contour were detected. Contour GeoJSON will be empty.")
        return gpd.GeoDataFrame(columns=['tissue_id', 'geometry'])
    else:
        contours_tissue = scale_contours(foreground_contours, contour_scale / scale, is_nested=False)
        contours_holes = scale_contours(hole_contours, contour_scale / scale, is_nested=True)

    if len(keep_ids) > 0:
        contour_ids = set(keep_ids) - set(exclude_ids)
    else:
        contour_ids = set(np.arange(len(contours_tissue))) - set(exclude_ids)

    tissue_ids = [i for i in contour_ids]
    polygons = []
    for i in contour_ids:
        holes = [contours_holes[i][j].squeeze(1) for j in range(len(contours_holes[i]))] if len(contours_holes[i]) > 0 else None
        polygon = Polygon(contours_tissue[i].squeeze(1), holes=holes)
        if not polygon.is_valid:
            if not polygon.is_valid:
                polygon = make_valid(polygon)
        polygons.append(polygon)
    
    gdf_contours = gpd.GeoDataFrame(pd.DataFrame(tissue_ids, columns=['tissue_id']), geometry=polygons)
    
    return gdf_contours


def filter_contours(contours, hierarchy, filter_params, pixel_size):
    """
    The `filter_contours` function processes a list of contours and their hierarchy, filtering 
    them based on specified criteria such as minimum area and hole limits. This function is 
    typically used in digital pathology workflows to isolate meaningful tissue regions.

    Original implementation from: https://github.com/mahmoodlab/CLAM/blob/f1e93945d5f5ac6ed077cb020ed01cf984780a77/wsi_core/WholeSlideImage.py#L97

    Parameters:
    -----------
    contours : list
        A list of contours representing detected regions.
    hierarchy : np.ndarray
        The hierarchy of the contours, used to identify relationships (e.g., parent-child).
    filter_params : dict
        A dictionary containing filtering criteria. Expected keys include:
        - `filter_color_mode`: Mode for filtering based on color (currently unsupported).
        - `max_n_holes`: Maximum number of holes to retain.
        - `a_t`: Minimum area threshold for contours.
        - `min_hole_area`: Minimum area threshold for holes.
    pixel_size : float
        The pixel size at level 0, used to scale areas.

    Returns:
    --------
    tuple:
        A tuple containing:
        - Filtered foreground contours (list)
        - Corresponding hole contours (list)

    Example:
    --------
    >>> filter_params = {
    ...     "filter_color_mode": "none",
    ...     "max_n_holes": 5,
    ...     "a_t": 500,
    ...     "min_hole_area": 100
    ... }
    >>> fg_contours, hole_contours = filter_contours(contours, hierarchy, filter_params, pixel_size=0.5)
    """
    if not hierarchy.size:
        return [], []

    # Find indices of foreground contours (parent == -1)
    foreground_indices = np.flatnonzero(hierarchy[:, 1] == -1)
    filtered_foregrounds = []
    filtered_holes = []

    # Loop through each foreground contour
    for cont_idx in foreground_indices:

        contour = contours[cont_idx]
        hole_indices = np.flatnonzero(hierarchy[:, 1] == cont_idx)

        # Calculate area of the contour (foreground area minus holes)
        contour_area = cv2.contourArea(contour)
        hole_areas = [cv2.contourArea(contours[hole_idx]) for hole_idx in hole_indices]
        net_area = (contour_area - sum(hole_areas)) * (pixel_size ** 2)

        # Skip contours with negligible area
        if net_area <= 0 or net_area <= filter_params['a_t']:
            continue

        # Filter based on color mode if applicable
        if filter_params.get('filter_color_mode') not in [None, 'none']:
            raise Exception("Unsupported filter_color_mode")

        # Append valid contours
        filtered_foregrounds.append(contour)

        # Filter and limit the number of holes
        valid_holes = [
            contours[hole_idx]
            for hole_idx in hole_indices
            if cv2.contourArea(contours[hole_idx]) * (pixel_size ** 2) > filter_params['min_hole_area']
        ]
        valid_holes = sorted(valid_holes, key=cv2.contourArea, reverse=True)[:filter_params['max_n_holes']]
        filtered_holes.append(valid_holes)

    return filtered_foregrounds, filtered_holes


def make_valid(polygon):
    """
    The `make_valid` function attempts to fix invalid polygons by applying small buffer operations. 
    This is particularly useful in cases where geometric operations result in self-intersecting 
    or malformed polygons.

    Parameters:
    -----------
    polygon : shapely.geometry.Polygon
        The input polygon that may be invalid.

    Returns:
    --------
    shapely.geometry.Polygon
        A valid polygon object.

    Raises:
    -------
    Exception:
        If the function fails to create a valid polygon after several attempts.

    Example:
    --------
    >>> invalid_polygon = Polygon([(0, 0), (1, 1), (1, 0), (0, 1), (0, 0)])  # Self-intersecting
    >>> valid_polygon = make_valid(invalid_polygon)
    >>> print(valid_polygon.is_valid)
    True
    """
    
    for i in [0, 0.1, -0.1, 0.2]:
        new_polygon = polygon.buffer(i)
        if isinstance(new_polygon, Polygon) and new_polygon.is_valid:
            return new_polygon
    raise Exception("Failed to make a valid polygon")


def scale_contours(contours, scale, is_nested=False):
    """
    The `scale_contours` function scales the dimensions of contours or nested contours (e.g., holes) 
    by a specified factor. This is useful for resizing detected regions in masks or GeoDataFrames.

    Parameters:
    -----------
    contours : list
        A list of contours (or nested lists for holes) to be scaled.
    scale : float
        The scaling factor to apply.
    is_nested : bool, optional
        Indicates whether the input is a nested list of contours (e.g., for holes). Defaults to False.

    Returns:
    --------
    list:
        A list of scaled contours or nested contours.

    Example:
    --------
    >>> contours = [np.array([[0, 0], [1, 1], [1, 0]])]
    >>> scaled_contours = scale_contours(contours, scale=2.0)
    >>> print(scaled_contours)
    [array([[0, 0], [2, 2], [2, 0]])]
    """
    if is_nested:
        return [[np.array(hole * scale, dtype='int32') for hole in holes] for holes in contours]
    return [np.array(cont * scale, dtype='int32') for cont in contours]


def overlay_gdf_on_thumbnail(
    gdf_contours, thumbnail, contours_saveto, scale, tissue_color=(0, 255, 0), hole_color=(255, 0, 0)
):
    """
    The `overlay_gdf_on_thumbnail` function overlays polygons from a GeoDataFrame onto a scaled 
    thumbnail image using OpenCV. This is particularly useful for visualizing tissue regions and 
    their boundaries on smaller representations of whole-slide images.

    Parameters:
    -----------
    gdf_contours : gpd.GeoDataFrame
        A GeoDataFrame containing the polygons to overlay, with a `geometry` column.
    thumbnail : np.ndarray
        The thumbnail image as a NumPy array.
    contours_saveto : str
        The file path to save the annotated thumbnail.
    scale : float
        The scaling factor between the GeoDataFrame coordinates and the thumbnail resolution.
    tissue_color : tuple, optional
        The color (BGR format) for tissue polygons. Defaults to green `(0, 255, 0)`.
    hole_color : tuple, optional
        The color (BGR format) for hole polygons. Defaults to red `(255, 0, 0)`.

    Returns:
    --------
    None
        The function saves the annotated image to the specified file path.

    Example:
    --------
    >>> overlay_gdf_on_thumbnail(
    ...     gdf_contours=gdf, 
    ...     thumbnail=thumbnail_img, 
    ...     contours_saveto="annotated_thumbnail.png", 
    ...     scale=0.5
    ... )
    """

    for poly in gdf_contours.geometry:
        if poly.is_empty:
            continue

        # Draw tissue boundary
        if poly.exterior:
            exterior_coords = (np.array(poly.exterior.coords) * scale).astype(np.int32)
            cv2.polylines(thumbnail, [exterior_coords], isClosed=True, color=tissue_color, thickness=2)

        # Draw holes (if any) in a different color
        if poly.interiors:
            for interior in poly.interiors:
                interior_coords = (np.array(interior.coords) * scale).astype(np.int32)
                cv2.polylines(thumbnail, [interior_coords], isClosed=True, color=hole_color, thickness=2)

    # Crop black borders of the annotated image
    nz = np.nonzero(cv2.cvtColor(thumbnail, cv2.COLOR_BGR2GRAY))  # Non-zero pixel locations
    xmin, xmax, ymin, ymax = np.min(nz[1]), np.max(nz[1]), np.min(nz[0]), np.max(nz[0])
    cropped_annotated = thumbnail[ymin:ymax, xmin:xmax]
 
    # Save the annotated image
    os.makedirs(os.path.dirname(contours_saveto), exist_ok=True)
    cropped_annotated = cv2.cvtColor(cropped_annotated, cv2.COLOR_BGR2RGB)
    cv2.imwrite(contours_saveto, cropped_annotated)

# .tools.register_tool(imports=["import numpy as np"])
def get_num_workers(batch_size: int, 
                    factor: float = 0.75, 
                    fallback: int = 16, 
                    max_workers: int | None = None) -> int:
    """
    The `get_num_workers` function calculates the optimal number of workers for a PyTorch DataLoader, 
    balancing system resources and workload. This ensures efficient data loading while avoiding 
    resource overutilization.

    Parameters:
    -----------
    batch_size : int
        The batch size for the DataLoader. This is used to limit the number of workers.
    factor : float, optional
        The fraction of available CPU cores to use. Defaults to 0.75 (75% of available cores).
    fallback : int, optional
        The default number of workers to use if the system's CPU core count cannot be determined. Defaults to 16.
    max_workers : int or None, optional
        The maximum number of workers allowed. Defaults to `2 * batch_size` if not provided.

    Returns:
    --------
    int
        The calculated number of workers for the DataLoader.

    Example:
    --------
    >>> num_workers = get_num_workers(batch_size=64, factor=0.5)
    >>> print(num_workers)
    8

    Notes:
    ------
    - The number of workers is clipped to a minimum of 1 to ensure multiprocessing is not disabled.
    - The maximum number of workers defaults to `2 * batch_size` unless explicitly specified.
    - The function ensures compatibility with systems where `os.cpu_count()` may return `None`.
    - On Windows systems, the number of workers is always set to 0 to ensure compatibility with PyTorch datasets whose attributes may not be serializable.
    """

    # Disable pytorch multiprocessing on Windows
    if os.name == 'nt':
        return 0
    
    num_cores = os.cpu_count() or fallback
    num_workers = int(factor * num_cores)  # Use a fraction of available cores
    max_workers = max_workers or (2 * batch_size)  # Optional cap
    num_workers = np.clip(num_workers, 1, max_workers)
    return int(num_workers)



================================================
File: trident/Maintenance.py
================================================


def deprecated(func):
    """This is a decorator which can be used to mark functions
    as deprecated. It will result in a warning being emitted
    when the function is used."""
    import functools
    import warnings
    @functools.wraps(func)
    def new_func(*args, **kwargs):
        warnings.simplefilter('always', DeprecationWarning)  # turn off filter
        warnings.warn("Call to deprecated function {}.".format(func.__name__),
                      category=DeprecationWarning,
                      stacklevel=2)
        warnings.simplefilter('default', DeprecationWarning)  # reset filter
        return func(*args, **kwargs)
    return new_func



================================================
File: trident/Processor.py
================================================
from __future__ import annotations
import os
import sys
from tqdm import tqdm
from typing import Optional, List, Dict, Any
from inspect import signature
import geopandas as gpd
import pandas as pd 

from trident import load_wsi, WSIReaderType
from trident.IO import create_lock, remove_lock, is_locked, update_log, collect_valid_slides
from trident.Maintenance import deprecated
from trident.Converter import OPENSLIDE_EXTENSIONS, PIL_EXTENSIONS


class Processor:

    def __init__(
        self,
        job_dir: str,
        wsi_source: str,
        wsi_ext: List[str] = None,
        wsi_cache: Optional[str] = None,
        clear_cache: bool = False,
        skip_errors: bool = False,
        custom_mpp_keys: Optional[List[str]] = None,
        custom_list_of_wsis: Optional[str] = None,
        max_workers: Optional[int] = None,
        reader_type: Optional[WSIReaderType] = None,
        search_nested: bool = False, 
    ) -> None:
        """
        The `Processor` class handles all preprocessing steps starting from whole-slide images (WSIs). 
    
        Available methods:
            - `run_segmentation_job`: Performs tissue segmentation on all slides managed by the processor.
            - `run_patching_job`: Extracts patch coordinates from the segmented tissue regions of slides.
            - `run_patch_feature_extraction_job`: Extracts patch-level features using a specified patch encoder.
                - Deprecated alias: `run_feature_extraction_job`
            - `run_slide_feature_extraction_job`: Extracts slide-level features using a specified slide encoder.
            
        Parameters:
            job_dir (str): 
                The directory where the results of processing, including segmentations, patches, and extracted features, 
                will be saved. This should be an existing directory with sufficient storage.
            wsi_source (str): 
                The directory containing the WSIs to be processed. This can either be a local directory 
                or a network-mounted drive. All slides in this directory matching the specified file 
                extensions will be considered for processing.
            wsi_ext (List[str]): 
                A list of accepted WSI file extensions, such as ['.ndpi', '.svs']. This allows for 
                filtering slides based on their format. If set to None, a default list of common extensions 
                will be used. Defaults to None.
            wsi_cache (str, optional): 
                [DEPRECATED as of v0.2.0] An optional directory for caching WSIs locally. If specified, slides will be copied 
                from the source directory to this local directory before processing, improving performance 
                when the source is a network drive. Defaults to None.
            clear_cache (str, optional):
                [DEPRECATED as of v0.2.0] A flag indicating whether slides in the cache should be deleted after processing. 
                This helps manage storage space. Defaults to False. 
            skip_errors (bool, optional): 
                A flag specifying whether to continue processing if an error occurs on a slide. 
                If set to False, the process will stop on the first error. Defaults to False.
            custom_mpp_keys (List[str], optional): 
                A list of custom keys in the slide metadata for retrieving the microns per pixel (MPP) value. 
                If not provided, standard keys will be used. Defaults to None.
            custom_list_of_wsis (str, optional): 
                Path to a csv file with a custom list of WSIs to process in a field called 'wsi' (including extensions). If provided, only 
                these slides will be considered for processing. Defaults to None, which means all 
                slides matching the wsi_ext extensions will be processed.
                Note: If `custom_list_of_wsis` is provided, any names that do not match the available slides will be ignored, and a warning will be printed.
            max_workers (int, optional):
                Maximum number of workers for data loading. If None, the default behavior will be used.
                Defaults to None.
            reader_type (WSIReaderType, optional):
                Force the image reader engine to use. Options are are ["openslide", "image", "cucim"]. Defaults to None
                (auto-determine the right engine based on image extension).
            search_nested (bool, optional):  
                If True, the processor will recursively search for WSIs within all subdirectories of `wsi_source`.
                All matching files (based on `wsi_ext`) found at any depth within the directory  
                tree will be included. Each slide will be identified by its relative path to `wsi_source`, but only  
                the filename (excluding directory structure) will be used for downstream outputs (e.g., segmentation filenames).  
                If False, only files directly inside `wsi_source` will be considered.  
                Defaults to False.


        Returns:
            None: This method initializes the class instance and sets up the environment for processing.

        Example
        -------
        Initialize the `Processor` for a directory of WSIs:

        >>> processor = Processor(
        ...     job_dir="results/",
        ...     wsi_source="data/slides/",
        ...     wsi_ext=[".svs", ".ndpi"],
        ... )
        >>> print(f"Processor initialized for {len(processor.wsis)} slides.")

        Raises:
            AssertionError: If `wsi_ext` is not a list or if any extension does not start with a period.
        """
        
        if not (sys.version_info.major >= 3 and sys.version_info.minor >= 9):
            raise EnvironmentError("Trident requires Python 3.9 or above. Python 3.10 is recommended.")

        self.job_dir = job_dir
        self.wsi_source = wsi_source
        self.wsi_ext = wsi_ext or (list(PIL_EXTENSIONS) + list(OPENSLIDE_EXTENSIONS))
        self.skip_errors = skip_errors
        self.custom_mpp_keys = custom_mpp_keys
        self.max_workers = max_workers

        # Validate extensions
        assert isinstance(self.wsi_ext, list), f'wsi_ext must be a list, got {type(self.wsi_ext)}'
        for ext in self.wsi_ext:
            assert ext.startswith('.'), f'Invalid extension: {ext} (must start with a period)'

        # === Collect slide paths and relative paths ===
        full_paths, rel_paths = collect_valid_slides(
            wsi_dir=wsi_source,
            custom_list_path=custom_list_of_wsis,
            wsi_ext=self.wsi_ext,
            search_nested=search_nested,
            max_workers=max_workers,
            return_relative_paths=True
        )

        self.wsi_rel_paths = rel_paths if custom_list_of_wsis else None

        # === Extract mpp column if provided ===
        if custom_list_of_wsis is not None:
            wsi_df = pd.read_csv(custom_list_of_wsis)
            valid_mpps = (
                wsi_df['mpp'].dropna().tolist()
                if 'mpp' in wsi_df.columns else None
            )
        else:
            valid_mpps = None

        print(f'[PROCESSOR] Found {len(full_paths)} valid slides in {wsi_source}.')

        # === Initialize WSIs ===
        self.wsis = []
        for wsi_idx, abs_path in enumerate(full_paths):
            name = os.path.basename(abs_path)
            tissue_seg_path = os.path.join(
                self.job_dir, 'contours_geojson',
                f'{os.path.splitext(name)[0]}.geojson'
            )
            if not os.path.exists(tissue_seg_path):
                tissue_seg_path = None

            slide = load_wsi(
                slide_path=abs_path,
                name=name,
                tissue_seg_path=tissue_seg_path,
                custom_mpp_keys=self.custom_mpp_keys,
                mpp=valid_mpps[wsi_idx] if valid_mpps is not None else None,
                max_workers=self.max_workers,
                reader_type=reader_type,
            )
            self.wsis.append(slide)

    def run_segmentation_job(
        self, 
        segmentation_model: torch.nn.Module, 
        seg_mag: int = 10, 
        holes_are_tissue: bool = False,
        batch_size: int = 16,
        artifact_remover_model: torch.nn.Module = None,
        device: str = 'cuda:0', 
    ) -> str:
        """
        The `run_segmentation_job` function performs tissue segmentation on all slides managed by the processor. 
        It uses a machine learning model to identify tissue regions and saves the resulting segmentations to the 
        output directory. This function is essential for workflows that require detailed tissue delineation.

        Parameters:
            segmentation_model (torch.nn.Module): 
                A pre-trained PyTorch model that performs the tissue segmentation. This model should be compatible 
                with the expected input data format of WSIs.
            seg_mag (int, optional): 
                The magnification level at which segmentation is performed. For example, a value of 10 indicates 
                10x magnification. Defaults to 10.
            holes_are_tissue (bool, optional): 
                Specifies whether to treat holes within tissue regions as part of the tissue. Defaults to False.
            batch_size (int, optional): 
                The batch size for segmentation. Defaults to 16.
            artifact_remover_model (torch.nn.Module, optional): 
                A pre-trained PyTorch model that can remove artifacts from an existing segmentation. Defaults to None.
            device (str): 
                The computation device to use (e.g., 'cuda:0' for GPU or 'cpu' for CPU).

        Returns:
            str: Absolute path to where directory containing contours is saved.

        Example
        -------
        Run a segmentation job with a pre-trained model:

        >>> from segmentation.models import TissueSegmenter
        >>> model = TissueSegmenter()
        >>> processor.run_segmentation_job(segmentation_model=model, seg_mag=20)
        """
        saveto = os.path.join(self.job_dir, 'contours')
        os.makedirs(saveto, exist_ok=True)

        sig = signature(self.run_segmentation_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        self.save_config(
            saveto=os.path.join(self.job_dir, '_config_segmentation.json'),
            local_attrs=local_attrs,
            ignore = ['segmentation_model', 'loop', 'valid_slides', 'wsis']
        )

        self.loop = tqdm(self.wsis, desc='Segmenting tissue', total = len(self.wsis))
        for wsi in self.loop:   
            # Check if contour already exists
            if os.path.exists(os.path.join(saveto, f'{wsi.name}.jpg')) and not is_locked(os.path.join(saveto, f'{wsi.name}.jpg')):
                self.loop.set_postfix_str(f'{wsi.name} already segmented. Skipping...')
                update_log(os.path.join(self.job_dir, '_logs_segmentation.txt'), f'{wsi.name}{wsi.ext}', 'Tissue segmented.')
                continue

            # Check if another process has claimed this slide
            if is_locked(os.path.join(saveto, f'{wsi.name}.jpg')):
                self.loop.set_postfix_str(f'{wsi.name} is locked. Skipping...')
                continue

            try:
                self.loop.set_postfix_str(f'Segmenting {wsi}')
                create_lock(os.path.join(saveto, f'{wsi.name}.jpg'))
                update_log(os.path.join(self.job_dir, '_logs_segmentation.txt'), f'{wsi.name}{wsi.ext}', 'LOCKED. Segmenting tissue...')

                # call a function from WSI object to do the work
                gdf_saveto = wsi.segment_tissue(
                    segmentation_model=segmentation_model,
                    target_mag=seg_mag,
                    holes_are_tissue=holes_are_tissue,
                    job_dir=self.job_dir,
                    batch_size=batch_size,
                    device=device
                )

                # additionally remove artifacts for better segmentation.
                if artifact_remover_model is not None:
                    gdf_saveto = wsi.segment_tissue(
                        segmentation_model=artifact_remover_model,
                        target_mag=artifact_remover_model.target_mag,
                        holes_are_tissue=False,
                        job_dir=self.job_dir
                    )

                remove_lock(os.path.join(saveto, f'{wsi.name}.jpg'))

                gdf = gpd.read_file(gdf_saveto, rows=1)
                if gdf.empty:
                    update_log(os.path.join(self.job_dir, '_logs_segmentation.txt'), f'{wsi.name}{wsi.ext}', 'Segmentation returned empty GeoDataFrame.')
                    self.loop.set_postfix_str(f'Empty GeoDataFrame for {wsi.name}.')
                else:
                    update_log(os.path.join(self.job_dir,  '_logs_segmentation.txt'), f'{wsi.name}{wsi.ext}', 'Tissue segmented.')

            except Exception as e:
                if isinstance(e, KeyboardInterrupt):
                    remove_lock(os.path.join(saveto, f'{wsi.name}.jpg'))
                if self.skip_errors:
                    update_log(os.path.join(self.job_dir, '_logs_segmentation.txt'), f'{wsi.name}{wsi.ext}', f'ERROR: {e}')
                    continue
                else:
                    raise e
                
        # Return the directory where the contours are saved
        return saveto

    def run_patching_job(
        self, 
        target_magnification: int, 
        patch_size: int, 
        overlap: int = 0, 
        saveto: str | None = None, 
        visualize: bool = True,
        min_tissue_proportion: float = 0.,
    ) -> str:
        """
        The `run_patching_job` function extracts patches from the segmented tissue regions of slides. 
        These patches are saved as coordinates in an h5 file for each slide.

        Parameters:
            target_magnification (int): 
                The magnification level for extracting patches. Higher magnifications result in smaller 
                but more detailed patches.
            patch_size (int): 
                The size of each patch in pixels. This refers to the dimensions of the patch at the target magnification.
            overlap (int, optional): 
                The amount of overlap between adjacent patches, specified in pixels. Defaults to 0.
            saveto (str, optional): 
                The directory where patch data and visualizations will be saved. If not provided, a directory 
                name will be generated automatically. Defaults to None.
            visualize (bool, optional): 
                Whether to generate and save visualizations of the patches. Defaults to True.
            min_tissue_proportion: float, optional 
                Minimum proportion of the patch under tissue to be kept. Defaults to 0. 

        Returns:
            str: Absolute path to directory containing patch coordinates.

        Example
        -------
        Extract patches with a size of 256x256 pixels at 20x magnification:

        >>> processor.run_patching_job(
        ...     target_magnification=20, 
        ...     patch_size=256, 
        ...     overlap=32, 
        ...     saveto="output/patches/"
        ... )
        """
        if saveto is None:
            saveto = f"{target_magnification}x_{patch_size}px_{overlap}px_overlap"

        self.target_magnification = target_magnification

        if visualize:
            save_patch_viz = os.path.join(saveto, 'visualization')
            os.makedirs(os.path.join(self.job_dir, save_patch_viz), exist_ok=True)

        os.makedirs(os.path.join(self.job_dir, saveto, 'patches'), exist_ok=True)

        sig = signature(self.run_patching_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        self.save_config(
            saveto=os.path.join(self.job_dir, saveto, '_config_coords.json'),
            local_attrs=local_attrs,
            ignore = ['segmentation_model', 'loop', 'valid_slides', 'wsis']
        )
        self.loop = tqdm(self.wsis, desc=f'Saving tissue coordinates to {saveto}', total = len(self.wsis))
        for wsi in self.loop:    
        
            # Check if patch coords already exist
            if os.path.exists(os.path.join(self.job_dir, saveto, 'patches', f'{wsi.name}_patches.h5')):
                self.loop.set_postfix_str(f'Patch coords already generated for {wsi.name}. Skipping...')
                update_log(os.path.join(self.job_dir, saveto, '_logs_coords.txt'), f'{wsi.name}{wsi.ext}', 'Coords generated')
                continue
            
            # Check if another process has claimed this slide
            if is_locked(os.path.join(self.job_dir, saveto, 'patches', f'{wsi.name}_patches.h5')):
                self.loop.set_postfix_str(f'{wsi.name} is locked. Skipping...')
                continue

            # Check if segmentation exists
            if wsi.tissue_seg_path is None or not os.path.exists(wsi.tissue_seg_path):
                self.loop.set_postfix_str(f'GeoJSON not found for {wsi.name}. Skipping...')
                update_log(os.path.join(self.job_dir, saveto, '_logs_coords.txt'), f'{wsi.name}{wsi.ext}', 'GeoJSON not found.')
                continue
            
            # Check if GeoJSON is empty
            gdf = gpd.read_file(wsi.tissue_seg_path, rows=1)
            if gdf.empty:
                self.loop.set_postfix_str(f'Empty GeoDataFrame for {wsi.name}. Skipping...')
                update_log(os.path.join(self.job_dir, saveto, '_logs_coords.txt'), f'{wsi.name}{wsi.ext}', 'Empty GeoDataFrame.')
                continue

            try:
                self.loop.set_postfix_str(f'Generating patch coords for {wsi.name}{wsi.ext}')
                update_log(os.path.join(self.job_dir, saveto, '_logs_coords.txt'), f'{wsi.name}{wsi.ext}', 'LOCKED. Generating coords...')
                create_lock(os.path.join(self.job_dir, saveto, 'patches', f'{wsi.name}_patches.h5'))

                # save tissue coords
                wsi.extract_tissue_coords(
                    target_mag=target_magnification,
                    patch_size=patch_size,
                    save_coords=os.path.join(self.job_dir, saveto),
                    overlap=overlap,
                    min_tissue_proportion=min_tissue_proportion,
                )

                # save tissue coords visualization
                if visualize:  
                    wsi.visualize_coords(
                        coords_path=os.path.join(self.job_dir, saveto, 'patches', f'{wsi.name}_patches.h5'),
                        save_patch_viz=os.path.join(self.job_dir, save_patch_viz),
                    )

                remove_lock(os.path.join(self.job_dir, saveto, 'patches', f'{wsi.name}_patches.h5'))
                update_log(os.path.join(self.job_dir, saveto, '_logs_coords.txt'), f'{wsi.name}{wsi.ext}', 'Coords generated')
            except Exception as e:
                if isinstance(e, KeyboardInterrupt):
                    remove_lock(os.path.join(self.job_dir, saveto, 'patches', f'{wsi.name}_patches.h5'))
                if self.skip_errors:
                    update_log(os.path.join(self.job_dir, saveto, '_logs_coords.txt'), f'{wsi.name}{wsi.ext}', f'ERROR: {e}')
                    continue
                else:
                    raise e
        
        # Return the directory where the coordinates are saved
        return os.path.join(self.job_dir, saveto)

    @deprecated
    def run_feature_extraction_job(
        self, 
        coords_dir: str, 
        patch_encoder: torch.nn.Module, 
        device: str, 
        saveas: str = 'h5', 
        batch_limit: int = 512, 
        saveto: str | None = None
    ) -> str:
        self.run_patch_feature_extraction_job(
            coords_dir, 
            patch_encoder, 
            device, 
            saveas, 
            batch_limit, 
            saveto,
        )
        
    def run_patch_feature_extraction_job(
        self, 
        coords_dir: str, 
        patch_encoder: torch.nn.Module, 
        device: str, 
        saveas: str = 'h5', 
        batch_limit: int = 512, 
        saveto: str | None = None
    ) -> str:
        """
        The `run_feature_extraction_job` function computes features from the patches generated during the 
        patching step. These features are extracted using a deep learning model and saved in a specified format. 
        This step is often used in workflows that involve downstream analysis, such as classification or clustering.

        Parameters:
            coords_dir (str): 
                Path to the directory containing patch coordinates, which are used to locate patches for feature extraction.
            patch_encoder (torch.nn.Module): 
                A pre-trained PyTorch model used to compute features from the extracted patches.
            device (str): 
                The computation device to use (e.g., 'cuda:0' for GPU or 'cpu' for CPU).
            saveas (str, optional): 
                The format in which extracted features are saved. Can be 'h5' or 'pt'. Defaults to 'h5'.
            batch_limit (int, optional): 
                The maximum number of patches processed in a single batch. Defaults to 512.
            saveto (str, optional): 
                Directory where the extracted features will be saved. If not provided, a directory name will 
                be generated automatically. Defaults to None.

        Returns:
            str: The absolute path to where the features are saved.

        Example
        -------
        Extract features from patches using a pre-trained encoder:

        >>> from models import PatchEncoder
        >>> encoder = PatchEncoder()
        >>> processor.run_feature_extraction_job(
        ...     coords_dir="output/patch_coords/",
        ...     patch_encoder=encoder,
        ...     device="cuda:0"
        ... )
        """
        if saveto is None:
            saveto = os.path.join(coords_dir, f'features_{patch_encoder.enc_name}')

        os.makedirs(os.path.join(self.job_dir, saveto), exist_ok=True)

        sig = signature(self.run_patch_feature_extraction_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        self.save_config(
            saveto=os.path.join(self.job_dir, coords_dir, f'_config_feats_{patch_encoder.enc_name}.json'),
            local_attrs=local_attrs,
            ignore = ['patch_encoder', 'loop', 'valid_slides', 'wsis']
        )

        log_fp = os.path.join(self.job_dir, coords_dir, f'_logs_feats_{patch_encoder.enc_name}.txt')
        self.loop = tqdm(self.wsis, desc=f'Extracting patch features from coords in {coords_dir}', total = len(self.wsis))
        for wsi in self.loop:    
            wsi_feats_fp = os.path.join(self.job_dir, saveto, f'{wsi.name}.{saveas}')
            # Check if features already exist
            if os.path.exists(wsi_feats_fp) and not is_locked(wsi_feats_fp):
                self.loop.set_postfix_str(f'Features already extracted for {wsi}. Skipping...')
                update_log(log_fp, f'{wsi.name}{wsi.ext}', 'Features extracted.')
                continue

            # Check if coords exist
            coords_path = os.path.join(self.job_dir, coords_dir, 'patches', f'{wsi.name}_patches.h5')
            if not os.path.exists(coords_path):
                self.loop.set_postfix_str(f'Coords not found for {wsi.name}. Skipping...')
                update_log(log_fp, f'{wsi.name}{wsi.ext}', 'Coords not found.')
                continue

            # Check if another process has claimed this slide
            if is_locked(wsi_feats_fp):
                self.loop.set_postfix_str(f'{wsi.name} is locked. Skipping...')
                continue

            try:
                self.loop.set_postfix_str(f'Extracting features from {wsi.name}{wsi.ext}')
                create_lock(wsi_feats_fp)
                update_log(log_fp, f'{wsi.name}{wsi.ext}', 'LOCKED. Extracting features...')

                # under construction
                wsi.extract_patch_features(
                    patch_encoder = patch_encoder,
                    coords_path = coords_path,
                    save_features=os.path.join(self.job_dir, saveto),
                    device=device,
                    saveas=saveas,
                    batch_limit=batch_limit
                )

                remove_lock(wsi_feats_fp)
                update_log(log_fp, f'{wsi.name}{wsi.ext}', 'Features extracted.')
            except Exception as e:
                if isinstance(e, KeyboardInterrupt):
                    remove_lock(wsi_feats_fp)
                if self.skip_errors:
                    update_log(log_fp, f'{wsi.name}{wsi.ext}', f'ERROR: {e}')
                    continue
                else:
                    raise e
        
        # Return the directory where the features are saved
        return os.path.join(self.job_dir, saveto)

    def run_slide_feature_extraction_job(
        self,
        slide_encoder: torch.nn.Module,
        coords_dir: str,
        device: str = 'cuda',
        batch_limit: int = 512, 
        saveas: str = 'h5', 
        saveto: str | None = None
    ) -> None:
        """
        Extract slide-level features from whole-slide images (WSIs) using a specified slide encoder.

        This function generates embeddings for WSIs by first ensuring that patch-level features
        required for the slide encoder are available. If patch features are missing, they are
        extracted using an appropriate patch encoder automatically inferred. The extracted slide features are saved in 
        the specified format and directory.

        Args:
            slide_encoder (torch.nn.Module): The slide encoder model used for generating slide-level
                features from patch-level features.
            coords_dir (str): Directory containing coordinates and features required for processing WSIs.
            device (str, optional): Device to use for computations (e.g., 'cuda', 'cpu'). Defaults to 'cuda'.
            batch_limit (int, optional): Maximum number of features processed in a batch during patch
                feature extraction. Defaults to 512.
            saveas (str, optional): File format to save slide features (e.g., 'h5'). Defaults to 'h5'.
            saveto (str | None, optional): Directory to save extracted slide features. If None, the
                directory is auto-generated based on `coords_dir` and `slide_encoder`. Defaults to None.

        Returns:
            str: The absolute path to where the slide embeddings are saved. 

        Workflow:
            1. Verify the compatibility of the slide encoder and patch features.
            2. Check if patch-level features are already extracted for all WSIs. If not, extract them.
            3. Save the configuration for slide feature extraction to maintain reproducibility.
            4. Process each WSI:
                - Skip if patch features required for the WSI are missing.
                - Extract slide features, ensuring proper synchronization in multiprocessing setups.
            5. Log the progress and errors during processing.

        Notes:
            - Patch features are expected in a specific directory structure under `coords_dir`.
            - Slide features are saved in the format specified by `saveas`.
            - Errors can be optionally skipped based on the `self.skip_errors` attribute.

        Raises:
            Exception: Propagates exceptions unless `self.skip_errors` is set to True.

        """
        from trident.slide_encoder_models.load import slide_to_patch_encoder_name
        
        if slide_encoder.enc_name.startswith('mean-'):
            slide_to_patch_encoder_name[slide_encoder.enc_name] = slide_encoder.enc_name.split('mean-')[1] # e.g. mean-resnet18 -> resnet18

        # Setting I/O
        mustbe_patch_encoder = slide_to_patch_encoder_name[slide_encoder.enc_name]
        patch_features_dir = os.path.join(coords_dir, f'features_{mustbe_patch_encoder}')
        if saveto is None:
            saveto = os.path.join(coords_dir, f'slide_features_{slide_encoder.enc_name}')
        os.makedirs(os.path.join(self.job_dir, saveto), exist_ok=True)

        # Run patch feature extraction if some patch features are missing:
        already_processed = []
        if os.path.isdir(os.path.join(self.job_dir, patch_features_dir)):
            already_processed = [os.path.splitext(x)[0] for x in os.listdir(os.path.join(self.job_dir, patch_features_dir)) if x.endswith(saveas)]
            wsi_names = [slide.name for slide in self.wsis]
            already_processed = [x for x in already_processed if x in wsi_names]
        if len(already_processed) < len(self.wsis):
            print(f"[PROCESSOR] Some patch features haven't been extracted in {len(already_processed)}/{len(self.wsis)} WSIs. Starting extraction.")
            from trident.patch_encoder_models.load import encoder_factory
            patch_encoder = encoder_factory(slide_to_patch_encoder_name[slide_encoder.enc_name])
            self.run_patch_feature_extraction_job(
                coords_dir=coords_dir,
                patch_encoder=patch_encoder,
                device=device,
                saveas='h5',  # must use h5 to run slide extraction later to get coords.
                batch_limit=batch_limit,
            )

        sig = signature(self.run_slide_feature_extraction_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        self.save_config(
            saveto=os.path.join(self.job_dir, coords_dir, f'_config_slide_features_{slide_encoder.enc_name}.json'),
            local_attrs=local_attrs,
            ignore=['loop', 'valid_slides', 'wsis']
        )

        self.loop = tqdm(self.wsis, desc=f'Extracting slide features using {slide_encoder.enc_name}', total=len(self.wsis))
        for wsi in self.loop:
            # Check if slide features already exist
            slide_feature_path = os.path.join(self.job_dir, saveto, f'{wsi.name}.{saveas}')
            if os.path.exists(slide_feature_path) and not is_locked(slide_feature_path):
                self.loop.set_postfix_str(f'Slide features already extracted for {wsi.name}. Skipping...')
                update_log(os.path.join(self.job_dir, coords_dir, f'_logs_slide_features_{slide_encoder.enc_name}.txt'), f'{wsi.name}{wsi.ext}', 'Slide features extracted.')
                continue

            # Check if patch features exist
            patch_features_path = os.path.join(self.job_dir, patch_features_dir, f'{wsi.name}.h5')
            if not os.path.exists(patch_features_path):
                self.loop.set_postfix_str(f'Patch features not found for {wsi.name}. Skipping...')
                update_log(os.path.join(self.job_dir, coords_dir, f'_logs_slide_features_{slide_encoder.enc_name}.txt'), f'{wsi.name}{wsi.ext}', 'Patch features not found.')
                continue

            # Check if another process has claimed this slide
            if is_locked(slide_feature_path):
                self.loop.set_postfix_str(f'{wsi.name} is locked. Skipping...')
                continue

            try:
                self.loop.set_postfix_str(f'Extracting slide features for {wsi.name}{wsi.ext}')
                create_lock(slide_feature_path)
                update_log(os.path.join(self.job_dir, coords_dir, f'_logs_slide_features_{slide_encoder.enc_name}.txt'), f'{wsi.name}{wsi.ext}', 'LOCKED. Extracting slide features...')

                # Call the extract_slide_features method
                wsi.extract_slide_features(
                    patch_features_path=patch_features_path,
                    slide_encoder=slide_encoder,
                    device=device,
                    save_features=os.path.join(self.job_dir, saveto)
                )

                remove_lock(slide_feature_path)
                update_log(os.path.join(self.job_dir, coords_dir, f'_logs_slide_features_{slide_encoder.enc_name}.txt'), f'{wsi.name}{wsi.ext}', 'Slide features extracted.')
            except Exception as e:
                if isinstance(e, KeyboardInterrupt):
                    remove_lock(slide_feature_path)
                if self.skip_errors:
                    update_log(os.path.join(self.job_dir, coords_dir, f'_logs_slide_features_{slide_encoder.enc_name}.txt'), f'{wsi.name}{wsi.ext}', f'ERROR: {e}')
                    continue
                else:
                    raise e
        
        return os.path.join(self.job_dir, saveto)

    def save_config(
        self,
        saveto: str,
        local_attrs: Optional[Dict[str, Any]] = None,
        ignore: List[str] = ['valid_slides']
    ) -> None:
        """
        The `save_config` function saves the current configuration of the `Processor` instance to a JSON file. 
        This configuration includes attributes of the instance as well as optional additional parameters 
        provided via the `local_attrs` argument.

        The function filters out attributes specified in the `ignore` list and ensures that only JSON-serializable 
        attributes are included. This makes it ideal for saving configurations in a structured format that can 
        later be reloaded or inspected for reproducibility.

        Parameters:
            saveto (str): 
                The path to the file where the configuration will be saved. This should include the file extension 
                (e.g., "config.json").
            local_attrs (dict, optional): 
                A dictionary of additional attributes to include in the configuration. This can be used to add 
                method-specific parameters or runtime settings. Defaults to None.
            ignore (list, optional): 
                A list of attribute names to exclude from the configuration. This is useful for omitting large 
                or non-serializable objects. Defaults to ['valid_slides'].

        Returns:
            None: The function saves the configuration to the specified file and does not return any value.

        Example
        -------
        Save the current processor configuration to a file:

        >>> processor.save_config(saveto="output/config.json")
        >>> # Check the saved configuration
        >>> with open("output/config.json", "r") as f:
        ...     config = json.load(f)
        ...     print(config)
        """
        import json
        from trident.IO import JSONsaver

        def serialize_safe(obj):
            try:
                return json.loads(json.dumps(obj))  # Ensure the object is JSON-serializable
            except (TypeError, OverflowError):
                return None

        # Merge instance attributes and local_attrs, filtering ignored and unserializable items
        config = {
            k: serialize_safe(v)
            for attr_dict in [vars(self), local_attrs or {}]
            for k, v in attr_dict.items()
            if k not in ignore and serialize_safe(v) is not None
        }

        # Save the combined configuration to the specified file
        with open(saveto, 'w') as f:
            json.dump(config, f, indent=4, cls=JSONsaver)

    def release(self) -> None:
        """
        Release all resources tied to the WSIs held by this Processor instance.
        Frees memory, closes file handles, and clears GPU memory.
        Should be called after processing is complete to avoid memory leaks.
        """
        if hasattr(self, "wsis"):
            for wsi in self.wsis:
                try:
                    wsi.release()
                except Exception:
                    pass
            self.wsis.clear()

        # Also clear loop references (e.g., tqdm)
        if hasattr(self, "loop"):
            self.loop = None

        # Explicit garbage collection and CUDA cache release
        import gc
        import torch
        gc.collect()
        torch.cuda.empty_cache()



================================================
File: trident/Visualization.py
================================================
import numpy as np
import cv2
import matplotlib.pyplot as plt
from scipy.stats import rankdata
from PIL import Image
from typing import Optional, Tuple
import os 


def create_overlay(
    scores: np.ndarray,
    coords: np.ndarray,
    patch_size_level0: int,
    scale: np.ndarray,
    region_size: Tuple[int, int]
) -> np.ndarray:
    """
    Create the heatmap overlay based on scores and coordinates.
    
    Args:
        scores (np.ndarray): Normalized scores.
        coords (np.ndarray): Coordinates of patches.
        patch_size_level0 (int): Patch size at level 0.
        scale (np.ndarray): Scaling factors.
        region_size (Tuple[int, int]): Dimensions of the region.
    
    Returns:
        np.ndarray: Heatmap overlay.
    """
    patch_size = np.ceil(np.array([patch_size_level0, patch_size_level0]) * scale).astype(int)
    coords = np.ceil(coords * scale).astype(int)
    
    overlay = np.zeros(tuple(np.flip(region_size)), dtype=float)
    counter = np.zeros_like(overlay, dtype=np.uint16)
    
    for idx, coord in enumerate(coords):
        overlay[coord[1]:coord[1] + patch_size[1], coord[0]:coord[0] + patch_size[0]] += scores[idx]
        counter[coord[1]:coord[1] + patch_size[1], coord[0]:coord[0] + patch_size[0]] += 1
    
    zero_mask = counter == 0
    overlay[~zero_mask] /= counter[~zero_mask]
    overlay[zero_mask] = np.nan  # Set areas with no data to NaN
    
    return overlay


def apply_colormap(overlay: np.ndarray, cmap_name: str) -> np.ndarray:
    """
    Apply a colormap to the heatmap overlay.
    
    Args:
        overlay (np.ndarray): Heatmap overlay.
        cmap_name (str): Colormap name.

    Returns:
        np.ndarray: Colored overlay image.
    """
    cmap = plt.get_cmap(cmap_name)
    overlay_colored = np.zeros((*overlay.shape, 3), dtype=np.uint8)
    valid_mask = ~np.isnan(overlay)
    colored_valid = (cmap(overlay[valid_mask]) * 255).astype(np.uint8)[:, :3]
    overlay_colored[valid_mask] = colored_valid
    return overlay_colored


def visualize_heatmap(
    wsi,
    scores: np.ndarray,
    coords: np.ndarray,
    patch_size_level0: int,
    vis_level: Optional[int] = 2,
    cmap: str = 'coolwarm',
    normalize: bool = True,
    num_top_patches_to_save: int = -1,
    output_dir: Optional[str] = "output",
) -> str:
    """
    Generate a heatmap visualization overlayed on a whole slide image (WSI).
    
    Args:
        wsi: Whole slide image object.
        scores (np.ndarray): Scores associated with each coordinate.
        coords (np.ndarray): Coordinates of patches at level 0.
        patch_size_level0 (int): Patch size at level 0.
        vis_level (Optional[int]): Visualization level.
        cmap (str): Colormap to use for the heatmap.
        normalize (bool): Whether to normalize the scores.
        num_top_patches_to_save (int): Number of high-score patches to save. If set to -1, do not save any. Defaults to -1.
        output_dir (Optional[str]): Directory to save heatmap and top-k patches.
    
    Returns:
        str: Path to the saved heatmap image.
    """

    if normalize:
        scores = rankdata(scores, 'average') / len(scores) * 100 / 100
    
    downsample = wsi.level_downsamples[vis_level]
    scale = np.array([1 / downsample, 1 / downsample])
    region_size = tuple((np.array(wsi.level_dimensions[0]) * scale).astype(int))
    
    overlay = create_overlay(scores, coords, patch_size_level0, scale, region_size)
    
    img = wsi.read_region((0, 0), vis_level, wsi.level_dimensions[vis_level]).convert("RGB")
    img = img.resize(region_size, resample=Image.Resampling.BICUBIC)
    img = np.array(img)
    
    overlay_colored = apply_colormap(overlay, cmap)
    blended_img = cv2.addWeighted(img, 0.6, overlay_colored, 0.4, 0)
    blended_img = Image.fromarray(blended_img)

    os.makedirs(output_dir, exist_ok=True)
    heatmap_path = os.path.join(output_dir, "heatmap.png")
    blended_img.save(heatmap_path)

    if num_top_patches_to_save > 0:
        topk_dir = os.path.join(output_dir, "topk_patches")
        os.makedirs(topk_dir, exist_ok=True)
        topk_indices = np.argsort(scores)[-num_top_patches_to_save:]
        for idx, i in enumerate(topk_indices):
            x, y = coords[i]
            patch = wsi.read_region((x, y), 0, (patch_size_level0, patch_size_level0))
            patch.save(os.path.join(topk_dir, f"top_{idx}_score_{scores[i]:.4f}.png"))

    return heatmap_path



================================================
File: trident/__init__.py
================================================
from importlib.metadata import version, PackageNotFoundError

try:
    __version__ = version("trident")
except PackageNotFoundError:
    __version__ = "unknown"

from trident.wsi_objects.OpenSlideWSI import OpenSlideWSI
from trident.wsi_objects.CuCIMWSI import CuCIMWSI
from trident.wsi_objects.ImageWSI import ImageWSI
from trident.wsi_objects.WSIFactory import load_wsi, WSIReaderType
from trident.wsi_objects.WSIPatcher import OpenSlideWSIPatcher, WSIPatcher
from trident.wsi_objects.WSIPatcherDataset import WSIPatcherDataset

from trident.Visualization import visualize_heatmap

from trident.Processor import Processor

from trident.Converter import AnyToTiffConverter

from trident.Maintenance import deprecated

__all__ = [
    "Processor",
    "load_wsi",
    "OpenSlideWSI", 
    "ImageWSI",
    "CuCIMWSI",
    "WSIPatcher",
    "OpenSlideWSIPatcher",
    "WSIPatcherDataset",
    "visualize_heatmap",
    "AnyToTiffConverter",
    "deprecated",
    "WSIReaderType",
]



================================================
File: trident/patch_encoder_models/__init__.py
================================================
from trident.patch_encoder_models.load import (
    encoder_factory,
    CustomInferenceEncoder,
    MuskInferenceEncoder,
    Conchv1InferenceEncoder,
    CTransPathInferenceEncoder,
    PhikonInferenceEncoder,
    ResNet50InferenceEncoder,
    UNIInferenceEncoder,
    UNIv2InferenceEncoder,
    GigaPathInferenceEncoder,
    VirchowInferenceEncoder,
    Virchow2InferenceEncoder,
    HOptimus0InferenceEncoder,
    HOptimus1InferenceEncoder,
    Conchv15InferenceEncoder,
    Phikonv2InferenceEncoder,
    LunitS8InferenceEncoder,
    HibouLInferenceEncoder,
    KaikoB16InferenceEncoder,
    KaikoB8InferenceEncoder,
    KaikoS16InferenceEncoder,
    KaikoS8InferenceEncoder,
    KaikoL14InferenceEncoder,
    Midnight12kInferenceEncoder,
)

__all__ = [
    "encoder_factory",
    "CustomInferenceEncoder",
    "MuskInferenceEncoder",
    "Conchv1InferenceEncoder",
    "CTransPathInferenceEncoder",
    "PhikonInferenceEncoder",
    "ResNet50InferenceEncoder",
    "UNIInferenceEncoder",
    "UNIv2InferenceEncoder",
    "GigaPathInferenceEncoder",
    "VirchowInferenceEncoder",
    "Virchow2InferenceEncoder",
    "HOptimus0InferenceEncoder",
    "HOptimus1InferenceEncoder",
    "Conchv15InferenceEncoder",
    "Phikonv2InferenceEncoder",
    "LunitS8InferenceEncoder",
    "HibouLInferenceEncoder",
    "KaikoB16InferenceEncoder",
    "KaikoB8InferenceEncoder",
    "KaikoS16InferenceEncoder",
    "KaikoS8InferenceEncoder",
    "KaikoL14InferenceEncoder",
    "Midnight12kInferenceEncoder",
]


================================================
File: trident/patch_encoder_models/load.py
================================================
import traceback
from abc import abstractmethod
from typing import Literal, Optional
import torch
import os 

from trident.patch_encoder_models.utils.constants import get_constants
from trident.patch_encoder_models.utils.transform_utils import get_eval_transforms
from trident.IO import get_weights_path, has_internet_connection

"""
This file contains an assortment of pretrained patch encoders, all loadable via the encoder_factory() function.
"""

def encoder_factory(model_name: str, **kwargs):
    """
    Instantiate a patch encoder model by name.

    This factory function returns a pre-configured encoder model class based on the provided
    `model_name`. Each encoder is designed for extracting representations from image patches
    using specific backbones or pretraining strategies.

    Args:
        model_name (str): Name of the encoder to instantiate. Must be one of the following:
            - "conch_v1"
            - "conch_v15"
            - "uni_v1"
            - "uni_v2"
            - "ctranspath"
            - "phikon"
            - "phikon_v2"
            - "resnet50"
            - "gigapath"
            - "virchow"
            - "virchow2"
            - "hoptimus0"
            - "hoptimus1"
            - "musk"
            - "hibou_l"
            - "kaiko-vitb8"
            - "kaiko-vitb16"
            - "kaiko-vits8"
            - "kaiko-vits16"
            - "kaiko-vitl14"
            - "lunit-vits8"

        **kwargs: Optional keyword arguments passed directly to the encoder constructor. These
            may include parameters such as:
            - weights_path (str): Path to a local checkpoint (optional)
            - normalize (bool): Whether to normalize output embeddings (default: False)
            - with_proj (bool): Whether to apply the projection head (default: True)
            - any model-specific configuration parameters

    Returns:
        torch.nn.Module: An instance of the specified encoder model.

    Raises:
        ValueError: If `model_name` is not among the recognized encoder names.
    """
    if model_name == 'conch_v1':
        enc = Conchv1InferenceEncoder
    elif model_name == 'conch_v15':
        enc = Conchv15InferenceEncoder
    elif model_name == 'uni_v1':
        enc = UNIInferenceEncoder
    elif model_name == 'uni_v2':
        enc = UNIv2InferenceEncoder
    elif model_name == 'ctranspath':
        enc = CTransPathInferenceEncoder
    elif model_name == 'phikon':
        enc = PhikonInferenceEncoder
    elif model_name == 'resnet50':
        enc = ResNet50InferenceEncoder
    elif model_name == 'gigapath':
        enc = GigaPathInferenceEncoder
    elif model_name == 'virchow':
        enc = VirchowInferenceEncoder
    elif model_name == 'virchow2':
        enc = Virchow2InferenceEncoder
    elif model_name == 'hoptimus0':
        enc = HOptimus0InferenceEncoder
    elif model_name == 'hoptimus1':
        enc = HOptimus1InferenceEncoder
    elif model_name == 'phikon_v2':
        enc = Phikonv2InferenceEncoder
    elif model_name == 'musk':
        enc = MuskInferenceEncoder
    elif model_name == 'hibou_l':
        enc = HibouLInferenceEncoder
    elif model_name == 'kaiko-vitb8':
        enc = KaikoB8InferenceEncoder
    elif model_name == 'kaiko-vitb16':
        enc = KaikoB16InferenceEncoder
    elif model_name == 'kaiko-vits8':
        enc = KaikoS8InferenceEncoder
    elif model_name == 'kaiko-vits16':
        enc = KaikoS16InferenceEncoder
    elif model_name == 'kaiko-vitl14':
        enc = KaikoL14InferenceEncoder
    elif model_name == 'lunit-vits8':
        enc = LunitS8InferenceEncoder
    elif model_name == 'midnight12k':
        enc = Midnight12kInferenceEncoder
    else:
        raise ValueError(f"Unknown encoder name {model_name}")

    return enc(**kwargs)


class BasePatchEncoder(torch.nn.Module):

    _has_internet = has_internet_connection()
    
    def __init__(self, weights_path: Optional[str] = None, **build_kwargs):
        """
        Initialize BasePatchEncoder.

        Args:
            weights_path (Optional[str]): 
                Optional path to local model weights. If None, the model is loaded from the model registry or downloaded from Hugging Face Hub.
            **build_kwargs: 
                Additional keyword arguments passed to the `_build()` method to customize model creation.

        Attributes:
            enc_name (Optional[str]): Name of the encoder architecture (set during `_build()`).
            weights_path (Optional[str]): Path to local model weights (if provided).
            model (nn.Module): The instantiated encoder model.
            eval_transforms (Callable): Evaluation-time preprocessing transforms.
            precision (torch.dtype): Precision used for inference.
        """

        super().__init__()
        self.enc_name: Optional[str] = None
        self.weights_path: Optional[str] = weights_path
        self.model, self.eval_transforms, self.precision = self._build(**build_kwargs)

    def ensure_valid_weights_path(self, weights_path):
        if weights_path and not os.path.isfile(weights_path):
            raise FileNotFoundError(f"Expected checkpoint at '{weights_path}', but the file was not found.")
    
    def ensure_has_internet(self, enc_name):
        if not BasePatchEncoder._has_internet:
            raise FileNotFoundError(
                f"Internet connection does seem not available. Auto checkpoint download is disabled."
                f"To proceed, please manually download: {enc_name},\n"
                f"and place it in the model registry in:\n`trident/patch_encoder_models/local_ckpts.json`"
            )
        
    def _get_weights_path(self):
        """
        If self.weights_path is provided, use it. 
        If not provided, check the model registry. 
            If path in model registry is empty, auto-download from huggingface
            else, use the path from the registry.
        """
        if self.weights_path:
            self.ensure_valid_weights_path(self.weights_path)
            return self.weights_path
        else:
            weights_path = get_weights_path('patch', self.enc_name)
            self.ensure_valid_weights_path(weights_path)
            return weights_path

    def forward(self, x):
        """
        Can be overwritten if model requires special forward pass.
        """
        z = self.model(x)
        return z
        
    @abstractmethod
    def _build(self, **build_kwargs):
        pass


class CustomInferenceEncoder(BasePatchEncoder):

    def __init__(self, enc_name, model, transforms, precision):
        """
        Initialize a CustomInferenceEncoder from user-defined components.

        This class is used when the model, transforms, and precision are pre-instantiated externally 
        and should be injected directly into the encoder wrapper.

        Args:
            enc_name (str): 
                A unique name or identifier for the encoder (used for registry or logging).
            model (torch.nn.Module): 
                A PyTorch model instance to use for inference.
            transforms (Callable): 
                A callable (e.g., torchvision or timm transform) to preprocess input images for evaluation.
            precision (torch.dtype): 
                The precision to use for inference (e.g., torch.float32, torch.float16).
        """
        super().__init__()
        self.enc_name = enc_name
        self.model = model
        self.eval_transforms = transforms
        self.precision = precision
        
    def _build(self):
        return None, None, None


class MuskInferenceEncoder(BasePatchEncoder):
    
    def __init__(self, **build_kwargs):
        """
        MUSK initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, inference_aug=False, with_proj=False, out_norm=False, return_global=True):
        """
        Args:
            inference_aug (bool): Whether to use test-time multiscale augmentation. Default is False to allow for fair comparison with other models.
        """
        import timm
        
        self.enc_name = 'musk'
        self.inference_aug = inference_aug
        self.with_proj = with_proj
        self.out_norm = out_norm
        self.return_global = return_global
    
        try:
            from musk import utils, modeling
        except:
            traceback.print_exc()
            raise Exception("Please install MUSK `pip install fairscale git+https://github.com/lilab-stanford/MUSK`")

        weights_path = self._get_weights_path()

        if weights_path:
            raise NotImplementedError("MUSK doesn't support local model loading. PR welcome!")
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("musk_large_patch16_384")
                utils.load_model_and_may_interpolate("hf_hub:xiangjx/musk", model, 'model|module', '')
            except:
                traceback.print_exc()
                raise Exception("Failed to download MUSK model, make sure that you were granted access and that you correctly registered your token")
        
        from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
        from torchvision.transforms import InterpolationMode
        eval_transform = get_eval_transforms(IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD, target_img_size = 384, center_crop = True, interpolation=InterpolationMode.BICUBIC, antialias=True)
        precision = torch.float16
        
        return model, eval_transform, precision
    
    def forward(self, x):
        return self.model(
                image=x,
                with_head=self.with_proj,
                out_norm=self.out_norm,
                ms_aug=self.inference_aug,
                return_global=self.return_global  
                )[0]  # Forward pass yields (vision_cls, text_cls). We only need vision_cls.


class Conchv1InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        CONCH initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, with_proj=False, normalize=False):
        self.enc_name = 'conch_v1'
        self.with_proj = with_proj
        self.normalize = normalize

        try:
            from conch.open_clip_custom import create_model_from_pretrained
        except:
            traceback.print_exc()
            raise Exception("Please install CONCH `pip install git+https://github.com/Mahmoodlab/CONCH.git`")
        
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model, eval_transform = create_model_from_pretrained('conch_ViT-B-16', checkpoint_path=weights_path)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create CONCH v1 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/MahmoodLab/CONCH."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model, eval_transform = create_model_from_pretrained('conch_ViT-B-16', checkpoint_path="hf_hub:MahmoodLab/conch")
            except:
                traceback.print_exc()
                raise Exception("Failed to download CONCH v1 model, make sure that you were granted access and that you correctly registered your token")
    
        precision = torch.float32
        
        return model, eval_transform, precision
    
    def forward(self, x):
        return self.model.encode_image(x, proj_contrast=self.with_proj, normalize=self.normalize)
    

class CTransPathInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        CTransPath initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        from torchvision.transforms import InterpolationMode
        from torch import nn

        try:
            from .model_zoo.ctranspath.ctran import ctranspath
        except:
            traceback.print_exc()
            raise Exception("Failed to import CTransPath model, make sure timm_ctp is installed. `pip install timm_ctp`")
        
        self.enc_name = 'ctranspath'
        weights_path = self._get_weights_path()

        model = ctranspath(img_size=224)
        model.head = nn.Identity()

        if not weights_path:
            self.ensure_has_internet(self.enc_name)
            try:
                from huggingface_hub import hf_hub_download   
                weights_path = hf_hub_download(
                    repo_id="MahmoodLab/hest-bench",
                    repo_type="dataset",
                    filename="CHIEF_CTransPath.pth",
                    subfolder="fm_v1/ctranspath",
                )
            except:
                traceback.print_exc()
                raise Exception("Failed to download CTransPath model, make sure that you were granted access and that you correctly registered your token")

        try:
            state_dict = torch.load(weights_path, weights_only=True)['model']
        except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create CTransPath model from local checkpoint at '{weights_path}'. "
                    "You can download the required `CHIEF_CTransPath.pth` from: https://huggingface.co/datasets/MahmoodLab/hest-bench/tree/main/fm_v1/ctranspath."
                )
        state_dict = {key: val for key, val in state_dict.items() if 'attn_mask' not in key}
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        assert len(unexpected) == 0, f"Unexpected keys found in state dict: {unexpected}"
        assert missing == ['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask'], f"Unexpected missing keys: {missing}"

        mean, std = get_constants('imagenet')
        eval_transform = get_eval_transforms(mean, std, target_img_size=224, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)

        precision = torch.float32
        
        return model, eval_transform, precision


class PhikonInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Phikon initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        from transformers import ViTModel
        from torchvision.transforms import InterpolationMode

        self.enc_name = 'phikon'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model_dir = os.path.dirname(weights_path)
                model = ViTModel.from_pretrained(model_dir, add_pooling_layer=False, local_files_only=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Phikon model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/owkin/phikon."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = ViTModel.from_pretrained("owkin/phikon", add_pooling_layer=False)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Phikon model, make sure that you were granted access and that you correctly registered your token")

        mean, std = get_constants('imagenet')
        eval_transform = get_eval_transforms(mean, std, target_img_size=224, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)
        precision = torch.float32
        return model, eval_transform, precision
    
    def forward(self, x):
        out = self.forward_features(x)
        out = out.last_hidden_state[:, 0, :]
        return out
    
    def forward_features(self, x):
        out = self.model(pixel_values=x)
        return out
    

class HibouLInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Hibou initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        from transformers import AutoModel
        from torchvision.transforms import InterpolationMode

        self.enc_name = 'hibou_l'
        weights_path = self._get_weights_path()

        if weights_path:
            raise NotImplementedError("Hibou-Large doesn't support local model loading. PR welcome!")
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = AutoModel.from_pretrained("histai/hibou-L", trust_remote_code=True)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Hibou-L model, make sure that you were granted access and that you correctly registered your token")
        
        mean, std = get_constants('hibou')
        eval_transform = get_eval_transforms(mean, std, target_img_size=224, interpolation=InterpolationMode.BICUBIC, max_size=None, antialias=True)
        precision = torch.float32

        return model, eval_transform, precision
    
    def forward(self, x):
        out = self.forward_features(x)
        out = out.pooler_output
        return out
    
    def forward_features(self, x):
        out = self.model(pixel_values=x)
        return out


class KaikoInferenceEncoder(BasePatchEncoder):
    MODEL_NAME = None  # set in subclasses
    HF_HUB_ID = None # set in subclasses
    IMG_SIZE = None

    def __init__(self, **build_kwargs):
        """
        Kaiko initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        import timm
        from torchvision.transforms import InterpolationMode
        self.enc_name = f"kaiko-{self.MODEL_NAME}"
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model = timm.create_model(
                    f"{self.HF_HUB_ID}",
                    num_classes=0,
                    checkpoint_path=weights_path,
                    img_size=self.IMG_SIZE,
                    dynamic_img_size=True
                )
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Kaiko model from local checkpoint at '{weights_path}'. "
                    "You can download the required `model.safetensors` and `config.yaml` from: https://huggingface.co/collections/1aurent/kaikoai-models-66636c99d8e1e34bc6dcf795."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model(
                    model_name=f"hf-hub:1aurent/{self.HF_HUB_ID}.kaiko_ai_towards_large_pathology_fms",
                    dynamic_img_size=True,
                    pretrained=True,
                    num_classes=0,
                    img_size=self.IMG_SIZE,
                )
            except:
                traceback.print_exc()
                raise Exception("Failed to download Kaiko model.")

        mean, std = get_constants("kaiko")
        eval_transform = get_eval_transforms(mean, std, target_img_size=224, center_crop=True, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)
        precision = torch.float32

        return model, eval_transform, precision

    def forward(self, x):
        return self.model(x)


class KaikoS16InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vits16"
    HF_HUB_ID = "vit_small_patch16_224"
    IMG_SIZE = 224

    def __init__(self, **build_kwargs):
        """
        Kaiko Small 16 initialization.
        """
        super().__init__(**build_kwargs)
    

class KaikoS8InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vits8"
    HF_HUB_ID = "vit_small_patch8_224"
    IMG_SIZE = 224

    def __init__(self, **build_kwargs):
        """
        Kaiko Small 8 initialization.
        """
        super().__init__(**build_kwargs)
    

class KaikoB16InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vitb16"
    HF_HUB_ID = "vit_base_patch16_224"
    IMG_SIZE = 224

    def __init__(self, **build_kwargs):
        """
        Kaiko Base 16 initialization.
        """
        super().__init__(**build_kwargs)
    

class KaikoB8InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vitb8"
    HF_HUB_ID = "vit_base_patch8_224"
    IMG_SIZE = 224

    def __init__(self, **build_kwargs):
        """
        Kaiko Base 8 initialization.
        """
        super().__init__(**build_kwargs)
    

class KaikoL14InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vitl14"
    HF_HUB_ID = "vit_large_patch14_reg4_dinov2"
    IMG_SIZE = 518

    def __init__(self, **build_kwargs):
        """
        Kaiko Large 14 initialization.
        """
        super().__init__(**build_kwargs)
    

class ResNet50InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        ResNet50-ImageNet initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self, 
        pretrained=True, 
        timm_kwargs={"features_only": True, "out_indices": [3], "num_classes": 0},
        img_size=224,
        pool=True
    ):
        import timm
        from torchvision.transforms import InterpolationMode

        self.enc_name = 'resnet50'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model = timm.create_model("resnet50", pretrained=False, **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=False)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create ResNet50 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/timm/resnet50.tv_in1k."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("resnet50.tv_in1k", pretrained=pretrained, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download ResNet50 model.")

        mean, std = get_constants('imagenet')
        eval_transform = get_eval_transforms(mean, std, target_img_size=img_size, center_crop=True, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)

        precision = torch.float32
        if pool:
            self.pool = torch.nn.AdaptiveAvgPool2d(1)
        else:
            self.pool = None
        
        return model, eval_transform, precision
    
    def forward(self, x):
        out = self.forward_features(x)
        if self.pool:
            out = self.pool(out).squeeze(-1).squeeze(-1)
        return out
    
    def forward_features(self, x):
        out = self.model(x)
        if isinstance(out, list):
            assert len(out) == 1
            out = out[0]
        return out


class LunitS8InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Lunit initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        import timm
        from timm.data import resolve_model_data_config
        from timm.data.transforms_factory import create_transform

        self.enc_name = 'lunit-vits8'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {"img_size": 224}
                model = timm.create_model("vit_small_patch8_224", checkpoint_path=weights_path, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Lunit-Small model from local checkpoint at '{weights_path}'. "
                    "You can download the required `model.safetensors` and `config.yaml` from: https://huggingface.co/1aurent/vit_small_patch8_224.lunit_dino."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:1aurent/vit_small_patch8_224.lunit_dino", pretrained=True)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Lunit S8 model, make sure that you were granted access and that you correctly registered your token.")

        data_config = resolve_model_data_config(model)
        eval_transform = create_transform(**data_config, is_training=False)
        precision = torch.float32

        return model, eval_transform, precision
    

class UNIInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        UNI initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self, 
        timm_kwargs={"dynamic_img_size": True, "num_classes": 0, "init_values": 1e-5}
    ):
        import timm
        from torchvision import transforms

        self.enc_name = 'uni_v1'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    'img_size': 224,
                    'patch_size': 16,
                    'init_values': 1e-5,
                    'num_classes': 0,
                    'dynamic_img_size': True,
                }
                model = timm.create_model("vit_large_patch16_224", **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create UNI model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/MahmoodLab/UNI."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:MahmoodLab/uni", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download UNI model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose([
            transforms.Resize(224),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ])

        precision = torch.float16
        return model, eval_transform, precision
    

class UNIv2InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        UNIv2 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        import timm
        from torchvision import transforms

        self.enc_name = 'uni_v2'
        weights_path = self._get_weights_path()

        timm_kwargs = {
            'img_size': 224,
            'patch_size': 14,
            'depth': 24,
            'num_heads': 24,
            'init_values': 1e-5,
            'embed_dim': 1536,
            'mlp_ratio': 2.66667 * 2,
            'num_classes': 0,
            'no_embed_class': True,
            'mlp_layer': timm.layers.SwiGLUPacked,
            'act_layer': torch.nn.SiLU,
            'reg_tokens': 8,
            'dynamic_img_size': True
        }

        if weights_path:
            try:
                model = timm.create_model(model_name='vit_giant_patch14_224', pretrained=False, **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create UNI2-h model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/MahmoodLab/UNI2-h."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:MahmoodLab/UNI2-h", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download UNI v2 model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose([
            transforms.Resize(224),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ])

        precision = torch.bfloat16
        return model, eval_transform, precision
    

class GigaPathInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        GigaPath initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self, 
    ):
        import timm
        assert timm.__version__ == '0.9.16', f"Gigapath requires timm version 0.9.16, but found {timm.__version__}. Please install the correct version using `pip install timm==0.9.16`"
        from torchvision import transforms

        self.enc_name = 'gigapath'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "img_size": 224,
                    "in_chans": 3,
                    "patch_size": 16,
                    "embed_dim": 1536,
                    "depth": 40,
                    "num_heads": 24,
                    "mlp_ratio": 5.33334,
                    "num_classes": 0
                }
                model = timm.create_model("vit_giant_patch14_dinov2", pretrained=False, **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create GigaPath model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/prov-gigapath/prov-gigapath."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf_hub:prov-gigapath/prov-gigapath", pretrained=True)
            except:
                traceback.print_exc()
                raise Exception("Failed to download GigaPath model, make sure that you were granted access and that you correctly registered your token")

        mean, std = get_constants('imagenet')
        eval_transform = transforms.Compose(
            [
                transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(mean, std),
            ]
        )
        precision = torch.float32
        return model, eval_transform, precision

    
class VirchowInferenceEncoder(BasePatchEncoder):
    import timm
    
    def __init__(self, **build_kwargs):
        """
        Virchow initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self,
        return_cls=False,
        timm_kwargs={'mlp_layer': timm.layers.SwiGLUPacked, 'act_layer': torch.nn.SiLU}
    ):
        import timm
        import torchvision
        from torchvision import transforms

        self.enc_name = 'virchow'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "img_size": 224,
                    "init_values": 1e-5,
                    "num_classes": 0,
                    "mlp_ratio": 5.3375,
                    "global_pool": "",
                    "dynamic_img_size": True,
                    'mlp_layer': timm.layers.SwiGLUPacked,
                    'act_layer': torch.nn.SiLU,
                }
                model = timm.create_model("vit_huge_patch14_224", **timm_kwargs)
                model.load_state_dict(state_dict=torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Virchow model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/paige-ai/Virchow."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:paige-ai/Virchow", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Virchow model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose(
            [
                transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ]
        )
        precision = torch.float16
        self.return_cls = return_cls
        
        return model, eval_transform, precision

    def forward(self, x):
        output = self.model(x)
        class_token = output[:, 0]

        if self.return_cls:
            return class_token
        else:
            patch_tokens = output[:, 1:]
            embeddings = torch.cat([class_token, patch_tokens.mean(1)], dim=-1)
            return embeddings


class Virchow2InferenceEncoder(BasePatchEncoder):
    import timm
    
    def __init__(self, **build_kwargs):
        """
        Virchow 2 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self,
        return_cls=False,
        timm_kwargs={'mlp_layer': timm.layers.SwiGLUPacked, 'act_layer': torch.nn.SiLU}
    ):
        import timm
        import torchvision
        from torchvision import transforms

        self.enc_name = 'virchow2'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "img_size": 224,
                    "init_values": 1e-5,
                    "num_classes": 0,
                    "reg_tokens": 4,
                    "mlp_ratio": 5.3375,
                    "global_pool": "",
                    "dynamic_img_size": True,
                    'mlp_layer': timm.layers.SwiGLUPacked,
                    'act_layer': torch.nn.SiLU,
                }
                model = timm.create_model("vit_huge_patch14_224", **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Virchow2 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/paige-ai/Virchow2."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:paige-ai/Virchow2", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Virchow-2 model, make sure that you were granted access and that you correctly registered your token")
        
        eval_transform = transforms.Compose(
            [
                transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ]
        )
        precision = torch.float16
        self.return_cls = return_cls
        
        return model, eval_transform, precision

    def forward(self, x):
        output = self.model(x)
    
        class_token = output[:, 0]
        if self.return_cls:
            return class_token
        
        patch_tokens = output[:, 5:]
        embedding = torch.cat([class_token, patch_tokens.mean(1)], dim=-1)
        return embedding


class HOptimus0InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        H-Optimus0 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self,
        timm_kwargs={'init_values': 1e-5, 'dynamic_img_size': False}
    ):
        import timm
        assert timm.__version__ == '0.9.16', f"H-Optimus requires timm version 0.9.16, but found {timm.__version__}. Please install the correct version using `pip install timm==0.9.16`"
        from torchvision import transforms

        self.enc_name = 'hoptimus0'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "num_classes": 0,
                    "img_size": 224,
                    "global_pool": "token",
                    'init_values': 1e-5,
                    'dynamic_img_size': False
                }
                model = timm.create_model("vit_giant_patch14_reg4_dinov2", **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create H-Optimus-0 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/bioptimus/H-optimus-0."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:bioptimus/H-optimus-0", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download HOptimus-0 model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose([
            transforms.Resize(224),  
            transforms.ToTensor(),
            transforms.Normalize(
                mean=(0.707223, 0.578729, 0.703617), 
                std=(0.211883, 0.230117, 0.177517)
            ),
        ])
        
        precision = torch.float16
        return model, eval_transform, precision


class HOptimus1InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        H-Optimus1 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self,
        timm_kwargs={'init_values': 1e-5, 'dynamic_img_size': False},
        **kwargs
    ):
        import timm
        assert timm.__version__ == '0.9.16', f"H-Optimus requires timm version 0.9.16, but found {timm.__version__}. Please install the correct version using `pip install timm==0.9.16`"
        from torchvision import transforms

        self.enc_name = 'hoptimus1'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "num_classes": 0,
                    "img_size": 224,
                    "global_pool": "token",
                    'init_values': 1e-5,
                    'dynamic_img_size': False
                }
                model = timm.create_model("vit_giant_patch14_reg4_dinov2", **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create H-Optimus-1 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/bioptimus/H-optimus-1."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:bioptimus/H-optimus-1", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download HOptimus-1 model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose([
            transforms.Resize(224),  
            transforms.ToTensor(),
            transforms.Normalize(
                mean=(0.707223, 0.578729, 0.703617), 
                std=(0.211883, 0.230117, 0.177517)
            ),
        ])
        
        precision = torch.float16
        return model, eval_transform, precision


class Phikonv2InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Phikonv2 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        from transformers import AutoModel
        import torchvision.transforms as T
        from .utils.constants import IMAGENET_MEAN, IMAGENET_STD

        self.enc_name = 'phikon_v2'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model_dir = os.path.dirname(weights_path)
                model = AutoModel.from_pretrained(model_dir)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Phikonv2 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `model.safetensors` and `config.json` from: https://huggingface.co/owkin/phikon-v2."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = AutoModel.from_pretrained("owkin/phikon-v2")
            except:
                traceback.print_exc()
                raise Exception("Failed to download Phikon v2 model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = T.Compose([
            T.Resize(224),  
            T.CenterCrop(224),  
            T.ToTensor(),
            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)  # Normalize with specified mean and std
        ])

        precision = torch.float32
        return model, eval_transform, precision
    
    def forward(self, x):
        out = self.model(x)
        out = out.last_hidden_state[:, 0, :]
        return out


class Conchv15InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        CONCHv1.5 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, img_size=448):
        from trident.patch_encoder_models.model_zoo.conchv1_5.conchv1_5 import create_model_from_pretrained

        self.enc_name = 'conch_v15'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model, eval_transform = create_model_from_pretrained(checkpoint_path=weights_path, img_size=img_size)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create CONCH v1.5 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model_vision.bin` and `config.json` from: https://huggingface.co/MahmoodLab/conchv1_5."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model, eval_transform = create_model_from_pretrained(checkpoint_path="hf_hub:MahmoodLab/conchv1_5", img_size=img_size)
            except:
                traceback.print_exc()
                raise Exception("Failed to download CONCH v1.5 model, make sure that you were granted access and that you correctly registered your token")

        precision = torch.float16
        return model, eval_transform, precision


class Midnight12kInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Midnight 12-k initialization by Kaiko.
        """
        super().__init__(**build_kwargs)

    def _build(self, return_type: Literal["cls_token", "cls+mean"] = "cls_token"):
        from transformers import AutoModel
        from .utils.constants import KAIKO_MEAN, KAIKO_STD
        from torchvision import transforms

        self.enc_name = "midnight12k"
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model_dir = os.path.dirname(weights_path)
                model = AutoModel.from_pretrained(model_dir)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Midnight-12k model from local checkpoint at '{weights_path}'. "
                    "You can download the required `model.safetensors` and `config.json` from: https://huggingface.co/kaiko-ai/midnight."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = AutoModel.from_pretrained("kaiko-ai/midnight")
            except:
                traceback.print_exc()
                raise Exception("Failed to download Midnight-12k model")

        eval_transform = transforms.Compose(
            [
                transforms.Resize(224),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(mean=KAIKO_MEAN, std=KAIKO_STD),
            ]
        )

        precision = torch.float32
        self.return_type = return_type
        return model, eval_transform, precision

    def forward(self, x):
        out = self.model(x).last_hidden_state
        cls_token = out[:, 0, :]
        if self.return_type == "cls_token":
            return cls_token
        elif self.return_type == "cls+mean":
            patch_embeddings = out[:, 1:, :]
            return torch.cat([cls_token, patch_embeddings.mean(1)], dim=-1)
        else:
            raise ValueError(
                f"expected return_type to be one of 'cls_token' or 'cls+mean', but got '{self.return_type}'"
            )



================================================
File: trident/patch_encoder_models/local_ckpts.json
================================================
{
    "conch_v1": "",
    "uni_v1": "",
    "uni_v2": "",
    "ctranspath": "",
    "phikon": "",
    "resnet50": "",
    "gigapath": "",
    "virchow": "",
    "virchow2": "",
    "hoptimus0": "",
    "hoptimus1": "",
    "phikon_v2": "",
    "hibou_l": "",
    "kaiko-vitb8": "",
    "kaiko-vitb16": "",
    "kaiko-vits8": "",
    "kaiko-vits16": "",
    "kaiko-vitl14": "",
    "lunit-vits8": "",
    "conch_v15": "",
    "musk": "",
    "custom_encoder": ""
}


================================================
File: trident/patch_encoder_models/model_zoo/__init__.py
================================================



================================================
File: trident/patch_encoder_models/model_zoo/conchv1_5/__init__.py
================================================




================================================
File: trident/patch_encoder_models/model_zoo/conchv1_5/conchv1_5.py
================================================
""" Modified based on https://github.com/bytedance/ibot/blob/da316d82636a7a7356835ef224b13d5f3ace0489/models/vision_transformer.py and timm (https://github.com/huggingface/pytorch-image-models) v0.9.2
"""
import math
from collections import OrderedDict
from functools import partial
from typing import Callable, List, Optional, Sequence, Tuple, Union

import timm
from timm.layers import Mlp, DropPath, trunc_normal_, PatchDropout, use_fused_attn
from timm.layers.helpers import to_2tuple
from timm.models._manipulate import named_apply, checkpoint_seq 
from timm.models.vision_transformer import init_weights_vit_timm, get_init_weights_vit, _load_weights 

from enum import Enum
from typing import Union
from typing import List, Optional, Callable

import torch
from torch import nn, einsum
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.jit import Final

from einops import rearrange, repeat
from einops_exts import rearrange_many


class Format(str, Enum):
    NCHW = 'NCHW'
    NHWC = 'NHWC'
    NCL = 'NCL'
    NLC = 'NLC'


FormatT = Union[str, Format]

def get_spatial_dim(fmt: FormatT):
    fmt = Format(fmt)
    if fmt is Format.NLC:
        dim = (1,)
    elif fmt is Format.NCL:
        dim = (2,)
    elif fmt is Format.NHWC:
        dim = (1, 2)
    else:
        dim = (2, 3)
    return dim


def get_channel_dim(fmt: FormatT):
    fmt = Format(fmt)
    if fmt is Format.NHWC:
        dim = 3
    elif fmt is Format.NLC:
        dim = 2
    else:
        dim = 1
    return dim


def nchw_to(x: torch.Tensor, fmt: Format):
    if fmt == Format.NHWC:
        x = x.permute(0, 2, 3, 1)
    elif fmt == Format.NLC:
        x = x.flatten(2).transpose(1, 2)
    elif fmt == Format.NCL:
        x = x.flatten(2)
    return x


def nhwc_to(x: torch.Tensor, fmt: Format):
    if fmt == Format.NCHW:
        x = x.permute(0, 3, 1, 2)
    elif fmt == Format.NLC:
        x = x.flatten(1, 2)
    elif fmt == Format.NCL:
        x = x.flatten(1, 2).transpose(1, 2)
    return x

class PatchEmbed(nn.Module):
    """ 2D Image to Patch Embedding
    """
    output_fmt: Format

    def __init__(
            self,
            img_size: int = 224,
            patch_size: int = 16,
            in_chans: int = 3,
            embed_dim: int = 768,
            norm_layer: Optional[Callable] = None,
            flatten: bool = True,
            output_fmt: Optional[str] = None,
            bias: bool = True,
            masked_im_modeling: bool = False
    ):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])
        self.num_patches = self.grid_size[0] * self.grid_size[1]
        if output_fmt is not None:
            self.flatten = False
            self.output_fmt = Format(output_fmt)
        else:
            # flatten spatial dim and transpose to channels last, kept for bwd compat
            self.flatten = flatten
            self.output_fmt = Format.NCHW

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

        ### Mask Image Modeling
        self.masked_im_modeling = masked_im_modeling
        if self.masked_im_modeling:
            self.masked_embed = nn.Parameter(torch.zeros(1, embed_dim))

    def forward(self, x, mask=None):
        B, C, H, W = x.shape
        x = self.proj(x)

        if mask is not None:
            x = self.mask_model(x, mask)

        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC
        elif self.output_fmt != Format.NCHW:
            x = nchw_to(x, self.output_fmt)
        x = self.norm(x)
        return x

    def mask_model(self, x, mask):
        x.permute(0, 2, 3, 1)[mask, :] = self.masked_embed.to(x.dtype)
        return x

class Attention(nn.Module):
    fused_attn: Final[bool]

    def __init__(
            self,
            dim,
            num_heads=8,
            qkv_bias=False,
            qk_norm=False,
            attn_drop=0.,
            proj_drop=0.,
            norm_layer=nn.LayerNorm,
    ):
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.fused_attn = use_fused_attn()
        self.fast_attn = self.fused_attn # legacy support

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, return_attention=False):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv.unbind(0)
        q, k = self.q_norm(q), self.k_norm(k)

        if self.fused_attn and (return_attention == False):
            x = F.scaled_dot_product_attention(
                q, k, v,
                dropout_p=self.attn_drop.p,
            )
        else:
            q = q * self.scale
            attn = q @ k.transpose(-2, -1)
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = attn @ v

        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)

        if return_attention:
            return x, attn
        return x

class LayerScale(nn.Module):
    def __init__(self, dim, init_values=1e-5, inplace=False):
        super().__init__()
        self.inplace = inplace
        self.gamma = nn.Parameter(init_values * torch.ones(dim))

    def forward(self, x):
        return x.mul_(self.gamma) if self.inplace else x * self.gamma

class Block(nn.Module):
    def __init__(
            self,
            dim,
            num_heads,
            mlp_ratio=4.,
            qkv_bias=False,
            qk_norm=False,
            proj_drop=0., # proj -> proj_drop renamed
            attn_drop=0.,
            init_values=None,
            drop_path=0.,
            act_layer=nn.GELU,
            norm_layer=nn.LayerNorm,
            mlp_layer=Mlp,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_norm=qk_norm,
            attn_drop=attn_drop,
            proj_drop=proj_drop,
            norm_layer=norm_layer,
        )
        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()

        self.norm2 = norm_layer(dim)
        self.mlp = mlp_layer( ### Mlp -> mlp_layer
            in_features=dim,
            hidden_features=int(dim * mlp_ratio),
            act_layer=act_layer,
            drop=proj_drop,
        )
        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
        return x

    def forward_with_attention(self, x):
        x_input = x
        x_postattn, attn = self.attn(self.norm1(x_input), return_attention=True)
        x_postls1 = x_input + self.drop_path1(self.ls1(x_postattn))
        x_postmlp = self.mlp(self.norm2(x_postls1))
        x_postls2 = x_postls1 + self.drop_path2(self.ls2(x_postmlp))
        return x_postls2, attn


class VisionTransformer(nn.Module):
    """ Vision Transformer
    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`
        - https://arxiv.org/abs/2010.11929
    Adapted entirely from: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py
    """

    def __init__(
            self,
            img_size: Union[int, Tuple[int, int]] = 224,
            patch_size: Union[int, Tuple[int, int]] = 16,
            in_chans: int = 3,
            num_classes: int = 0,
            global_pool: str = 'token',
            embed_dim: int = 768,
            depth: int = 12,
            num_heads: int = 12,
            mlp_ratio: float = 4.,
            qkv_bias: bool = True,
            qk_norm: bool = False,
            init_values: Optional[float] = None,
            class_token: bool = True,
            no_embed_class: bool = False,
            pre_norm: bool = False,
            fc_norm: Optional[bool] = None,
            drop_rate: float = 0.,
            pos_drop_rate: float = 0.,   # new
            patch_drop_rate: float = 0., # new
            proj_drop_rate: float = 0.,  # renamed
            attn_drop_rate: float = 0.,  # same
            drop_path_rate: float = 0.,  # same
            weight_init: str = '',
            embed_layer: Callable = PatchEmbed,
            norm_layer: Optional[Callable] = None,
            act_layer: Optional[Callable] = None,
            block_fn: Callable = Block,     
            mlp_layer: Callable = Mlp,  # New
            return_all_tokens=False,    # iBOT
            masked_im_modeling=False    # iBOT
    ):
        """
        Args:
            img_size: Input image size.
            patch_size: Patch size.
            in_chans: Number of image input channels.
            num_classes: Mumber of classes for classification head.
            global_pool: Type of global pooling for final sequence (default: 'token').
            embed_dim: Transformer embedding dimension.
            depth: Depth of transformer.
            num_heads: Number of attention heads.
            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
            qkv_bias: Enable bias for qkv projections if True.
            init_values: Layer-scale init values (layer-scale enabled if not None).
            class_token: Use class token.
            fc_norm: Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.
            drop_rate: Head dropout rate.
            pos_drop_rate: Position embedding dropout rate.
            attn_drop_rate: Attention dropout rate.
            drop_path_rate: Stochastic depth rate.
            weight_init: Weight initialization scheme.
            embed_layer: Patch embedding layer.
            norm_layer: Normalization layer.
            act_layer: MLP activation layer.
            block_fn: Transformer block layer.
        """
        super().__init__()
        assert global_pool in ('', 'avg', 'token')
        assert class_token or global_pool != 'token'
        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
        act_layer = act_layer or nn.GELU
        self.return_all_tokens = return_all_tokens     # from ibot
        self.masked_im_modeling = masked_im_modeling   # from ibot

        self.num_classes = num_classes
        self.global_pool = global_pool
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.num_prefix_tokens = 1 if class_token else 0
        self.no_embed_class = no_embed_class
        self.grad_checkpointing = False

        self.patch_embed = embed_layer(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=in_chans,
            embed_dim=embed_dim,
            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)
            masked_im_modeling=masked_im_modeling,
        )
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None
        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens
        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)
        self.pos_drop = nn.Dropout(p=pos_drop_rate)

        if patch_drop_rate > 0:
            self.patch_drop = PatchDropout(
                patch_drop_rate,
                num_prefix_tokens=self.num_prefix_tokens,
            )
        else:
            self.patch_drop = nn.Identity()
        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks = nn.Sequential(*[
            block_fn(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_norm=qk_norm,
                init_values=init_values,
                proj_drop=proj_drop_rate, # renamed
                attn_drop=attn_drop_rate,
                drop_path=dpr[i],
                norm_layer=norm_layer,
                act_layer=act_layer,
                mlp_layer=mlp_layer,  # new
            )
            for i in range(depth)])
        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()

        # Classifier Head
        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()
        self.head_drop = nn.Dropout(drop_rate) # new
        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        if weight_init != 'skip':
            self.init_weights(weight_init)

    def init_weights(self, mode=''):
        assert mode in ('jax', 'jax_nlhb', 'moco', '')
        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.
        trunc_normal_(self.pos_embed, std=.02)
        if self.cls_token is not None:
            nn.init.normal_(self.cls_token, std=1e-6)
        named_apply(get_init_weights_vit(mode, head_bias), self)

    def _init_weights(self, m):
        # this fn left here for compat with downstream users
        init_weights_vit_timm(m)

    @torch.jit.ignore()
    def load_pretrained(self, checkpoint_path, prefix=''):
        _load_weights(self, checkpoint_path, prefix)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token', 'dist_token'}

    @torch.jit.ignore
    def group_matcher(self, coarse=False):
        return dict(
            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed
            blocks=[(r'^blocks\.(\d+)', None), (r'^norm', (99999,))]
        )

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable=True):
        self.grad_checkpointing = enable

    @torch.jit.ignore
    def get_classifier(self):
        return self.head

    def reset_classifier(self, num_classes: int, global_pool=None):
        self.num_classes = num_classes
        if global_pool is not None:
            assert global_pool in ('', 'avg', 'token')
            self.global_pool = global_pool
        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()

    def _pos_embed(self, x, w, h):
        if self.no_embed_class:
            # deit-3, updated JAX (big vision)
            # position embedding does not overlap with class token, add then concat
            x = x + self.interpolate_pos_encoding(x, w, h)
            if self.cls_token is not None:
                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
        else:
            # original timm, JAX, and deit vit impl
            # pos_embed has entry for class token, concat then add
            if self.cls_token is not None:
                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
            x = x + self.interpolate_pos_encoding(x, w, h)
        return self.pos_drop(x)

    def _intermediate_layers(
            self,
            x: torch.Tensor,
            n: Union[int, Sequence] = 1,
    ):
        outputs, num_blocks = [], len(self.blocks)
        take_indices = set(range(num_blocks - n, num_blocks) if isinstance(n, int) else n)

        # forward pass
        x = self.patch_embed(x)
        x = self._pos_embed(x)
        x = self.patch_drop(x)
        x = self.norm_pre(x)
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if i in take_indices:
                outputs.append(x)

        return outputs

    def get_intermediate_layers(
            self,
            x: torch.Tensor,
            n: Union[int, Sequence] = 1,
            reshape: bool = False,
            return_class_token: bool = False,
            norm: bool = False,
    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:
        # take last n blocks if n is an int, if in is a sequence, select by matching indices
        outputs = self._intermediate_layers(x, n)
        if norm:
            outputs = [self.norm(out) for out in outputs]
        class_tokens = [out[:, 0:self.num_prefix_tokens] for out in outputs]
        outputs = [out[:, self.num_prefix_tokens:] for out in outputs]

        if reshape:
            grid_size = self.patch_embed.grid_size
            outputs = [
                out.reshape(x.shape[0], grid_size[0], grid_size[1], -1).permute(0, 3, 1, 2).contiguous()
                for out in outputs
            ]

        if return_class_token:
            return tuple(zip(outputs, class_tokens))
        return tuple(outputs)

    def forward_features(self, x):
        B, nc, w, h = x.shape
        x = self.patch_embed(x)

        x = self._pos_embed(x, w, h)
        x = self.patch_drop(x)
        x = self.norm_pre(x)
        if self.grad_checkpointing and not torch.jit.is_scripting():
            x = checkpoint_seq(self.blocks, x)
        else:
            x = self.blocks(x)
        x = self.norm(x)
        return x

    def forward_head(self, x, pre_logits: bool = False):
        if self.global_pool:
            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
        x = self.fc_norm(x)
        x = self.head_drop(x) # new
        return x if pre_logits else self.head(x)

    def get_attention(self, x, block_num: int=-1):
        B, nc, w, h = x.shape
        x = self.patch_embed(x)
        x = self._pos_embed(x, w, h)
        x = self.patch_drop(x)
        x = self.norm_pre(x)

        if block_num < 0:
            block_num = len(self.blocks) + block_num

        if self.grad_checkpointing and not torch.jit.is_scripting():
            raise NotImplementedError
        else:
            for i, blk in enumerate(self.blocks):
                if i < block_num:
                    x = blk(x)
                else:
                    x, attn = blk.forward_with_attention(x)
                    return attn

    def forward(self, x, return_all_tokens=None):
        x = self.forward_features(x)

        return_all_tokens = self.return_all_tokens if \
            return_all_tokens is None else return_all_tokens
        if return_all_tokens:
            return x

        x = self.forward_head(x)
        return x

    def interpolate_pos_encoding(self, x, w, h):
        npatch = x.shape[1] - 1
        N = self.pos_embed.shape[1] - 1
        if npatch == N and w == h:
            return self.pos_embed
        class_pos_embed = self.pos_embed[:, 0]
        patch_pos_embed = self.pos_embed[:, 1:]
        dim = x.shape[-1]
        w0 = w // self.patch_embed.patch_size[0]
        h0 = h // self.patch_embed.patch_size[0]
        # we add a small number to avoid floating point error in the interpolation
        # see discussion at https://github.com/facebookresearch/dino/issues/8
        w0, h0 = w0 + 0.1, h0 + 0.1
        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),
            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),
            mode='bicubic',
        )
        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]
        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)

def vit_large(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True, **kwargs)
    return model

def resize_pos_embed(model, pos_embed_w, verbose=True):
    resized = False
    if pos_embed_w.shape != model.pos_embed.shape:
        # see https://github.com/rwightman/pytorch-image-models/blob/624266148d8fa5ddb22a6f5e523a53aaf0e8a9eb/timm/models/vision_transformer.py#L509
        interpolation = 'bilinear'
        antialias = False
        try:
            from timm.layers import resample_abs_pos_embed
        except ImportError:
            print(f'{__file__}: import timm utility functions failed with version {timm.__version__}!')
        num_prefix_tokens = 0 if getattr(model, 'no_embed_class', False) else getattr(model, 'num_prefix_tokens', 1)
        pos_embed_w = resample_abs_pos_embed(  # resize pos embedding when different size from pretrained weights
                    pos_embed_w,
                    new_size=model.patch_embed.grid_size,
                    num_prefix_tokens=num_prefix_tokens,
                    interpolation=interpolation,
                    antialias=antialias,
                    verbose=verbose,
                )

        resized = True
    if not resized and verbose:
        print('pos embedding not resized.')
    return pos_embed_w


class AttentionalPooler(nn.Module):

    def __init__(
            self,
            d_model: int,
            context_dim: int,
            n_head: int = 8,
            n_queries: int = 256,
            norm_layer: Callable = nn.LayerNorm
    ):
        super().__init__()
        self.query = nn.Parameter(torch.randn(n_queries, d_model))
        dim_head = d_model // n_head
        self.scale = dim_head ** -0.5
        self.heads = n_head
        inner_dim = dim_head * n_head
        self.ln_k = norm_layer(context_dim)
        self.ln_q = norm_layer(d_model)
        self.to_q = nn.Linear(d_model, inner_dim, bias=False)
        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)
        self.to_out = nn.Linear(inner_dim, d_model, bias=False)
    
    def forward(self, x: torch.Tensor):
        if x.ndim == 3:
            x = rearrange(x, 'b n d -> b 1 n d')
        q = repeat(self.query, 'n d -> b m n d', b=x.shape[0], m=x.shape[1])
        x = self.ln_k(x)
        q = self.ln_q(q)
        b, m, h = *x.shape[:2], self.heads
        q = self.to_q(q)
        kv_input = x
        k, v = self.to_kv(kv_input).chunk(2, dim=-1)
        q, k, v = rearrange_many((q, k, v), 'b t n (h d) -> b h t n d', h=h)
        q = q * self.scale
        # attention
        sim = einsum('... i d, ... j d  -> ... i j', q, k)
        sim = sim - sim.amax(dim=-1, keepdim=True).detach()
        attn = sim.softmax(dim=-1)
        out = einsum('... i j, ... j d -> ... i d', attn, v)
        out = rearrange(out, 'b h t n d -> b t n (h d)', h=h)
        return self.to_out(out).squeeze(dim=1)


class CONCHVisionTower(nn.Module):

    def __init__(self):
        super().__init__()
        self.trunk = vit_large(init_values=1.0)
        self.attn_pool_contrast = AttentionalPooler(d_model=768, context_dim=1024, n_head=8, n_queries=1)
        self.ln_contrast = nn.LayerNorm(768)

    def forward(self, x):
        x = self.trunk.forward_features(x)
        x = self.attn_pool_contrast(x)[:, 0]
        x = self.ln_contrast(x)
        return x


def create_model_from_pretrained(
        checkpoint_path: str,
        cache_dir: Optional[str] = None,
        img_size: int = 448
    ):

    import torchvision.transforms as T
    from ...utils.constants import IMAGENET_MEAN, IMAGENET_STD
    model = CONCHVisionTower()
    # download checkpoint from huggingface if providing hub address 
    if checkpoint_path.startswith("hf_hub:"): 
        from huggingface_hub import hf_hub_download
        _ = hf_hub_download(
            checkpoint_path[len("hf_hub:"):], 
            cache_dir=cache_dir,
            filename="meta.yaml",
        )
        checkpoint_path = hf_hub_download(
            checkpoint_path[len("hf_hub:"):], 
            cache_dir=cache_dir,
            filename="pytorch_model_vision.bin",
        )

    # restore checkpoint 
    state_dict = torch.load(checkpoint_path, map_location="cpu", weights_only=True)
    state_dict['trunk.pos_embed'] = resize_pos_embed(model.trunk, state_dict['trunk.pos_embed'], verbose=False)
    model.load_state_dict(state_dict, strict=True)

    eval_transform = T.Compose([
            T.Resize(img_size, interpolation=T.InterpolationMode.BILINEAR),
            T.CenterCrop(img_size),
            T.ToTensor(),
            T.Normalize(IMAGENET_MEAN, IMAGENET_STD)
    ])


    return model, eval_transform


if __name__ == '__main__':
    # test 448 x 448 image
    x = torch.randn(1, 3, 448, 448)
    model, eval_transform = create_model_from_pretrained("./conch_v1_5_official/pytorch_model_vision.bin")
    model.eval()
    out = model(x)
    print(out.shape)


================================================
File: trident/patch_encoder_models/model_zoo/ctranspath/__init__.py
================================================



================================================
File: trident/patch_encoder_models/model_zoo/ctranspath/ctran.py
================================================
"""
Credits to original CTransPath implementation: https://github.com/Xiyue-Wang/TransPath/blob/main/ctran.py
"""

from timm_ctp.models.layers.helpers import to_2tuple
from timm_ctp import create_model as ctp_create_model
import torch.nn as nn
import pdb


class ConvStem(nn.Module):

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):
        super().__init__()

        assert patch_size == 4
        assert embed_dim % 8 == 0

        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])
        self.num_patches = self.grid_size[0] * self.grid_size[1]
        self.flatten = flatten


        stem = []
        input_dim, output_dim = 3, embed_dim // 8
        for l in range(2):
            stem.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=2, padding=1, bias=False))
            stem.append(nn.BatchNorm2d(output_dim))
            stem.append(nn.ReLU(inplace=True))
            input_dim = output_dim
            output_dim *= 2
        stem.append(nn.Conv2d(input_dim, embed_dim, kernel_size=1))
        self.proj = nn.Sequential(*stem)

        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

    def forward(self, x):
        B, C, H, W = x.shape
        # assert H == self.img_size[0] and W == self.img_size[1], \
        #     f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
        x = self.norm(x)
        return x

def ctranspath(img_size = 224, **kwargs):
    model = ctp_create_model('swin_tiny_patch4_window7_224', 
                                  embed_layer=ConvStem, 
                                  pretrained=False,
                                  img_size=img_size,
                                  **kwargs)
    return model



================================================
File: trident/patch_encoder_models/utils/__init__.py
================================================



================================================
File: trident/patch_encoder_models/utils/constants.py
================================================
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]
OPENAI_MEAN = [0.48145466, 0.4578275, 0.40821073]
OPENAI_STD = [0.26862954, 0.26130258, 0.27577711]
HIBOU_MEAN = [0.7068, 0.5755, 0.722]
HIBOU_STD = [0.195, 0.2316, 0.1816]
KAIKO_MEAN = [0.5, 0.5, 0.5]
KAIKO_STD = [0.5, 0.5, 0.5]
NONE_MEAN = None
NONE_STD = None

def get_constants(norm='imagenet'):
    if norm == 'imagenet':
        return IMAGENET_MEAN, IMAGENET_STD
    elif norm == 'openai_clip':
        return OPENAI_MEAN, OPENAI_STD
    elif norm == 'hibou':
        return HIBOU_MEAN, HIBOU_STD
    elif norm == 'none':
        return NONE_MEAN, NONE_STD
    elif norm == 'kaiko':
        return KAIKO_MEAN, KAIKO_STD
    else:
        raise ValueError(f"Invalid norm: {norm}")



================================================
File: trident/patch_encoder_models/utils/transform_utils.py
================================================
from torchvision import transforms

def get_eval_transforms(mean, std, target_img_size = -1, center_crop = False, **resize_kwargs):
    trsforms = []
    
    if target_img_size > 0:
        trsforms.append(transforms.Resize(target_img_size, **resize_kwargs))
    if center_crop:
        assert target_img_size > 0, "target_img_size must be set if center_crop is True"
        trsforms.append(transforms.CenterCrop(target_img_size))
        
    
    trsforms.append(transforms.ToTensor())
    if mean is not None and std is not None:
        trsforms.append(transforms.Normalize(mean, std))
    trsforms = transforms.Compose(trsforms)

    return trsforms


================================================
File: trident/segmentation_models/__init__.py
================================================
# in submodule
from trident.segmentation_models.load import (
    segmentation_model_factory,
    HESTSegmenter,
    GrandQCSegmenter,
    GrandQCArtifactSegmenter
)

__all__ = [
    "segmentation_model_factory",
    "HESTSegmenter",
    "GrandQCSegmenter",
    "GrandQCArtifactSegmenter",
    ]



================================================
File: trident/segmentation_models/load.py
================================================
import os
import torch
import torch.nn.functional as F
from torch import nn
from torchvision import transforms
from abc import abstractmethod

from trident.IO import get_dir, get_weights_path, has_internet_connection


class SegmentationModel(torch.nn.Module):

    _has_internet = has_internet_connection()

    def __init__(self, freeze=True, confidence_thresh=0.5, **build_kwargs):
        """
        Initialize Segmentation model wrapper.

        Args:
            freeze (bool, optional): If True, the model's parameters are frozen 
                (i.e., not trainable) and the model is set to evaluation mode. 
                Defaults to True.
            confidence_thresh (float, optional): Threshold for prediction confidence. 
                Predictions below this threshold may be filtered out or ignored. 
                Default is 0.5. Set to 0.4 to keep more tissue.
            **build_kwargs: Additional keyword arguments passed to the internal 
                `_build` method.

        Attributes:
            model (torch.nn.Module): The constructed model.
            eval_transforms (Callable): Transformations to apply to input data during inference.
        """
        super().__init__()
        self.model, self.eval_transforms = self._build(**build_kwargs)
        self.confidence_thresh = confidence_thresh

        # Set all parameters to be non-trainable
        if freeze and self.model is not None:
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.eval()
            
    def forward(self, image):
        """
        Can be overwritten if model requires special forward pass.
        """
        z = self.model(image)
        return z
        
    @abstractmethod
    def _build(self, **build_kwargs) -> tuple[nn.Module, transforms.Compose]:
        """
        Build the segmentation model and preprocessing transforms.
        """
        pass


class HESTSegmenter(SegmentationModel):

    def __init__(self, **build_kwargs):
        """
        HESTSegmenter initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        """
        Build and load HESTSegmenter model.

        Returns:
            Tuple[nn.Module, transforms.Compose]: Model and preprocessing transforms.
        """

        from torchvision.models.segmentation import deeplabv3_resnet50

        model_ckpt_name = 'deeplabv3_seg_v4.ckpt'
        weights_path = get_weights_path('seg', 'hest')

        # Check if a path is provided but doesn't exist
        if weights_path and not os.path.isfile(weights_path):
            raise FileNotFoundError(f"Expected checkpoint at '{weights_path}', but the file was not found.")

        # Initialize base model
        model = deeplabv3_resnet50(weights=None)
        model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1, stride=1)

        if not weights_path:
            if not SegmentationModel._has_internet:
                raise FileNotFoundError(
                    f"Internet connection not available and checkpoint not found locally in model registry at trident/segmentation_models/local_ckpts.json.\n\n"
                    f"To proceed, please manually download {model_ckpt_name} from:\n"
                    f"https://huggingface.co/MahmoodLab/hest-tissue-seg/\n"
                    f"and place it at:\nlocal_ckpts.json"
                )

            # If internet is available, download from HuggingFace
            from huggingface_hub import snapshot_download
            checkpoint_dir = snapshot_download(
                repo_id="MahmoodLab/hest-tissue-seg",
                repo_type='model',
                local_dir=get_dir(),
                cache_dir=get_dir(),
                allow_patterns=[model_ckpt_name]
            )

            weights_path = os.path.join(checkpoint_dir, model_ckpt_name)

        # Load and clean checkpoint
        checkpoint = torch.load(weights_path, map_location='cpu')
        state_dict = {
            k.replace('model.', ''): v
            for k, v in checkpoint.get('state_dict', {}).items()
            if 'aux' not in k
        }

        model.load_state_dict(state_dict)

        # Store configuration
        self.input_size = 512
        self.precision = torch.float16
        self.target_mag = 10

        eval_transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406),
                                 std=(0.229, 0.224, 0.225))
        ])

        return model, eval_transforms
    
    def forward(self, image: torch.Tensor) -> torch.Tensor:
        # input should be of shape (batch_size, C, H, W)
        assert len(image.shape) == 4, f"Input must be 4D image tensor (shape: batch_size, C, H, W), got {image.shape} instead"
        logits = self.model(image)['out']
        softmax_output = F.softmax(logits, dim=1)
        predictions = (softmax_output[:, 1, :, :] > self.confidence_thresh).to(torch.uint8)  # Shape: [bs, 512, 512]
        return predictions
        

class JpegCompressionTransform:
    def __init__(self, quality=80):
        self.quality = quality

    def __call__(self, image):
        import cv2
        import numpy as np
        from PIL import Image
        # Convert PIL Image to NumPy array
        image = np.array(image)

        # Apply JPEG compression
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.quality]
        _, image = cv2.imencode('.jpg', image, encode_param)
        image = cv2.imdecode(image, cv2.IMREAD_COLOR)

        # Convert back to PIL Image
        return Image.fromarray(image)


class GrandQCArtifactSegmenter(SegmentationModel):

    _class_mapping = {
        1: "Normal Tissue",
        2: "Fold",
        3: "Darkspot & Foreign Object",
        4: "PenMarking",
        5: "Edge & Air Bubble",
        6: "OOF",
        7: "Background"
    }

    def __init__(self, **build_kwargs):
        """
        GrandQCArtifactSegmenter initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, remove_penmarks_only=False):
        """
        Load the GrandQC artifact removal segmentation model.
        Credit: https://www.nature.com/articles/s41467-024-54769-y
        """

        import segmentation_models_pytorch as smp

        self.remove_penmarks_only = remove_penmarks_only  # ignore all other artifacts than penmakrs.
        model_ckpt_name = 'GrandQC_MPP1_state_dict.pth'
        encoder_name = 'timm-efficientnet-b0'
        encoder_weights = 'imagenet'
        weights_path = get_weights_path('seg', 'grandqc_artifact')

        # Verify that user-provided weights_path is valid
        if weights_path and not os.path.isfile(weights_path):
            raise FileNotFoundError(
                f"Expected checkpoint at '{weights_path}', but the file was not found."
            )

        # Initialize model
        model = smp.Unet(
            encoder_name=encoder_name,
            encoder_weights=encoder_weights,
            classes=8,
            activation=None,
        )

        # Attempt to download if file is missing and not already available
        if not weights_path:
            if not SegmentationModel._has_internet:
                raise FileNotFoundError(
                    f"Internet connection not available and checkpoint not found locally.\n\n"
                    f"To proceed, please manually download {model_ckpt_name} from:\n"
                    f"https://huggingface.co/MahmoodLab/hest-tissue-seg/\n"
                    f"and place it at:\nlocal_ckpts.json"
                )

            from huggingface_hub import snapshot_download
            checkpoint_dir = snapshot_download(
                repo_id="MahmoodLab/hest-tissue-seg",
                repo_type='model',
                local_dir=get_dir(),
                cache_dir=get_dir(),
                allow_patterns=[model_ckpt_name],
            )

            weights_path = os.path.join(checkpoint_dir, model_ckpt_name)

        # Load checkpoint
        state_dict = torch.load(weights_path, map_location='cpu', weights_only=True)
        model.load_state_dict(state_dict)

        # Model config
        self.input_size = 512
        self.precision = torch.float32
        self.target_mag = 10

        # Evaluation transforms
        eval_transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        return model, eval_transforms

    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """
        Custom forward pass.
        """
        logits = self.model.predict(image)
        probs = torch.softmax(logits, dim=1)  
        _, predicted_classes = torch.max(probs, dim=1)  
        if self.remove_penmarks_only:
            predictions = torch.where((predicted_classes == 4) | (predicted_classes == 7), 0, 1)
        else:
            predictions = torch.where(predicted_classes > 1, 0, 1)
        predictions = predictions.to(torch.uint8)

        return predictions


class GrandQCSegmenter(SegmentationModel):
    
    def __init__(self, **build_kwargs):
        """
        GrandQCSegmenter initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        """
        Load the GrandQC tissue detection segmentation model.
        Credit: https://www.nature.com/articles/s41467-024-54769-y
        """
        import segmentation_models_pytorch as smp

        model_ckpt_name = 'Tissue_Detection_MPP10.pth'
        encoder_name = 'timm-efficientnet-b0'
        encoder_weights = 'imagenet'
        weights_path = get_weights_path('seg', 'grandqc') 

        # Verify that user-provided weights_path is valid
        if weights_path and not os.path.isfile(weights_path):
            raise FileNotFoundError(
                f"Expected checkpoint at '{weights_path}', but the file was not found."
            )

        # Verify checkpoint path
        if not weights_path:
            if not SegmentationModel._has_internet:
                raise FileNotFoundError(
                    f"Internet connection not available and checkpoint not found locally at '{weights_path}'.\n\n"
                    f"To proceed, please manually download {model_ckpt_name} from:\n"
                    f"https://huggingface.co/MahmoodLab/hest-tissue-seg/\n"
                    f"and place it at:\nlocal_ckpts.json"
                )

            from huggingface_hub import snapshot_download
            checkpoint_dir = snapshot_download(
                repo_id="MahmoodLab/hest-tissue-seg",
                repo_type='model',
                local_dir=get_dir(),
                cache_dir=get_dir(),
                allow_patterns=[model_ckpt_name],
            )
            weights_path = os.path.join(checkpoint_dir, model_ckpt_name)

        # Initialize model
        model = smp.UnetPlusPlus(
            encoder_name=encoder_name,
            encoder_weights=encoder_weights,
            classes=2,
            activation=None,
        )

        # Load checkpoint
        state_dict = torch.load(weights_path, map_location='cpu', weights_only=True)
        model.load_state_dict(state_dict)

        # Model config
        self.input_size = 512
        self.precision = torch.float32
        self.target_mag = 1

        # Evaluation transforms
        eval_transforms = transforms.Compose([
            JpegCompressionTransform(quality=80),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        return model, eval_transforms

    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """
        Custom forward pass.
        """
        logits = self.model.predict(image)
        probs = torch.softmax(logits, dim=1)  
        max_probs, predicted_classes = torch.max(probs, dim=1)  
        predictions = (max_probs >= self.confidence_thresh) * (1 - predicted_classes)
        predictions = predictions.to(torch.uint8)
 
        return predictions


def segmentation_model_factory(
    model_name: str, 
    confidence_thresh: float = 0.5, 
    freeze: bool = True,
    **build_kwargs,
) -> SegmentationModel:
    """
    Factory function to build a segmentation model by name.
    """

    if "device" in build_kwargs:
        import warnings
        warnings.warn(
            "Passing `device` to `segmentation_model_factory` is deprecated as of version 0.1.0 "
            "Please pass `device` when segmenting the tissue, e.g., `slide.segment_tissue(..., device='cuda:0')`.",
            DeprecationWarning,
            stacklevel=2
        )

    if model_name == 'hest':
        return HESTSegmenter(freeze=freeze, confidence_thresh=confidence_thresh, **build_kwargs)
    elif model_name == 'grandqc':
        return GrandQCSegmenter(freeze=freeze, confidence_thresh=confidence_thresh, **build_kwargs)
    elif model_name == 'grandqc_artifact':
        return GrandQCArtifactSegmenter(freeze=freeze, **build_kwargs)
    else:
        raise ValueError(f"Model type {model_name} not supported")



================================================
File: trident/segmentation_models/local_ckpts.json
================================================
{
    "hest": "",
    "grandqc": "",
    "grandqc_artifact": ""
}


================================================
File: trident/slide_encoder_models/__init__.py
================================================
# in submodule
from trident.slide_encoder_models.load import (
    encoder_factory,
    MeanSlideEncoder,
    ABMILSlideEncoder,
    PRISMSlideEncoder,
    CHIEFSlideEncoder,
    GigaPathSlideEncoder,
    TitanSlideEncoder,
    ThreadsSlideEncoder,
    MadeleineSlideEncoder,
)

__all__ = [
    "encoder_factory",
    "TitanSlideEncoder",
    "ThreadsSlideEncoder",
    "MadeleineSlideEncoder",
    "MeanSlideEncoder",
    "ABMILSlideEncoder",
    "PRISMSlideEncoder",
    "CHIEFSlideEncoder",
    "GigaPathSlideEncoder",
]


================================================
File: trident/slide_encoder_models/load.py
================================================
import sys
import os
import torch
import traceback
from abc import abstractmethod
from einops import rearrange
from typing import Optional, Tuple

from trident.IO import get_weights_path

"""
This file contains an assortment of pretrained slide encoders, all loadable via the encoder_factory() function.
"""

def encoder_factory(model_name: str, pretrained: bool = True, freeze: bool = True, **kwargs) -> torch.nn.Module:
        """
        Build a slide encoder model.

        Args:
            model_name (str): Name of the model to build.
            pretrained (bool): Whether to load pretrained weights.
            freeze (bool): Whether to freeze the weights of the model.
            **kwargs: Additional arguments to pass to the model constructor.

        Returns:
            torch.nn.Module: The slide encoder model.
        """

        if model_name.startswith('mean-'):
            enc = MeanSlideEncoder
            return enc(model_name = model_name)
        elif 'threads' in model_name:
            enc = ThreadsSlideEncoder
        elif 'titan' in model_name:
            enc = TitanSlideEncoder
        elif 'prism' in model_name:
            enc = PRISMSlideEncoder
        elif 'chief' in model_name:
            enc = CHIEFSlideEncoder
        elif 'gigapath' in model_name:
            enc = GigaPathSlideEncoder
        elif 'madeleine' in model_name:
            enc = MadeleineSlideEncoder
        elif 'abmil' in model_name:
            enc = ABMILSlideEncoder
        else:
            raise ValueError(f"Model type {model_name} not supported")
        
        return enc(pretrained=pretrained, freeze=freeze, **kwargs)


# Map from slide encoder to required patch encoder
# Used in Processor.py to load the correct patch encoder for a given slide encoder
slide_to_patch_encoder_name = {
    'threads': 'conch_v15',
    'titan': 'conch_v15',
    'tcga': 'conch_v15',
    'prism': 'virchow',
    'chief': 'ctranspath',
    'gigapath': 'gigapath',
    'madeleine': 'conch_v1',
}



class BaseSlideEncoder(torch.nn.Module):
    
    def __init__(self, freeze: bool = True, **build_kwargs: dict) -> None:
        """
        Parent class for all pretrained slide encoders.
        """
        super().__init__()
        self.enc_name = None
        self.model, self.precision, self.embedding_dim = self._build(**build_kwargs)

        # Set all parameters to be non-trainable
        if freeze and self.model is not None:
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.eval()
        
    def forward(self, batch):
        """
        Can be overwritten if model requires special forward pass.
        """
        z = self.model(batch)
        return z
        
    @abstractmethod
    def _build(self, **build_kwargs):
        """
        Initialization method, must be defined in child class.
        """
        pass


class CustomSlideEncoder(BaseSlideEncoder):
    def __init__(
        self, 
        enc_name: str, 
        model: torch.nn.Module, 
        precision: torch.dtype = torch.float32, 
        embedding_dim: Optional[int] = None
    ):
        """
        CustomSlideEncoder initialization.

        This class is used when the model and precision are pre-instantiated externally 
        and should be injected directly into the encoder wrapper.

        Args:
            enc_name (str): 
                A unique name or identifier for the encoder.
            model (torch.nn.Module): 
                A PyTorch model instance to use for slide-level inference.
            precision (torch.dtype, optional): 
                The precision to use for inference (e.g., torch.float32, torch.float16).
            embedding_dim (int, optional): 
                The output embedding dimension. If not provided, will attempt to use 
                `model.embedding_dim` if it exists.
        """
        super().__init__(freeze=False)  # Freezing should be handled externally
        self.enc_name = enc_name
        self.model = model
        self.precision = precision
        self.embedding_dim = embedding_dim or getattr(model, 'embedding_dim', None)

    def _build(self, **build_kwargs):
        return None, None, None


class ABMILSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        ABMIL initialization.
        """
        super().__init__(**build_kwargs)
    
    def _build(
        self,
        input_feature_dim: int,
        n_heads: int,
        head_dim: int,
        dropout: float,
        gated: bool,
        pretrained: bool = False
    ) -> Tuple[torch.nn.ModuleDict, torch.dtype, int]:
        
        from trident.slide_encoder_models.model_zoo.reusable_blocks.ABMIL import ABMIL
        import torch.nn as nn

        self.enc_name = 'abmil'
        
        assert pretrained is False, "ABMILSlideEncoder has no corresponding pretrained models. Please load with pretrained=False."
                                
        pre_attention_layers = nn.Sequential(
            nn.Linear(input_feature_dim, input_feature_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        image_pooler = ABMIL(
            n_heads=n_heads,
            feature_dim=input_feature_dim,
            head_dim=head_dim,
            dropout=dropout,
            n_branches=1,
            gated=gated
        )
        
        post_attention_layers = nn.Sequential(
            nn.Linear(input_feature_dim, input_feature_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        model = nn.ModuleDict({
            'pre_attention_layers': pre_attention_layers,
            'image_pooler': image_pooler,
            'post_attention_layers': post_attention_layers
        })
        
        precision = torch.float32
        embedding_dim = input_feature_dim
        return model, precision, embedding_dim

    def forward(self, batch, device='cuda', return_raw_attention=False):
        image_features = self.model['pre_attention_layers'](batch['features'].to(device))
        image_features, attn = self.model['image_pooler'](image_features) # Features shape: (b n_branches f), where n_branches = 1. Branching is not used in this implementation.
        image_features = rearrange(image_features, 'b 1 f -> b f')
        image_features = self.model['post_attention_layers'](image_features)# Attention scores shape: (b r h n), where h is number of attention heads 
        if return_raw_attention:
            return image_features, attn
        return image_features


class PRISMSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        PRISM initialization.
        """
        super().__init__(**build_kwargs)
    
    def _build(self, pretrained=True):
        
        self.enc_name = 'prism'

        if sys.version_info < (3, 10):
            raise RuntimeError("PRISM requires Python 3.10 or above. Please update your Python interpreter.")

        try:
            import environs  # weird dependencies required by PRISM
            import sacremoses
            from transformers import AutoModel, AutoConfig
        except:
            traceback.print_exc()
            raise Exception(
                "Please run `pip install environs==11.0.0 transformers==4.42.4 sacremoses==0.1.1` "
                "and ensure Python version is 3.10 or above."
            )

        if pretrained:
            model = AutoModel.from_pretrained('paige-ai/Prism', trust_remote_code=True)
        else:
            model = AutoModel.from_config(AutoConfig.from_pretrained('paige-ai/Prism'))
        model.text_decoder = None
        precision = torch.float16
        embedding_dim = 1280
        return model, precision, embedding_dim
    
    def forward(self, batch, device='cuda'):
        # input should be of shape (batch_size, tile_seq_len, tile_embed_dim)
        x = batch['features'].to(device)
        z = self.model.slide_representations(x)
        z = z['image_embedding'] 
        return z
    

class CHIEFSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        CHIEF initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):
        
        self.enc_name = 'chief'
        weights_path = get_weights_path('slide', self.enc_name)

        # Ensure model can be built.
        try:
            sys.path.append(weights_path)
            from models.CHIEF import CHIEF
        except Exception:
            traceback.print_exc()
            raise Exception(
                f"\nError: Unable to import the CHIEF repository from '{weights_path}'.\n\n"
                "To resolve this issue:\n"
                "1. Ensure you have cloned the CHIEF repository to a convenient location:\n"
                "   `git clone https://github.com/hms-dbmi/CHIEF/`\n"
                "2. Set the path to CHIEF repo in `trident/slide_encoder_models/load_ckpts.json`, e.g., `./CHIEF`.\n"
                "3. Verify that CHIEF dependencies are installed:\n"
                "   `pip install addict`\n\n"
            )

        # Ensure weights can be loaded.
        try:
            current_wd = os.getcwd()  # Get current working directory
            os.chdir(weights_path)  # Change to CHIEF repo directory
            os.makedirs(os.path.join(weights_path, "model_weight"), exist_ok=True)

            required_files = {
                "Text_emdding.pth": "https://drive.google.com/drive/folders/1uRv9A1HuTW5m_pJoyMzdN31bE1i-tDaV",
                "CHIEF_pretraining.pth": "https://drive.google.com/drive/folders/1uRv9A1HuTW5m_pJoyMzdN31bE1i-tDaV",
            }

            for file_name, download_link in required_files.items():
                file_path = os.path.join(weights_path, "model_weight", file_name)
                if not os.path.exists(file_path):
                    raise Exception(
                        f"\nError: Missing required file '{file_name}'.\n\n"
                        "To resolve this issue:\n"
                        f"1. Download the file from:\n   {download_link}\n"
                        f"2. Copy '{file_name}' to the following directory:\n   {file_path}\n\n"
                        "Ensure the file is correctly placed before retrying."
                    )

            print("All necessary files are present. CHIEF setup is complete!")

        except Exception as e:
            print("\nAn error occurred during CHIEF setup:")
            traceback.print_exc()
            raise e

        model = CHIEF(size_arg="small", dropout=True, n_classes=2)

        # Load pretrained weights
        if pretrained:
            td = torch.load(os.path.join('model_weight', 'CHIEF_pretraining.pth'), map_location='cpu', weights_only=True)
            model.load_state_dict(td, strict=True)
            
        # Return to original working directory
        os.chdir(current_wd)
        
        precision = torch.float32
        embedding_dim = 768
        return model, precision, embedding_dim
    
    def forward(self, batch, device='cuda'):
        x = batch['features'].squeeze(0).to(device)
        z = self.model(x, torch.tensor([0]))
        z = z['WSI_feature']  # Shape (1,768)
        return z
    

class GigaPathSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        GigaPath initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):

        self.enc_name = 'gigapath'

        try:
            from gigapath.slide_encoder import create_model
        except:
            traceback.print_exc()
            raise Exception("Please install fairscale and gigapath using `pip install fairscale git+https://github.com/prov-gigapath/prov-gigapath.git`.")
        
        # Make sure flash_attn is correct version
        try:
            import flash_attn; assert flash_attn.__version__ == '2.5.8'
        except:
            traceback.print_exc()
            raise Exception("Please install flash_attn version 2.5.8 using `pip install flash_attn==2.5.8`.")
        
        if pretrained:
            model = create_model("hf_hub:prov-gigapath/prov-gigapath", "gigapath_slide_enc12l768d", 1536, global_pool=True)
        else:
            model = create_model("", "gigapath_slide_enc12l768d", 1536, global_pool=True)
        
        
        precision = torch.float16
        embedding_dim = 768
        return model, precision, embedding_dim

    def forward(self, batch, device='cuda'):
        self.model.tile_size = batch['attributes']['patch_size_level0']
        z = self.model(batch['features'].to(device), batch['coords'].to(device), all_layer_embed=True)[11]
        return z


class MadeleineSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        Madeleine initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):

        assert pretrained, "MadeleineSlideEncoder has no non-pretrained models. Please load with pretrained=True."

        self.enc_name = 'madeleine'
        weights_path = get_weights_path('slide', self.enc_name)
        embedding_dim = 512

        try:
            from madeleine.models.factory import create_model_from_pretrained
        except:
            traceback.print_exc()
            raise Exception("Please install Madeleine using `pip install git+https://github.com/mahmoodlab/MADELEINE.git`")  
        
        model, precision = create_model_from_pretrained(weights_path)

        return model, precision, embedding_dim
    
    def forward(self, x, device='cuda'):
        z = self.model.encode_he(x['features'], device)
        return z


class ThreadsSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        Threads initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):

        self.enc_name = 'threads'

        try:
            from threadsmodel.inference import create_model, create_model_from_pretrained
        except:
            traceback.print_exc()
            raise Exception("Coming Soon! Thanks for your patience.")
        
        return None, None, None

    def forward(self, batch, device='cuda', return_raw_attention=False):
        pass


class TitanSlideEncoder(BaseSlideEncoder):
    
    def __init__(self, **build_kwargs):
        """
        Titan initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):
        self.enc_name = 'titan'
        assert pretrained, "TitanSlideEncoder has no non-pretrained models. Please load with pretrained=True."
        from transformers import AutoModel 
        model = AutoModel.from_pretrained('MahmoodLab/TITAN', trust_remote_code=True)
        precision = torch.float16
        embedding_dim = 768
        return model, precision, embedding_dim

    def forward(self, batch, device='cuda'):
        z = self.model.encode_slide_from_patch_features(batch['features'].to(device), batch['coords'].to(device), batch['attributes']['patch_size_level0'])        
        return z


class MeanSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        Mean pooling initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, model_name = 'mean-default'):
        self.enc_name = model_name
        
        if model_name == 'mean-conch_v1':
            embedding_dim = 768
        elif model_name == 'mean-conch_v15':
            embedding_dim = 768
        elif model_name == 'mean-uni_v1':
            embedding_dim = 1024
        elif model_name == 'mean-uni_v2':
            embedding_dim = 1536
        elif model_name == 'mean-ctranspath':
            embedding_dim = 768
        elif model_name == 'mean-phikon':
            embedding_dim = 768
        elif model_name == 'mean-resnet50':
            embedding_dim = 1024
        elif model_name == 'mean-gigapath':
            embedding_dim = 1536
        elif model_name == 'mean-virchow':
            embedding_dim = 2560
        elif model_name == 'mean-virchow2':
            embedding_dim = 2560
        elif model_name == 'mean-hoptimus0':
            embedding_dim = 1536
        elif model_name == 'mean-phikon_v2':
            embedding_dim = 1024
        elif model_name == 'mean-musk':
            embedding_dim = 1024
        elif model_name == 'mean-hibou_l':
            embedding_dim = 1024
        elif model_name == 'mean-kaiko-vit8s':
            embedding_dim = 384
        elif model_name == 'mean-kaiko-vit16s':
            embedding_dim = 384
        elif model_name == 'mean-kaiko-vit8b':
            embedding_dim = 768
        elif model_name == 'mean-kaiko-vit16b':
            embedding_dim = 768
        elif model_name == 'mean-kaiko-vit14l':
            embedding_dim = 1024
        elif model_name == 'lunit-vits8':
            embedding_dim = 384
        else:
            print(f"\033[93mWARNING: Could not automatically infer embedding_dim for mean encoder {self.enc_name}. Setting to None.\033[0m")
            embedding_dim = None
            
        return None, None, embedding_dim

    def forward(self, batch, device='cuda'):
        z = batch['features'].to(device).mean(dim=1) # Just mean pooling
        return z



================================================
File: trident/slide_encoder_models/local_ckpts.json
================================================
{
    "chief": "./CHIEF",
    "madeleine": "./MADELEINE"
}


================================================
File: trident/slide_encoder_models/model_zoo/__init__.py
================================================




================================================
File: trident/slide_encoder_models/model_zoo/reusable_blocks/ABMIL.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from einops import rearrange


class ABMIL(nn.Module):
    """
    Multi-headed attention network with optional gating. Uses tanh-attention and sigmoid-gating as in ABMIL (https://arxiv.org/abs/1802.04712).
    Note that this is different from canonical attention in that the attention scores are computed directly by a linear layer rather than by a dot product between queries and keys.

    Args:
        feature_dim (int): Input feature dimension
        head_dim (int): Hidden layer dimension for each attention head. Defaults to 256.
        n_heads (int): Number of attention heads. Defaults to 8.
        dropout (float): Dropout probability. Defaults to 0.
        n_branches (int): Number of attention branches. Defaults to 1, but can be set to n_classes to generate one set of attention scores for each class.
        gated (bool): If True, sigmoid gating is applied. Otherwise, the simple attention mechanism is used.
    """

    def __init__(self, feature_dim = 1024, head_dim = 256, n_heads = 8, dropout = 0., n_branches = 1, gated = False):
        super().__init__()
        self.gated = gated
        self.n_heads = n_heads

        # Initialize attention head(s)
        self.attention_heads = nn.ModuleList([nn.Sequential(nn.Linear(feature_dim, head_dim),
                                                               nn.Tanh(),
                                                               nn.Dropout(dropout)) for _ in range(n_heads)])
        
        # Initialize gating layers if gating is used
        if self.gated:
            self.gating_layers = nn.ModuleList([nn.Sequential(nn.Linear(feature_dim, head_dim),
                                                                   nn.Sigmoid(),
                                                                   nn.Dropout(dropout)) for _ in range(n_heads)])
        
        # Initialize branching layers
        self.branching_layers = nn.ModuleList([nn.Linear(head_dim, n_branches) for _ in range(n_heads)])

        # Initialize condensing layer if multiple heads are used
        if n_heads > 1:
            self.condensing_layer = nn.Linear(n_heads * feature_dim, feature_dim)
        
    def forward(self, features, attn_mask = None):
        """
        Forward pass

        Args:
            features (torch.Tensor): Input features, acting as queries and values. Shape: batch_size x num_images x feature_dim
            attn_mask (torch.Tensor): Attention mask to enforce zero attention on empty images. Defaults to None. Shape: batch_size x num_images

        Returns:
            aggregated_features (torch.Tensor): Attention-weighted features aggregated across heads. Shape: batch_size x n_branches x feature_dim
        """

        assert features.dim() == 3, f'Input features must be 3-dimensional (batch_size x num_images x feature_dim). Got {features.shape} instead.'
        if attn_mask is not None:
            assert attn_mask.dim() == 2, f'Attention mask must be 2-dimensional (batch_size x num_images). Got {attn_mask.shape} instead.'
            assert features.shape[:2] == attn_mask.shape, f'Batch size and number of images must match between features and mask. Got {features.shape[:2]} and {attn_mask.shape} instead.'

        # Get attention scores for each head
        head_attentions = []
        head_features = []
        for i in range(len(self.attention_heads)):
            attention_vectors = self.attention_heads[i](features)        # Main attention vectors (shape: batch_size x num_images x head_dim)
            
            if self.gated:
                gating_vectors = self.gating_layers[i](features)                # Gating vectors (shape: batch_size x num_images x head_dim)
                attention_vectors = attention_vectors.mul(gating_vectors)       # Element-wise multiplication to apply gating vectors
                
            attention_scores = self.branching_layers[i](attention_vectors)       # Attention scores for each branch (shape: batch_size x num_images x n_branches)

            # Set attention scores for empty images to -inf
            if attn_mask is not None:
                attention_scores = attention_scores.masked_fill(~attn_mask.unsqueeze(-1), -1e9) # Mask is automatically broadcasted to shape: batch_size x num_images x n_branches

            # Softmax attention scores over num_images
            attention_scores_softmax = F.softmax(attention_scores, dim=1) # Shape: batch_size x num_images x n_branches

            # Multiply features by attention scores
            weighted_features = torch.einsum('bnr,bnf->brf', attention_scores_softmax, features) # Shape: batch_size x n_branches x feature_dim

            head_attentions.append(attention_scores)
            head_features.append(weighted_features)

        # Concatenate multi-head outputs and condense
        aggregated_features = torch.cat(head_features, dim=-1) # Shape: batch_size x n_branches x (n_heads * feature_dim)
        if self.n_heads > 1:
            aggregated_features = self.condensing_layer(aggregated_features) # Shape: batch_size x n_branches x feature_dim
        
        # Stack attention scores
        head_attentions = torch.stack(head_attentions, dim=-1) # Shape: batch_size x num_images x n_branches x n_heads
        head_attentions = rearrange(head_attentions, 'b n r h -> b r h n') # Shape: batch_size x n_branches x n_heads x num_images

        return aggregated_features, head_attentions


================================================
File: trident/slide_encoder_models/model_zoo/reusable_blocks/__init__.py
================================================




================================================
File: trident/wsi_objects/CuCIMWSI.py
================================================
from __future__ import annotations
import numpy as np
from PIL import Image
from typing import Tuple, Optional, Union

from trident.wsi_objects.WSI import WSI, ReadMode


class CuCIMWSI(WSI):

    def __init__(self, **kwargs) -> None:
        self.img = None
        super().__init__(**kwargs)

    def _lazy_initialize(self) -> None:
        """
        Lazily load the whole-slide image (WSI) and its metadata using CuCIM.

        This method performs deferred initialization by reading the WSI file
        only when needed. It also retrieves key metadata such as dimensions,
        magnification, and microns-per-pixel (MPP). If a tissue segmentation
        mask is available, it is also loaded.

        Raises
        ------
        ImportError
            If `cupy` and/or `cucim` are not installed.
        FileNotFoundError
            If the WSI file or required segmentation mask is missing.
        Exception
            For any other errors that occur while initializing the WSI.

        Notes
        -----
        After initialization, the following attributes are set:
        - `width` and `height`: spatial dimensions of the WSI.
        - `mpp`: microns per pixel, inferred if not already set.
        - `mag`: estimated magnification level of the image.
        - `level_count`, `level_downsamples`, and `level_dimensions`: multiresolution pyramid metadata.
        - `properties`: raw metadata from the image.
        - `gdf_contours`: tissue mask contours, if applicable.
        """

        super()._lazy_initialize()

        try:
            from cucim import CuImage
            import cupy as cp
        except ImportError as e:
            raise ImportError(
                "Required dependencies not found: `cupy` and/or `cucim`.\n"
                "Please install them with:\n"
                "  pip install cucim cupy-cuda12x\n"
                "Make sure `cupy-cuda12x` matches your local CUDA version.\n"
                "Links:\n"
                "  cucim: https://docs.rapids.ai/install/\n"
                "  cupy: https://docs.cupy.dev/en/stable/install.html"
            ) from e

        if not self.lazy_init:
            try:
                self.img = CuImage(self.slide_path)
                self.dimensions = (self.img.size()[1], self.img.size()[0])  # width, height are reverted compared to openslide!!
                self.width, self.height = self.dimensions
                self.level_count = self.img.resolutions['level_count']
                self.level_downsamples = self.img.resolutions['level_downsamples']
                self.level_dimensions = self.img.resolutions['level_dimensions']
                self.properties = self.img.metadata
                if self.mpp is None:
                    self.mpp = self._fetch_mpp(self.custom_mpp_keys)
                self.mag = self._fetch_magnification(self.custom_mpp_keys)
                self.lazy_init = True

            except Exception as e:
                raise RuntimeError(f"Failed to initialize WSI using CuCIM: {e}") from e

    def _fetch_mpp(self, custom_keys: dict = None) -> float:
        """
        Fetch the microns per pixel (MPP) from CuImage metadata.

        Parameters
        ----------
        custom_keys : dict, optional
            Optional dictionary with keys for 'mpp_x' and 'mpp_y' metadata fields to check first.

        Returns
        -------
        float
            MPP value in microns per pixel.

        Raises
        ------
        ValueError
            If MPP cannot be determined from metadata.
        """
        import json

        def try_parse(val):
            try:
                return float(val)
            except:
                return None

        # CuCIM metadata can be a JSON string
        metadata = self.img.metadata
        if isinstance(metadata, str):
            metadata = json.loads(metadata)

        # Flatten nested CuCIM metadata for convenience
        flat_meta = {}
        def flatten(d, parent_key=''):
            for k, v in d.items():
                key = f"{parent_key}.{k}" if parent_key else k
                if isinstance(v, dict):
                    flatten(v, key)
                else:
                    flat_meta[key.lower()] = v
        flatten(metadata)

        # Check custom keys first if provided
        mpp_x = mpp_y = None
        if custom_keys:
            if 'mpp_x' in custom_keys:
                mpp_x = try_parse(flat_meta.get(custom_keys['mpp_x'].lower()))
            if 'mpp_y' in custom_keys:
                mpp_y = try_parse(flat_meta.get(custom_keys['mpp_y'].lower()))

        # Standard fallback keys used in SVS, NDPI, MRXS, etc.
        fallback_keys = [
            'openslide.mpp-x', 'openslide.mpp-y',
            'tiff.resolution-x', 'tiff.resolution-y',
            'mpp', 'spacing', 'microns_per_pixel',
            'aperio.mpp', 'hamamatsu.mpp',
            'metadata.resolutions.level[0].spacing',
            'metadata.resolutions.level[0].physical_size.0',
        ]

        for key in fallback_keys:
            if mpp_x is None and key in flat_meta:
                mpp_x = try_parse(flat_meta[key])
            elif mpp_y is None and key in flat_meta:
                mpp_y = try_parse(flat_meta[key])
            if mpp_x is not None and mpp_y is not None:
                break

        # Use same value for both axes if only one was found
        if mpp_x is not None and mpp_y is None:
            mpp_y = mpp_x
        if mpp_y is not None and mpp_x is None:
            mpp_x = mpp_y

        if mpp_x is not None and mpp_y is not None:
            return float((mpp_x + mpp_y) / 2)
        
        raise ValueError(
            f"Unable to extract MPP from CuCIM metadata for: '{self.slide_path}'.\n"
            "Suggestions:\n"
            "- Provide `custom_keys` with metadata key mappings for 'mpp_x' and 'mpp_y'.\n"
            "- Set the MPP manually when constructing the CuCIMWSI object."
        )

    def get_thumbnail(self, size: tuple[int, int]) -> Image.Image:
        """
        Generate a thumbnail image of the WSI.

        Args:
        -----
        size : tuple[int, int]
            A tuple specifying the desired width and height of the thumbnail.

        Returns:
        --------
        Image.Image:
            The thumbnail as a PIL Image in RGB format.
        """
        target_width, target_height = size

        # Compute desired downsample factor and level
        downsample_x = self.width / target_width
        downsample_y = self.height / target_height
        desired_downsample = max(downsample_x, downsample_y)
        level, _ = self.get_best_level_and_custom_downsample(desired_downsample)

        # Compute the size to read at that level
        level_width, level_height = self.level_dimensions[level]

        # Read region at (0, 0) in target level
        region = self.read_region(
            location=(0, 0),
            size=(level_width, level_height),
            level=level
        ).convert("RGB")
        # region = region.resize((size[1], size[0]), resample=Image.BILINEAR)
        region = region.resize(size, resample=Image.BILINEAR)

        return region

    def read_region(
        self,
        location: Tuple[int, int],
        level: int,
        size: Tuple[int, int],
        read_as: ReadMode = 'pil',
    ) -> Union[Image.Image, np.ndarray]:
        """
        Extract a specific region from the whole-slide image (WSI) using CuCIM.

        Parameters
        ----------
        location : Tuple[int, int]
            (x, y) coordinates of the top-left corner of the region to extract.
        level : int
            Pyramid level to read from.
        size : Tuple[int, int]
            (width, height) of the region to extract.
        read_as : {'pil', 'numpy'}, optional
            Output format for the region:
            - 'pil': returns a PIL Image (default)
            - 'numpy': returns a NumPy array (H, W, 3)

        Returns
        -------
        Union[PIL.Image.Image, np.ndarray]
            The extracted region in the specified format.

        Raises
        ------
        ValueError
            If `read_as` is not one of the supported options.

        Example
        -------
        >>> region = wsi.read_region((1000, 1000), level=0, size=(512, 512), read_as='pil')
        >>> region.show()
        """

        import cupy as cp

        region = self.img.read_region(location=location, level=level, size=size, device='cpu')
        region = cp.asnumpy(region)  # Convert from CuPy to NumPy

        if read_as == 'numpy':
            return region
        elif read_as == 'pil':
            return Image.fromarray(region).convert("RGB")
        else:
            raise ValueError(f"Invalid `read_as` value: {read_as}. Must be 'pil' or 'numpy'.")
        
    def get_dimensions(self) -> Tuple[int, int]:
        """
        Return the (width, height) dimensions of the CuCIM-managed WSI.

        Returns:
        --------
        Tuple[int, int]:
            A tuple containing the width and height of the WSI in pixels.

        Example:
        --------
        >>> wsi.get_dimensions()
        (100000, 80000)
        """
        return self.dimensions

    def segment_tissue(self, **kwargs) -> str:
        out = super().segment_tissue(**kwargs)
        self.close()
        return out
    
    def extract_tissue_coords(self, **kwargs) -> str:
        out = super().extract_tissue_coords(**kwargs)
        self.close()
        return out

    def visualize_coords(self, **kwargs) -> str:
        out = super().visualize_coords(**kwargs)
        self.close()
        return out

    def extract_patch_features(self, **kwargs) -> str:
        out = super().extract_patch_features(**kwargs)
        self.close()
        return out

    def extract_slide_features(self, **kwargs) -> str:
        out = super().extract_slide_features(**kwargs)
        self.close()
        return out

    def close(self):
        if self.img is not None:
            self.img.close()
            self.img = None
            self.lazy_init = False



================================================
File: trident/wsi_objects/ImageWSI.py
================================================
from __future__ import annotations
import numpy as np
from PIL import Image
from typing import Tuple, Union

from trident.wsi_objects.WSI import WSI, ReadMode


class ImageWSI(WSI):

    def __init__(self, **kwargs) -> None:
        """
        Initialize a WSI object from a standard image file (e.g., PNG, JPEG, etc.).

        Parameters
        ----------
        slide_path : str
            Path to the image file.
        mpp : float
            Microns per pixel. Required since standard image formats do not store this metadata.
        name : str, optional
            Optional name for the slide.
        lazy_init : bool, default=True
            Whether to defer initialization until the WSI is accessed.

        Raises
        ------
        ValueError
            If the required 'mpp' argument is not provided.

        Example
        -------
        >>> wsi = ImageWSI(slide_path="path/to/image.png", lazy_init=False, mpp=0.51)
        >>> print(wsi)
        <width=5120, height=3840, backend=ImageWSI, mpp=0.51, mag=20>
        """
        mpp = kwargs.get("mpp")
        if mpp is None:
            raise ValueError(
                "Missing required argument `mpp`. Standard image formats do not contain microns-per-pixel "
                "information, so you must specify it manually via the `ImageWSI` constructor."
            )
        
        #enable loading large images.
        from PIL import PngImagePlugin
        PngImagePlugin.MAX_TEXT_CHUNK = 2**30  # ~1GB
        PngImagePlugin.MAX_TEXT_MEMORY = 2**30
        PngImagePlugin.MAX_IMAGE_PIXELS = None  # Optional: disables large image warning

        self.img = None
        super().__init__(**kwargs)

    def _lazy_initialize(self) -> None:
        """
        Lazily initialize the WSI using a standard image file (e.g., JPEG, PNG, etc.).

        This method loads the image using PIL and extracts relevant metadata such as
        dimensions and magnification. It assumes a single-resolution image (no pyramid).
        If a tissue segmentation mask is available, it is also loaded.

        Raises
        ------
        FileNotFoundError
            If the WSI file or tissue segmentation mask is not found.
        Exception
            If an unexpected error occurs during initialization.

        Notes
        -----
        After initialization, the following attributes are set:
        - `width` and `height`: dimensions of the image.
        - `dimensions`: (width, height) tuple of the image.
        - `level_downsamples`: set to `[1]` (single resolution).
        - `level_dimensions`: set to a list containing the image dimensions.
        - `level_count`: set to `1`.
        - `mag`: estimated magnification level.
        - `gdf_contours`: loaded from `tissue_seg_path`, if available.
        """

        super()._lazy_initialize()

        if not self.lazy_init:
            try:
                self._ensure_image_open()
                self.level_downsamples = [1]
                self.dimensions = (self.img.width, self.img.height)
                self.width, self.height = self.dimensions[0], self.dimensions[1]
                self.mag = self._fetch_magnification(self.custom_mpp_keys)
                self.dimensions = self.img.size
                self.level_dimensions = [(self.img.width, self.img.height)]
                self.level_count = 1
                self.lazy_init = True

            except Exception as e:
                raise Exception(f"Error initializing WSI with PIL.Image: {e}")

    def _ensure_image_open(self):
        if self.img is None:
            self.img = Image.open(self.slide_path).convert("RGB")

    def get_dimensions(self):
        return self.dimensions

    def get_thumbnail(self, size):
        """
        Generate a thumbnail of the image.

        Parameters
        ----------
        size : tuple of int
            Desired thumbnail size (width, height).

        Returns
        -------
        PIL.Image.Image
            RGB thumbnail image.
        """
        self._ensure_image_open()
        img = self.img.copy()
        img.thumbnail(size)
        return img

    def read_region(
        self,
        location: Tuple[int, int],
        level: int,
        size: Tuple[int, int],
        read_as: ReadMode = 'pil',
    ) -> Union[Image.Image, np.ndarray]:
        """
        Extract a specific region from a single-resolution image (e.g., JPEG, PNG, TIFF).

        Parameters
        ----------
        location : Tuple[int, int]
            (x, y) coordinates of the top-left corner of the region to extract.
        level : int
            Pyramid level to read from. Only level 0 is supported for non-pyramidal images.
        size : Tuple[int, int]
            (width, height) of the region to extract.
        read_as : {'pil', 'numpy'}, optional
            Output format for the region:
            - 'pil': returns a PIL Image (default)
            - 'numpy': returns a NumPy array (H, W, 3)

        Returns
        -------
        Union[PIL.Image.Image, np.ndarray]
            Extracted image region in the specified format.

        Raises
        ------
        ValueError
            If `level` is not 0 or if `read_as` is not one of the supported options.

        Example
        -------
        >>> region = wsi.read_region((0, 0), level=0, size=(512, 512), read_as='numpy')
        >>> print(region.shape)
        (512, 512, 3)
        """
        if level != 0:
            raise ValueError("ImageWSI only supports reading at level=0 (no pyramid levels).")

        self._ensure_image_open()
        region = self.img.crop((
            location[0],
            location[1],
            location[0] + size[0],
            location[1] + size[1]
        )).convert('RGB')

        if read_as == 'pil':
            return region
        elif read_as == 'numpy':
            return np.array(region)
        else:
            raise ValueError(f"Invalid `read_as` value: {read_as}. Must be 'pil' or 'numpy'.")

    def segment_tissue(self, **kwargs):
        out = super().segment_tissue(**kwargs)
        self.close()
        return out
    
    def extract_tissue_coords(self, **kwargs):
        out = super().extract_tissue_coords(**kwargs)
        self.close()
        return out

    def visualize_coords(self, **kwargs):
        out = super().visualize_coords(**kwargs)
        self.close()
        return out

    def extract_patch_features(self, **kwargs):
        out = super().extract_patch_features(**kwargs)
        self.close()
        return out

    def extract_slide_features(self, **kwargs):
        out = super().extract_slide_features(**kwargs)
        self.close()
        return out

    def close(self):
        """
        Close the internal image object to free memory. These can take several GB in RAM.
        """
        if self.img is not None:
            self.img.close()
            self.img = None



================================================
File: trident/wsi_objects/OpenSlideWSI.py
================================================
from __future__ import annotations
import numpy as np
import openslide
from PIL import Image
from typing import List, Tuple, Union, Optional

from trident.wsi_objects.WSI import WSI, ReadMode


class OpenSlideWSI(WSI):

    def __init__(self, **kwargs) -> None:
        """
        Initialize an OpenSlideWSI instance.

        Parameters
        ----------
        **kwargs : dict
            Keyword arguments forwarded to the base `WSI` class. Most important key is:
            - slide_path (str): Path to the WSI.
            - lazy_init (bool, default=True): Whether to defer loading WSI and metadata.

        Please refer to WSI constructor for all parameters. 

        Example
        -------
        >>> wsi = OpenSlideWSI(slide_path="path/to/wsi.svs", lazy_init=False)
        >>> print(wsi)
        <width=100000, height=80000, backend=OpenSlideWSI, mpp=0.25, mag=40>
        """
        super().__init__(**kwargs)

    def _lazy_initialize(self) -> None:
        """
        Lazily initialize the WSI using OpenSlide.

        This method opens a whole-slide image using the OpenSlide backend, extracting
        key metadata including dimensions, magnification, and multiresolution pyramid
        information. If a tissue segmentation mask is provided, it is also loaded.

        Raises
        ------
        FileNotFoundError
            If the WSI file or the tissue segmentation mask cannot be found.
        Exception
            If an unexpected error occurs during WSI initialization.

        Notes
        -----
        After initialization, the following attributes are set:
        - `width` and `height`: spatial dimensions of the base level.
        - `dimensions`: (width, height) tuple from the highest resolution.
        - `level_count`: number of resolution levels in the image pyramid.
        - `level_downsamples`: downsampling factors for each level.
        - `level_dimensions`: image dimensions at each level.
        - `properties`: metadata dictionary from OpenSlide.
        - `mpp`: microns per pixel, inferred if not manually specified.
        - `mag`: estimated magnification level.
        - `gdf_contours`: loaded from `tissue_seg_path` if provided.
        """

        super()._lazy_initialize()

        if not self.lazy_init:
            try:
                self.img = openslide.OpenSlide(self.slide_path)
                # set openslide attrs as self
                self.dimensions = self.get_dimensions()
                self.width, self.height = self.dimensions
                self.level_count = self.img.level_count
                self.level_downsamples = self.img.level_downsamples
                self.level_dimensions = self.img.level_dimensions
                self.properties = self.img.properties
                if self.mpp is None:
                    self.mpp = self._fetch_mpp(self.custom_mpp_keys)
                self.mag = self._fetch_magnification(self.custom_mpp_keys)
                self.lazy_init = True

            except Exception as e:
                raise RuntimeError(f"Failed to initialize WSI with OpenSlide: {e}") from e

    def _fetch_mpp(self, custom_mpp_keys: Optional[List[str]] = None) -> float:
        """
        Retrieve microns per pixel (MPP) from OpenSlide metadata.

        Parameters
        ----------
        custom_mpp_keys : list of str, optional
            Additional metadata keys to check for MPP.

        Returns
        -------
        float
            MPP value in microns per pixel.

        Raises
        ------
        ValueError
            If MPP cannot be determined from metadata.
        """
        mpp_keys = [
            openslide.PROPERTY_NAME_MPP_X,
            'openslide.mirax.MPP',
            'aperio.MPP',
            'hamamatsu.XResolution',
            'openslide.comment',
        ]

        if custom_mpp_keys:
            mpp_keys.extend(custom_mpp_keys)

        for key in mpp_keys:
            if key in self.img.properties:
                try:
                    mpp_x = float(self.img.properties[key])
                    return round(mpp_x, 4)
                except ValueError:
                    continue

        x_resolution = self.img.properties.get('tiff.XResolution')
        unit = self.img.properties.get('tiff.ResolutionUnit')

        if x_resolution and unit:
            try:
                if unit.lower() == 'centimeter':
                    return round(10000 / float(x_resolution), 4)
                elif unit.upper() == 'INCH':
                    return round(25400 / float(x_resolution), 4)
            except ValueError:
                pass

        raise ValueError(
            f"Unable to extract MPP from slide metadata: '{self.slide_path}'.\n"
            "Suggestions:\n"
            "- Provide `custom_mpp_keys` to specify metadata keys to look for.\n"
            "- Set the MPP explicitly via the class constructor.\n"
            "- If using the `run_batch_of_slides.py` script, pass the MPP via the "
            "`--custom_list_of_wsis` argument in a CSV file. Refer to TRIDENT/README/Q&A."
        )

    def _fetch_magnification(self, custom_mpp_keys: Optional[List[str]] = None) -> int:

        """
        Retrieve estimated magnification from metadata.

        Parameters
        ----------
        custom_mpp_keys : list of str, optional
            Keys to aid in computing magnification from MPP.

        Returns
        -------
        int
            Estimated magnification.

        Raises
        ------
        ValueError
            If magnification cannot be determined.
        """
        mag = super()._fetch_magnification(custom_mpp_keys)
        if mag is not None:
            return mag

        metadata_mag = self.img.properties.get(openslide.PROPERTY_NAME_OBJECTIVE_POWER)
        if metadata_mag is not None:
            try:
                return int(metadata_mag)
            except ValueError:
                pass

        raise ValueError(f"Unable to determine magnification from metadata for: {self.slide_path}")

    def read_region(
        self,
        location: Tuple[int, int],
        level: int,
        size: Tuple[int, int],
        read_as: ReadMode = 'pil',
    ) -> Union[Image.Image, np.ndarray]:
        """
        Extract a specific region from the whole-slide image (WSI).

        Parameters
        ----------
        location : Tuple[int, int]
            (x, y) coordinates of the top-left corner of the region to extract.
        level : int
            Pyramid level to read from.
        size : Tuple[int, int]
            (width, height) of the region to extract.
        read_as : {'pil', 'numpy'}, optional
            Output format for the region:
            - 'pil': returns a PIL Image (default)
            - 'numpy': returns a NumPy array (H, W, 3)

        Returns
        -------
        Union[PIL.Image.Image, np.ndarray]
            Extracted image region in the specified format.

        Raises
        ------
        ValueError
            If `read_as` is not one of 'pil' or 'numpy'.

        Example
        -------
        >>> region = wsi.read_region((0, 0), level=0, size=(512, 512), read_as='numpy')
        >>> print(region.shape)
        (512, 512, 3)
        """
        region = self.img.read_region(location, level, size).convert('RGB')

        if read_as == 'pil':
            return region
        elif read_as == 'numpy':
            return np.array(region)
        else:
            raise ValueError(f"Invalid `read_as` value: {read_as}. Must be 'pil', 'numpy'.")

    def get_dimensions(self) -> Tuple[int, int]:
        """
        Return the dimensions (width, height) of the WSI.

        Returns
        -------
        tuple of int
            (width, height) in pixels.
        """
        return self.img.dimensions

    def get_thumbnail(self, size: tuple[int, int]) -> Image.Image:
        """
        Generate a thumbnail of the WSI.

        Parameters
        ----------
        size : tuple of int
            Desired (width, height) of the thumbnail.

        Returns
        -------
        PIL.Image.Image
            RGB thumbnail as a PIL Image.
        """
        return self.img.get_thumbnail(size).convert('RGB')



================================================
File: trident/wsi_objects/WSI.py
================================================
from __future__ import annotations
import numpy as np
from PIL import Image
import os 
import warnings
import torch 
from typing import List, Tuple, Optional, Literal
from torch.utils.data import DataLoader
from tqdm import tqdm

from trident.wsi_objects.WSIPatcher import *
from trident.wsi_objects.WSIPatcherDataset import WSIPatcherDataset
from trident.IO import (
    save_h5, read_coords, read_coords_legacy,
    mask_to_gdf, overlay_gdf_on_thumbnail, get_num_workers
)

ReadMode = Literal['pil', 'numpy']


class WSI:
    """
    The `WSI` class provides an interface to work with Whole Slide Images (WSIs). 
    It supports lazy initialization, metadata extraction, tissue segmentation,
    patching, and feature extraction. The class handles various WSI file formats and 
    offers utilities for integration with AI models.

    Attributes
    ----------
    slide_path : str
        Path to the WSI file.
    name : str
        Name of the WSI (inferred from the file path if not provided).
    custom_mpp_keys : dict
        Custom keys for extracting microns per pixel (MPP) and magnification metadata.
    lazy_init : bool
        Indicates whether lazy initialization is used.
    tissue_seg_path : str
        Path to a tissue segmentation mask (if available).
    width : int
        Width of the WSI in pixels (set during lazy initialization).
    height : int
        Height of the WSI in pixels (set during lazy initialization).
    dimensions : Tuple[int, int]
        (width, height) tuple of the WSI (set during lazy initialization).
    mpp : float
        Microns per pixel (set during lazy initialization or inferred).
    mag : float
        Estimated magnification level (set during lazy initialization or inferred).
    level_count : int
        Number of resolution levels in the WSI (set during lazy initialization).
    level_downsamples : List[float]
        Downsampling factors for each pyramid level (set during lazy initialization).
    level_dimensions : List[Tuple[int, int]]
        Dimensions of the WSI at each pyramid level (set during lazy initialization).
    properties : dict
        Metadata properties extracted from the image backend (set during lazy initialization).
    img : Any
        Backend-specific image object used for reading regions (set during lazy initialization).
    gdf_contours : geopandas.GeoDataFrame
        Tissue segmentation mask as a GeoDataFrame, if available (set during lazy initialization).
    """

    def __init__(
        self,
        slide_path: str,
        name: Optional[str] = None,
        tissue_seg_path: Optional[str] = None,
        custom_mpp_keys: Optional[List[str]] = None,
        lazy_init: bool = True,
        mpp: Optional[float] = None,
        max_workers: Optional[int] = None,
    ):
        """
        Initialize the `WSI` object for working with a Whole Slide Image (WSI).

        Args:
        -----
        slide_path : str
            Path to the WSI file.
        name : str, optional
            Optional name for the WSI. Defaults to the filename (without extension).
        tissue_seg_path : str, optional
            Path to the tissue segmentation mask file. Defaults to None.
        custom_mpp_keys : Optional[List[str]]
            Custom keys for extracting MPP and magnification metadata. Defaults to None.
        lazy_init : bool, optional
            If True, defer loading the WSI until required. Defaults to True.
        mpp: float, optional
            If not None, will be the reference micron per pixel (mpp). Handy when mpp is not provided in the WSI.
        max_workers (Optional[int]): Maximum number of workers for data loading

        """
        self.slide_path = slide_path
        if name is None:
            self.name, self.ext = os.path.splitext(os.path.basename(slide_path)) 
        else:
            self.name, self.ext = os.path.splitext(name)
        self.tissue_seg_path = tissue_seg_path
        self.custom_mpp_keys = custom_mpp_keys

        self.width, self.height = None, None  # Placeholder dimensions
        self.mpp = mpp  # Placeholder microns per pixel. Defaults will be None unless specified in constructor. 
        self.mag = None  # Placeholder magnification
        self.lazy_init = lazy_init  # Initialize immediately if lazy_init is False
        self.max_workers = max_workers

        if not self.lazy_init:
            self._lazy_initialize()
        else: 
            self.lazy_init = not self.lazy_init

    def __repr__(self) -> str:
        if self.lazy_init:
            return f"<width={self.width}, height={self.height}, backend={self.__class__.__name__}, mpp={self.mpp}, mag={self.mag}>"
        else:
            return f"<name={self.name}>"
    
    def _lazy_initialize(self) -> None:
        """
        Perform lazy initialization of internal attributes for the WSI interface.

        This method is intended to be called by subclasses of `WSI`, and should not be used directly.
        It sets default values for key image attributes and optionally loads a tissue segmentation mask
        if a path is provided. Subclasses must override this method to implement backend-specific behavior.

        Raises
        ------
        FileNotFoundError
            If the tissue segmentation mask file is provided but cannot be found.

        Notes
        -----
        This method sets the following attributes:
        - `img`, `dimensions`, `width`, `height`: placeholder image properties (set to None).
        - `level_count`, `level_downsamples`, `level_dimensions`: multiresolution placeholders (None).
        - `properties`, `mag`: metadata and magnification (None).
        - `gdf_contours`: loaded from `tissue_seg_path` if available.
        """

        if not self.lazy_init:
            self.img = None
            self.dimensions = None
            self.width, self.height = None, None
            self.level_count = None
            self.level_downsamples = None
            self.level_dimensions = None
            self.properties = None
            self.mag = None
            if self.tissue_seg_path is not None:
                try:
                    self.gdf_contours = gpd.read_file(self.tissue_seg_path)
                except FileNotFoundError:
                    raise FileNotFoundError(f"Tissue segmentation file not found: {self.tissue_seg_path}")

    def create_patcher(
        self, 
        patch_size: int, 
        src_pixel_size: Optional[float] = None, 
        dst_pixel_size: Optional[float] = None, 
        src_mag: Optional[int] = None, 
        dst_mag: Optional[int] = None, 
        overlap: int = 0, 
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False, 
        custom_coords:  Optional[np.ndarray] = None,
        threshold: float = 0.15,
        pil: bool = False,
    ) -> WSIPatcher:
        """
        The `create_patcher` function from the class `WSI` Create a patcher object for extracting patches from the WSI.

        Args:
        -----
        patch_size : int
            Size of each patch in pixels.
        src_pixel_size : float, optional
            Source pixel size. Defaults to None.
        dst_pixel_size : float, optional
            Destination pixel size. Defaults to None.
        ...

        Returns:
        --------
        WSIPatcher:
            An object for extracting patches.

        Example:
        --------
        >>> patcher = wsi.create_patcher(patch_size=512, src_pixel_size=0.25, dst_pixel_size=0.5)
        >>> for patch in patcher:
        ...     process(patch)
        """
        return WSIPatcher(
            self, patch_size, src_pixel_size, dst_pixel_size, src_mag, dst_mag,
            overlap, mask, coords_only, custom_coords, threshold, pil
        )
    
    def _fetch_magnification(self, custom_mpp_keys: Optional[List[str]] = None) -> int:
        """
        The `_fetch_magnification` function of the class `WSI` calculates the magnification level 
        of the WSI based on the microns per pixel (MPP) value or other metadata. The magnification levels are 
        approximated to commonly used values such as 80x, 40x, 20x, etc. If the MPP is unavailable or insufficient 
        for calculation, it attempts to fallback to metadata-based values.

        Args:
        -----
        custom_mpp_keys : Optional[List[str]], optional
            Custom keys to search for MPP values in the WSI properties. Defaults to None.

        Returns:
        --------
        Optional[int]]:
            The approximated magnification level, or None if the magnification could not be determined.

        Raises:
        -------
        ValueError:
            If the identified MPP is too low for valid magnification values.

        Example:
        --------
        >>> mag = wsi._fetch_magnification()
        >>> print(mag)
        40
        """
        if self.mpp is None:
            mpp_x = self._fetch_mpp(custom_mpp_keys)
        else:
            mpp_x = self.mpp

        if mpp_x is not None:
            if mpp_x < 0.16:
                return 80
            elif mpp_x < 0.2:
                return 60
            elif mpp_x < 0.3:
                return 40
            elif mpp_x < 0.6:
                return 20
            elif mpp_x < 1.2:
                return 10
            elif mpp_x < 2.4:
                return 5
            else:
                raise ValueError(f"Identified mpp is very low: mpp={mpp_x}. Most WSIs are at 20x, 40x magnfication.")

    @torch.inference_mode()
    @torch.autocast(device_type="cuda", dtype=torch.float16)
    def segment_tissue(
        self,
        segmentation_model: torch.nn.Module,
        target_mag: int = 10,
        holes_are_tissue: bool = True,
        job_dir: Optional[str] = None,
        batch_size: int = 16,
        device: str = 'cuda:0',
        verbose=False
    ) -> str:
        """
        The `segment_tissue` function of the class `WSI` segments tissue regions in the WSI using 
        a specified segmentation model. It processes the WSI at a target magnification level, optionally 
        treating holes in the mask as tissue. The segmented regions are saved as thumbnails and GeoJSON contours.

        Args:
        -----
        segmentation_model : torch.nn.Module
            The model used for tissue segmentation.
        target_mag : int, optional
            Target magnification level for segmentation. Defaults to 10.
        holes_are_tissue : bool, optional
            Whether to treat holes in the mask as tissue. Defaults to True.
        job_dir :  Optional[str], optional
            Directory to save the segmentation results. Defaults to None.
        batch_size : int, optional
            Batch size for processing patches. Defaults to 16.
        device (str): 
            The computation device to use (e.g., 'cuda:0' for GPU or 'cpu' for CPU).
        verbose: bool, optional:
            Whenever to print segmentation progress. Defaults to False.


        Returns:
        --------
        str:
            The absolute path to where the segmentation as GeoJSON is saved. 

        Example:
        --------
        >>> wsi.segment_tissue(segmentation_model, target_mag=10, job_dir="output_dir")
        >>> # Results saved in "output_dir"
        """

        self._lazy_initialize()
        segmentation_model.to(device)
        max_dimension = 1000
        if self.width > self.height:
            thumbnail_width = max_dimension
            thumbnail_height = int(thumbnail_width * self.height / self.width)
        else:
            thumbnail_height = max_dimension
            thumbnail_width = int(thumbnail_height * self.width / self.height)
        thumbnail = self.get_thumbnail((thumbnail_width, thumbnail_height))

        # Get patch iterator
        destination_mpp = 10 / target_mag
        patcher = self.create_patcher(
            patch_size = segmentation_model.input_size,
            src_pixel_size = self.mpp,
            dst_pixel_size = destination_mpp,
            mask=self.gdf_contours if hasattr(self, "gdf_contours") else None
        )
        precision = segmentation_model.precision
        eval_transforms = segmentation_model.eval_transforms
        dataset = WSIPatcherDataset(patcher, eval_transforms)
        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=get_num_workers(batch_size, max_workers=self.max_workers), pin_memory=True)
        # dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0, pin_memory=True)

        mpp_reduction_factor = self.mpp / destination_mpp
        width, height = self.get_dimensions()
        width, height = int(round(width * mpp_reduction_factor)), int(round(height * mpp_reduction_factor))
        predicted_mask = np.zeros((height, width), dtype=np.uint8)

        dataloader = tqdm(dataloader) if verbose else dataloader

        for imgs, (xcoords, ycoords) in dataloader:

            imgs = imgs.to(device, dtype=precision)  # Move to device and match dtype
            with torch.autocast(device_type=device.split(":")[0], dtype=precision, enabled=(precision != torch.float32)):
                preds = segmentation_model(imgs).cpu().numpy()

            x_starts = np.clip(np.round(xcoords.numpy() * mpp_reduction_factor).astype(int), 0, width - 1) # clip for starts
            y_starts = np.clip(np.round(ycoords.numpy() * mpp_reduction_factor).astype(int), 0, height - 1)
            x_ends = np.clip(x_starts + segmentation_model.input_size, 0, width)
            y_ends = np.clip(y_starts + segmentation_model.input_size, 0, height)
            
            for i in range(len(preds)):
                x_start, x_end = x_starts[i], x_ends[i]
                y_start, y_end = y_starts[i], y_ends[i]
                if x_start >= x_end or y_start >= y_end: # invalid patch
                    continue
                patch_pred = preds[i][:y_end - y_start, :x_end - x_start]
                predicted_mask[y_start:y_end, x_start:x_end] += patch_pred
        
        # Post-process the mask
        predicted_mask = (predicted_mask > 0).astype(np.uint8) * 255

        # # Fill holes if desired
        # if not holes_are_tissue:
        #     holes, _ = cv2.findContours(predicted_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)
        #     for hole in holes:
        #         cv2.drawContours(predicted_mask, [hole], 0, 255, -1)

        # Save thumbnail image
        thumbnail_saveto = os.path.join(job_dir, 'thumbnails', f'{self.name}.jpg')
        os.makedirs(os.path.dirname(thumbnail_saveto), exist_ok=True)
        thumbnail.save(thumbnail_saveto)

        # Save geopandas contours
        gdf_saveto = os.path.join(job_dir, 'contours_geojson', f'{self.name}.geojson')
        os.makedirs(os.path.dirname(gdf_saveto), exist_ok=True)
        gdf_contours = mask_to_gdf(
            mask=predicted_mask,
            max_nb_holes=0 if holes_are_tissue else 20,
            min_contour_area=1000,
            pixel_size=self.mpp,
            contour_scale=1/mpp_reduction_factor
        )
        gdf_contours.set_crs("EPSG:3857", inplace=True)  # used to silent warning // Web Mercator
        gdf_contours.to_file(gdf_saveto, driver="GeoJSON")
        self.gdf_contours = gdf_contours
        self.tissue_seg_path = gdf_saveto

        # Draw the contours on the thumbnail image
        contours_saveto = os.path.join(job_dir, 'contours', f'{self.name}.jpg')
        annotated = np.array(thumbnail)
        overlay_gdf_on_thumbnail(gdf_contours, annotated, contours_saveto, thumbnail_width / self.width)

        return gdf_saveto

    def get_best_level_and_custom_downsample(
        self,
        downsample: float,
        tolerance: float = 0.01
    ) -> Tuple[int, float]:
        """
        The `get_best_level_and_custom_downsample` function of the class `WSI` determines the best level 
        and custom downsample factor to approximate a desired downsample value. It identifies the most suitable 
        resolution level of the WSI and calculates any additional scaling required.

        Args:
        -----
        downsample : float
            The desired downsample factor.
        tolerance : float, optional
            Tolerance for rounding differences. Defaults to 0.01.

        Returns:
        --------
        Tuple[int, float]:
            The closest resolution level and the custom downsample factor.

        Raises:
        -------
        ValueError:
            If no suitable resolution level is found for the specified downsample factor.

        Example:
        --------
        >>> level, custom_downsample = wsi.get_best_level_and_custom_downsample(2.5)
        >>> print(level, custom_downsample)
        2, 1.1
        """
        level_downsamples = self.level_downsamples

        # First, check for an exact match within tolerance
        for level, level_downsample in enumerate(level_downsamples):
            if abs(level_downsample - downsample) <= tolerance:
                return level, 1.0  # Exact match, no custom downsampling needed

        if downsample >= level_downsamples[0]:
            # Downsampling: find the highest level_downsample less than or equal to the desired downsample
            closest_level = None
            closest_downsample = None
            for level, level_downsample in enumerate(level_downsamples):
                if level_downsample <= downsample:
                    closest_level = level
                    closest_downsample = level_downsample
                else:
                    break  # Since level_downsamples are sorted, no need to check further
            if closest_level is not None:
                custom_downsample = downsample / closest_downsample
                return closest_level, custom_downsample
        else:
            # Upsampling: find the smallest level_downsample greater than or equal to the desired downsample
            for level, level_downsample in enumerate(level_downsamples):
                if level_downsample >= downsample:
                    custom_downsample = level_downsample / downsample
                    return level, custom_downsample

        # If no suitable level is found, raise an error
        raise ValueError(f"No suitable level found for downsample {downsample}.")

    def extract_tissue_coords(
        self,
        target_mag: int,
        patch_size: int,
        save_coords: str,
        overlap: int = 0,
        min_tissue_proportion: float  = 0.,
    ) -> str:
        """
        The `extract_tissue_coords` function of the class `WSI` extracts patch coordinates 
        from tissue regions in the WSI. It generates coordinates of patches at the specified 
        magnification and saves the results in an HDF5 file.

        Args:
        -----
        target_mag : int
            Target magnification level for the patches.
        patch_size : int
            Size of each patch at the target magnification.
        save_coords : str
            Directory path to save the extracted coordinates.
        overlap : int, optional
            Overlap between patches in pixels. Defaults to 0.
        min_tissue_proportion: float, optional 
            Minimum proportion of the patch under tissue to be kept. Defaults to 0. 

        Returns:
        --------
        str:
            The absolute file path to the saved HDF5 file containing the patch coordinates.

        Example:
        --------
        >>> coords_path = wsi.extract_tissue_coords(20, 256, "output_coords", overlap=32)
        >>> print(coords_path)
        output_coords/patches/sample_name_patches.h5
        """

        self._lazy_initialize()

        patcher = self.create_patcher(
            patch_size=patch_size,
            src_mag=self.mag,
            dst_mag=target_mag,
            mask=self.gdf_contours,
            coords_only=True,
            overlap=overlap,
            threshold=min_tissue_proportion,
        )

        coords_to_keep = [(x, y) for x, y in patcher]

        # Prepare assets for saving
        assets = {'coords' : np.array(coords_to_keep)}
        attributes = {
            'patch_size': patch_size, # Reference frame: patch_level
            'patch_size_level0': patch_size * self.mag // target_mag, # Reference frame: level0
            'level0_magnification': self.mag,
            'target_magnification': target_mag,
            'overlap': overlap,
            'name': self.name,
            'savetodir': save_coords
        }

        # Save the assets and attributes to an hdf5 file
        os.makedirs(os.path.join(save_coords, 'patches'), exist_ok=True)
        out_fname = os.path.join(save_coords, 'patches', str(self.name) + '_patches.h5')
        save_h5(out_fname,
                assets = assets,
                attributes = {'coords': attributes},
                mode='w')
        
        return out_fname

    def visualize_coords(self, coords_path: str, save_patch_viz: str) -> str:
        """
        The `visualize_coords` function of the class `WSI` overlays patch coordinates 
        onto a scaled thumbnail of the WSI. It creates a visualization of the extracted patches 
        and saves it as an image file.

        Args:
        -----
        coords_path : str
            Path to the file containing the patch coordinates.
        save_patch_viz : str
            Directory path to save the visualization image.

        Returns:
        --------
        str:
            The file path to the saved visualization image.

        Example:
        --------
        >>> viz_path = wsi.visualize_coords("output_coords/sample_name_patches.h5", "output_viz")
        >>> print(viz_path)
        output_viz/sample_name.png
        """

        self._lazy_initialize()

        try:
            coords_attrs, coords = read_coords(coords_path)  # Coords are ALWAYS wrt. level 0 of the slide.
            patch_size = coords_attrs.get('patch_size', None)
            level0_magnification = coords_attrs.get('level0_magnification', None)
            target_magnification = coords_attrs.get('target_magnification', None)
            overlap = coords_attrs.get('overlap', 'NA')
            
            if None in (patch_size, level0_magnification, target_magnification):
                raise KeyError('Missing essential attributes in coords_attrs.')
        except (KeyError, FileNotFoundError, ValueError) as e:
            warnings.warn(f"Cannot read using Trident coords format ({str(e)}). Trying with CLAM/Fishing-Rod.")
            patch_size, patch_level, custom_downsample, coords = read_coords_legacy(coords_path)
            level0_magnification = self.mag
            target_magnification = int(self.mag / (self.level_downsamples[patch_level] * custom_downsample))

        patcher = self.create_patcher(
            patch_size=patch_size,
            src_mag=level0_magnification,
            dst_mag=target_magnification,
            custom_coords=coords,
            coords_only=True
        )

        img =  patcher.visualize()

        # Save visualization
        os.makedirs(save_patch_viz, exist_ok=True)
        viz_coords_path = os.path.join(save_patch_viz, f'{self.name}.jpg')
        img.save(viz_coords_path)
        return viz_coords_path

    @torch.inference_mode()
    def extract_patch_features(
        self,
        patch_encoder: torch.nn.Module,
        coords_path: str,
        save_features: str,
        device: str = 'cuda:0',
        saveas: str = 'h5',
        batch_limit: int = 512
    ) -> str:
        """
        The `extract_patch_features` function of the class `WSI` extracts feature embeddings 
        from the WSI using a specified patch encoder. It processes the patches as specified 
        in the coordinates file and saves the features in the desired format.

        Args:
        -----
        patch_encoder : torch.nn.Module
            The model used for feature extraction.
        coords_path : str
            Path to the file containing patch coordinates.
        save_features : str
            Directory path to save the extracted features.
        device : str, optional
            Device to run feature extraction on (e.g., 'cuda:0'). Defaults to 'cuda:0'.
        saveas : str, optional
            Format to save the features ('h5' or 'pt'). Defaults to 'h5'.
        batch_limit : int, optional
            Maximum batch size for feature extraction. Defaults to 512.

        Returns:
        --------
        str:
            The absolute file path to the saved feature file in the specified format.

        Example:
        --------
        >>> features_path = wsi.extract_features(patch_encoder, "output_coords/sample_name_patches.h5", "output_features")
        >>> print(features_path)
        output_features/sample_name.h5
        """

        self._lazy_initialize()
        patch_encoder.to(device)
        patch_encoder.eval()
        precision = getattr(patch_encoder, 'precision', torch.float32)
        patch_transforms = patch_encoder.eval_transforms

        try:
            coords_attrs, coords = read_coords(coords_path)
            patch_size = coords_attrs.get('patch_size', None)
            level0_magnification = coords_attrs.get('level0_magnification', None)
            target_magnification = coords_attrs.get('target_magnification', None)            
            if None in (patch_size, level0_magnification, target_magnification):
                raise KeyError('Missing attributes in coords_attrs.')
        except (KeyError, FileNotFoundError, ValueError) as e:
            warnings.warn(f"Cannot read using Trident coords format ({str(e)}). Trying with CLAM/Fishing-Rod.")
            patch_size, patch_level, custom_downsample, coords = read_coords_legacy(coords_path)
            level0_magnification = self.mag
            target_magnification = int(self.mag / (self.level_downsamples[patch_level] * custom_downsample))

        patcher = self.create_patcher(
            patch_size=patch_size,
            src_mag=level0_magnification,
            dst_mag=target_magnification,
            custom_coords=coords,
            coords_only=False,
            pil=True,
        )
        dataset = WSIPatcherDataset(patcher, patch_transforms)
        dataloader = DataLoader(dataset, batch_size=batch_limit, num_workers=get_num_workers(batch_limit, max_workers=self.max_workers), pin_memory=True)
        # dataloader = DataLoader(dataset, batch_size=batch_limit, num_workers=0, pin_memory=True)

        features = []
        for imgs, _ in dataloader:
            imgs = imgs.to(device)
            with torch.autocast(device_type='cuda', dtype=precision, enabled=(precision != torch.float32)):
                batch_features = patch_encoder(imgs)  
            features.append(batch_features.cpu().numpy())

        # Concatenate features
        features = np.concatenate(features, axis=0)

        # Save the features to disk
        os.makedirs(save_features, exist_ok=True)
        if saveas == 'h5':
            save_h5(os.path.join(save_features, f'{self.name}.{saveas}'),
                    assets = {
                        'features' : features,
                        'coords': coords,
                    },
                    attributes = {
                        'features': {'name': self.name, 'savetodir': save_features},
                        'coords': coords_attrs
                    },
                    mode='w')
        elif saveas == 'pt':
            torch.save(features, os.path.join(save_features, f'{self.name}.{saveas}'))
        else:
            raise ValueError(f'Invalid save_features_as: {saveas}. Only "h5" and "pt" are supported.')

        return os.path.join(save_features, f'{self.name}.{saveas}')

    @torch.inference_mode()
    def extract_slide_features(
        self,
        patch_features_path: str,
        slide_encoder: torch.nn.Module,
        save_features: str,
        device: str = 'cuda',
    ) -> str:
        """
        Extract slide-level features by encoding patch-level features using a pretrained slide encoder.

        This function processes patch-level features extracted from a whole-slide image (WSI) and
        generates a single feature vector representing the entire slide. The extracted features are
        saved to a specified directory in HDF5 format.

        Args:
            patch_features_path (str): Path to the HDF5 file containing patch-level features and coordinates.
            slide_encoder (torch.nn.Module): Pretrained slide encoder model for generating slide-level features.
            save_features (str): Directory where the extracted slide features will be saved.
            device (str, optional): Device to run computations on (e.g., 'cuda', 'cpu'). Defaults to 'cuda'.

        Returns:
            str: The absolute path to the slide-level features.

        Workflow:
            1. Load the pretrained slide encoder model and set it to evaluation mode.
            2. Load patch-level features and corresponding coordinates from the provided HDF5 file.
            3. Convert patch-level features into a tensor and move it to the specified device.
            4. Generate slide-level features using the slide encoder, with automatic mixed precision if supported.
            5. Save the slide-level features and associated metadata (e.g., coordinates) in an HDF5 file.
            6. Return the path to the saved slide features.

        Notes:
            - The `patch_features_path` must point to a valid HDF5 file containing datasets named `features` and `coords`.
            - The saved HDF5 file includes both the slide-level features and metadata such as patch coordinates.
            - Automatic mixed precision is enabled if the slide encoder supports precision lower than `torch.float32`.

        Raises:
            FileNotFoundError: If the `patch_features_path` does not exist.
            RuntimeError: If there is an issue with the slide encoder or tensor operations.

        Example:
            >>> slide_features = extract_slide_features(
            ...     patch_features_path='path/to/patch_features.h5',
            ...     slide_encoder=pretrained_model,
            ...     save_features='output/slide_features',
            ...     device='cuda'
            ... )
            >>> print(slide_features.shape)  # Outputs the shape of the slide-level feature vector.
        """
        import h5py

        # Set the slide encoder model to device and eval
        slide_encoder.to(device)
        slide_encoder.eval()
        
        # Load patch-level features from h5 file
        with h5py.File(patch_features_path, 'r') as f:
            coords = f['coords'][:]
            patch_features = f['features'][:]
            coords_attrs = dict(f['coords'].attrs)

        # Convert slide_features to tensor
        patch_features = torch.from_numpy(patch_features).float().to(device)
        patch_features = patch_features.unsqueeze(0)  # Add batch dimension

        coords = torch.from_numpy(coords).to(device)
        coords = coords.unsqueeze(0)  # Add batch dimension

        # Prepare input batch dictionary
        batch = {
            'features': patch_features,
            'coords': coords,
            'attributes': coords_attrs
        }

        # Generate slide-level features
        with torch.autocast(device_type='cuda', enabled=(slide_encoder.precision != torch.float32)):
            features = slide_encoder(batch, device)
        features = features.float().cpu().numpy().squeeze()

        # Save slide-level features if save path is provided
        os.makedirs(save_features, exist_ok=True)
        save_path = os.path.join(save_features, f'{self.name}.h5')

        save_h5(os.path.join(save_features, f'{self.name}.h5'),
                    assets = {
                        'features' : features,
                        'coords': coords.cpu().numpy().squeeze(),
                    },
                    attributes = {
                        'features': {'name': self.name, 'savetodir': save_features},
                        'coords': coords_attrs
                    },
                    mode='w')

        return save_path

    def release(self) -> None:
        """
        Release internal data (CPU/GPU/memory) and clear heavy references in the WSI instance.
        Call this method after you're done processing to avoid memory/GPU leaks.
        """
        # Clear backend image object

        if hasattr(self, "close"):
            self.close()

        if hasattr(self, "img"):
            try:
                if hasattr(self.img, "close"):
                    self.img.close()
            except Exception:
                pass
            self.img = None

        # Clear segmentation results and coordinates
        for attr in ["gdf_contours", "tissue_seg_path"]:
            if hasattr(self, attr):
                setattr(self, attr, None)

        import gc
        import torch
        gc.collect()
        torch.cuda.empty_cache()



================================================
File: trident/wsi_objects/WSIFactory.py
================================================

import os
from typing import Optional, Literal, Union

from trident.wsi_objects.OpenSlideWSI import OpenSlideWSI
from trident.wsi_objects.ImageWSI import ImageWSI
from trident.wsi_objects.CuCIMWSI import CuCIMWSI

WSIReaderType = Literal['openslide', 'image', 'cucim']
OPENSLIDE_EXTENSIONS = {'.svs', '.tif', '.tiff', '.ndpi', '.vms', '.vmu', '.scn', '.mrxs'}
CUCIM_EXTENSIONS = {'.svs', '.tif', '.tiff'}

def load_wsi(
    slide_path: str,
    reader_type: Optional[WSIReaderType] = None,
    **kwargs
) -> Union[OpenSlideWSI, ImageWSI, CuCIMWSI]:
    """
    Load a whole-slide image (WSI) using the appropriate backend.

    By default, uses OpenSlideWSI for OpenSlide-supported file extensions,
    and ImageWSI for others. Users may override this behavior by explicitly
    specifying a reader using the `reader_type` argument.

    Parameters
    ----------
    slide_path : str
        Path to the whole-slide image.
    reader_type : {'openslide', 'image', 'cucim'}, optional
        Manually specify the WSI reader to use. If None (default), selection
        is automatic based on file extension.
    **kwargs : dict
        Additional keyword arguments passed to the WSI reader constructor.

    Returns
    -------
    Union[OpenSlideWSI, ImageWSI, CuCIMWSI]
        An instance of the appropriate WSI reader.

    Raises
    ------
    ValueError
        If `reader_type` is 'cucim' but the cucim package is not installed.
        Or if an unknown reader type is specified.
    """
    ext = os.path.splitext(slide_path)[1].lower()

    if reader_type == 'openslide':
        return OpenSlideWSI(slide_path=slide_path, **kwargs)

    elif reader_type == 'image':
        return ImageWSI(slide_path=slide_path, **kwargs)

    elif reader_type == 'cucim':
        if ext in CUCIM_EXTENSIONS:
            return CuCIMWSI(slide_path=slide_path, **kwargs)
        else:
            raise ValueError(
                f"Unsupported file format '{ext}' for CuCIM. "
                f"Supported whole-slide image formats are: {', '.join(CUCIM_EXTENSIONS)}."
            )

    elif reader_type is None:
        if ext in OPENSLIDE_EXTENSIONS:
            return OpenSlideWSI(slide_path=slide_path, **kwargs)
        else:
            return ImageWSI(slide_path=slide_path, **kwargs)

    else:
        raise ValueError(f"Unknown reader_type: {reader_type}. Choose from 'openslide', 'image', or 'cucim'.")



================================================
File: trident/wsi_objects/WSIPatcher.py
================================================
from __future__ import annotations

from typing import Tuple
import warnings
import cv2
import numpy as np
import geopandas as gpd
from shapely import Polygon
from PIL import Image

class WSIPatcher:
    """ Iterator class to handle patching, patch scaling and tissue mask intersection """
    
    def __init__(
        self, 
        wsi, 
        patch_size: int, 
        src_pixel_size: float = None,
        dst_pixel_size: float = None,
        src_mag: int = None,
        dst_mag: int = None,
        overlap: int = 0,
        mask: gpd.GeoDataFrame = None,
        coords_only = False,
        custom_coords = None,
        threshold = 0.,
        pil=False,
    ):
        """ Initialize patcher, compute number of (masked) rows, columns.

        Args:
            wsi (WSI): wsi to patch
            patch_size (int): patch width/height in pixel on the slide after rescaling
            src_pixel_size (float, optional): pixel size in um/px of the slide before rescaling. Defaults to None.
            dst_pixel_size (float, optional): pixel size in um/px of the slide after rescaling. Defaults to None.
	    src_mag (int, optional): level0 magnification of the slide before rescaling. Defaults to None.
            dst_mag (int, optional): target magnification of the slide after rescaling. Defaults to None.
            overlap (int, optional): Overlap between patches in pixels. Defaults to 0. 
            mask (gpd.GeoDataFrame, optional): geopandas dataframe of Polygons. Defaults to None.
            coords_only (bool, optional): whenever to extract only the coordinates insteaf of coordinates + tile. Default to False.
            threshold (float, optional): minimum proportion of the patch under tissue to be kept.
                This argument is ignored if mask=None, passing threshold=0 will be faster. Defaults to 0.15
            pil (bool, optional): whenever to get patches as `PIL.Image` (numpy array by default). Defaults to False
        """
        self.wsi = wsi
        self.overlap = overlap
        self.width, self.height = self.wsi.get_dimensions()
        self.patch_size_target = patch_size
        self.mask = mask
        self.i = 0
        self.coords_only = coords_only
        self.custom_coords = custom_coords
        self.pil = pil
        self.dst_mag = dst_mag
        
        # set src magnification and pixel size. 
        if src_pixel_size is not None:
            self.src_pixel_size = src_pixel_size
        else:
            self.src_pixel_size = 10 / src_mag

        if dst_pixel_size is not None:
            self.dst_pixel_size = dst_pixel_size
        else:
            self.dst_pixel_size = 10 / dst_mag

        self.downsample = self.dst_pixel_size / self.src_pixel_size
        self.patch_size_src = round(patch_size * self.downsample)
        self.overlap_src = round(overlap * self.downsample)
        
        self.level, self.patch_size_level, self.overlap_level = self._prepare()  
        
        if custom_coords is None: 
            self.cols, self.rows = self._compute_cols_rows()
            
            col_rows = np.array([
                [col, row] 
                for col in range(self.cols) 
                for row in range(self.rows)
            ])
            coords = np.array([self._colrow_to_xy(xy[0], xy[1]) for xy in col_rows])
        else:
            if round(custom_coords[0][0]) != custom_coords[0][0]:
                raise ValueError("custom_coords must be a (N, 2) array of int")
            coords = custom_coords
        if self.mask is not None:
            self.valid_patches_nb, self.valid_coords = self._compute_masked(coords, threshold)
        else:
            self.valid_patches_nb, self.valid_coords = len(coords), coords
            
    def _colrow_to_xy(self, col, row):
        """ Convert col row of a tile to its top-left coordinates before rescaling (x, y) """
        x = col * (self.patch_size_src) - self.overlap_src * np.clip(col - 1, 0, None)
        y = row * (self.patch_size_src) - self.overlap_src * np.clip(row - 1, 0, None)
        return (x, y)   
            
    def _xy_to_colrow(self, x, y):
        """Convert x, y coordinates to col, row indices."""
        if x == 0:
            col = 0
        else:
            col = ((x - self.patch_size_src) // (self.patch_size_src - self.overlap_src)) + 1
        
        if y == 0:
            row = 0
        else:
            row = ((y - self.patch_size_src) // (self.patch_size_src - self.overlap_src)) + 1
        
        return col, row

    def _compute_masked(self, coords, threshold, simplify_shape=True) -> None:
        """ Compute tiles which overlap with > threshold with the tissue """
        
		# Filter coordinates by bounding boxes of mask polygons
        if simplify_shape:
            mask = self.mask.simplify(tolerance=self.patch_size_target / 4, preserve_topology=True)
        else:
            mask = self.mask
        bounding_boxes = mask.geometry.bounds
        bbox_masks = []
        for _, bbox in bounding_boxes.iterrows():
            bbox_mask = (
                (coords[:, 0] >= bbox['minx'] - self.patch_size_src) & (coords[:, 0] <= bbox['maxx'] + self.patch_size_src) & 
                (coords[:, 1] >= bbox['miny'] - self.patch_size_src) & (coords[:, 1] <= bbox['maxy'] + self.patch_size_src)
            )
            bbox_masks.append(bbox_mask)

        if len(bbox_masks) > 0:
            bbox_mask = np.vstack(bbox_masks).any(axis=0)
        else:
            bbox_mask = np.zeros(len(coords), dtype=bool)
            
        
        union_mask = mask.union_all()

        squares = [
            Polygon([
                (xy[0], xy[1]), 
                (xy[0] + self.patch_size_src, xy[1]), 
                (xy[0] + self.patch_size_src, xy[1] + self.patch_size_src), 
                (xy[0], xy[1] + self.patch_size_src)]) 
            for xy in coords[bbox_mask]
        ]
        if threshold == 0:
            valid_mask = gpd.GeoSeries(squares).intersects(union_mask).values
        else:
            gdf = gpd.GeoSeries(squares)
            areas = gdf.area
            valid_mask = gdf.intersection(union_mask).area >= threshold * areas
            
        full_mask = bbox_mask
        full_mask[bbox_mask] &= valid_mask 

        valid_patches_nb = full_mask.sum()
        self.valid_mask = full_mask
        valid_coords = coords[full_mask]
        return valid_patches_nb, valid_coords
        
    def __len__(self):
        return self.valid_patches_nb
    
    def __iter__(self):
        self.i = 0
        return self
    
    def __next__(self):
        if self.i >= self.valid_patches_nb:
            raise StopIteration
        x = self.__getitem__(self.i)
        self.i += 1
        return x
    
    def __getitem__(self, index):
        if 0 <= index < len(self):
            xy = self.valid_coords[index]
            x, y = xy[0], xy[1]
            if self.coords_only:
                return x, y
            tile, x, y = self.get_tile_xy(x, y)
            return tile, x, y
        else:
            raise IndexError("Index out of range")
        
    def _prepare(self) -> None:
        level, _ = self.wsi.get_best_level_and_custom_downsample(self.downsample, tolerance=0.1)
        level_downsample = int(self.wsi.level_downsamples[level])
        patch_size_level = round(self.patch_size_src / level_downsample)
        overlap_level = round(self.overlap_src / level_downsample)
        return level, patch_size_level, overlap_level
    
    def get_cols_rows(self) -> Tuple[int, int]:
        """ Get the number of columns and rows in the associated WSI

        Returns:
            Tuple[int, int]: (nb_columns, nb_rows)
        """
        return self.cols, self.rows
      
    def get_tile_xy(self, x: int, y: int) -> Tuple[np.ndarray, int, int]:

        tile = self.wsi.read_region(
            location=(x, y),
            level=self.level,
            size=(self.patch_size_level, self.patch_size_level),
            read_as='pil' if self.pil else 'numpy'
        )

        if self.patch_size_target is not None:
            if self.pil:
                tile = tile.resize((self.patch_size_target, self.patch_size_target))
            else:
                tile = cv2.resize(tile, (self.patch_size_target, self.patch_size_target))[:, :, :3]

        assert x < self.width and y < self.height
        return tile, x, y
    
    def get_tile(self, col: int, row: int) -> Tuple[np.ndarray, int, int]:
        """ get tile at position (column, row)

        Args:
            col (int): column
            row (int): row

        Returns:
            Tuple[np.ndarray, int, int]: (tile, pixel x of top-left corner (before rescaling), pixel_y of top-left corner (before rescaling))
        """
        if self.custom_coords is not None:
            raise ValueError("Can't use get_tile as 'custom_coords' was passed to the constructor")
            
        x, y = self._colrow_to_xy(col, row)
        return self.get_tile_xy(x, y)
    
    def _compute_cols_rows(self) -> Tuple[int, int]:
        col = 0
        row = 0
        x, y = self._colrow_to_xy(col, row)
        while x < self.width:
            col += 1
            x, _ = self._colrow_to_xy(col, row)
        cols = col
        while y < self.height:
            row += 1
            _, y = self._colrow_to_xy(col, row)
        rows = row
        return cols, rows 
    

    def visualize(self) -> Image.Image:
        """ 
        The `visualize` function of the class `WSI` overlays patch coordinates computed by the WSIPatcher
        onto a scaled thumbnail of the WSI. It creates a visualization of the patcher coordinates 
        and returns it as an image.

        Returns
        -------
        Image.Image
            Patch visualization

        Example:
        --------
        >>> img = wsi_patcher.visualize()
        >>> img.save('test_vis.jpg')
        """
        max_dimension = 1000
        if self.width > self.height:
            thumbnail_width = max_dimension
            thumbnail_height = int(thumbnail_width * self.height / self.width)
        else:
            thumbnail_height = max_dimension
            thumbnail_width = int(thumbnail_height * self.width / self.height)

        downsample_factor = self.width / thumbnail_width

        thumbnail_patch_size = max(1, int(self.patch_size_src / downsample_factor))

        # Get thumbnail in right format
        canvas = np.array(self.wsi.get_thumbnail((thumbnail_width, thumbnail_height))).astype(np.uint8)

        tmp_coords = self.coords_only
        self.coords_only = True
        # Draw rectangles for patches
        for (x, y) in self:
            x, y = int(x/downsample_factor), int(y/downsample_factor)
            thickness = max(1, thumbnail_patch_size // 10)
            canvas = cv2.rectangle(
                canvas, 
                (x, y), 
                (x + thumbnail_patch_size, y + thumbnail_patch_size), 
                (255, 0, 0), 
                thickness
            )

        self.coords_only = tmp_coords

        # Add annotations
        text_area_height = 130
        text_x_offset = int(thumbnail_width * 0.03)  # Offset as 3% of thumbnail width
        text_y_spacing = 25  # Vertical spacing between lines of text

        canvas[:text_area_height, :300] = (
            canvas[:text_area_height, :300] * 0.5
        ).astype(np.uint8)

        cv2.putText(canvas, f'{len(self)} patches', (text_x_offset, text_y_spacing), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 1)
        cv2.putText(canvas, f'width={self.width}, height={self.height}', (text_x_offset, text_y_spacing * 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        cv2.putText(canvas, f'mpp={self.wsi.mpp}, mag={self.wsi.mag}', (text_x_offset, text_y_spacing * 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(canvas, f'patch={self.patch_size_target} w. overlap={self.overlap} @ {self.dst_mag}x', (text_x_offset, text_y_spacing * 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        return Image.fromarray(canvas)
    
class OpenSlideWSIPatcher(WSIPatcher):
    def __init__(self, *args, **kwargs):
        warnings.warn(
            "OpenSlideWSIPatcher is deprecated and will be removed in a future release. "
            "Please use WSIPatcher instead.",
            category=DeprecationWarning,
            stacklevel=2
        )
        super().__init__(*args, **kwargs)


================================================
File: trident/wsi_objects/WSIPatcherDataset.py
================================================
from torch.utils.data import Dataset


class WSIPatcherDataset(Dataset):
    """ Dataset from a WSI patcher to directly read tiles on a slide  """
    
    def __init__(self, patcher, transform):
        self.patcher = patcher
        self.transform = transform
                              
    def __len__(self):
        return len(self.patcher)
    
    def __getitem__(self, index):
        tile, x, y = self.patcher[index]

        if self.transform:
            tile = self.transform(tile)

        return tile, (x, y)



================================================
File: trident/wsi_objects/__init__.py
================================================



================================================
File: tutorials/1-Step-by-Step-Patch-Feature-Extraction-with-Trident.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
### Welcome to Trident: A Python Package for Whole-Slide Image Processing 


This tutorial will guide you step-by-step to process a single whole-slide image (WSI) using Trident:

- Tissue vs. background segmentation 
- Tissue coordinate extraction
- Tissue feature extraction

"""

"""
#### 0- Installation 


```
conda create -n "trident" python=3.10
conda activate trident
git clone git@github.com:mahmoodlab/trident.git && cd trident
pip install . -e

```

Please refer to the FAQ if you face installation issues.
"""

"""
#### 1- Tissue vs background segmentation
"""

import os 
import torch 
from PIL import Image
import geopandas as gpd
from IPython.display import display
from huggingface_hub import snapshot_download

from trident import OpenSlideWSI
from trident.segmentation_models import segmentation_model_factory

# a. Download a WSI
OUTPUT_DIR = "tutorial-1/"
DEVICE = f"cuda:0" if torch.cuda.is_available() else "cpu"
WSI_FNAME = '394140.svs'
os.makedirs(OUTPUT_DIR, exist_ok=True)
local_wsi_dir = snapshot_download(
    repo_id="MahmoodLab/unit-testing",
    repo_type='dataset',
    local_dir=os.path.join(OUTPUT_DIR, 'wsis'),
    allow_patterns=[WSI_FNAME]
)

# b. Create OpenSlideWSI
wsi_path = os.path.join(local_wsi_dir, WSI_FNAME)
slide = OpenSlideWSI(slide_path=wsi_path, lazy_init=False)

# c. Run segmentation 
segmentation_model = segmentation_model_factory("hest")
geojson_contours = slide.segment_tissue(segmentation_model=segmentation_model, target_mag=10, job_dir=OUTPUT_DIR, device=DEVICE)

# d. Visualize contours
contour_image = Image.open(os.path.join(OUTPUT_DIR, 'contours', WSI_FNAME.replace('.svs', '.jpg')))
display(contour_image)

# e. Check contours saved into GeoJSON with GeoPandas
gdf = gpd.read_file(geojson_contours)
gdf.head(n=10)


"""
#### 2- Tissue coordinate extraction

We are patching the whole-slide image into non-overlapping patches of size 256x256 at 20x magnification (0.5 um/px).
"""

import h5py 

TARGET_MAG = 20
PATCH_SIZE = 256

# a. Run patch coordinate extraction
coords_path = slide.extract_tissue_coords(
    target_mag=TARGET_MAG,
    patch_size=PATCH_SIZE,
    save_coords=OUTPUT_DIR
)

# b. Visualize
viz_coords_path = slide.visualize_coords(
    coords_path=coords_path,
    save_patch_viz=os.path.join(OUTPUT_DIR, "visualization")
)
display(Image.open(viz_coords_path))

# c. Inspect h5 with patch coordinates 
def print_attrs(name, obj):
    print(f"Object: {name}")
    for key, value in obj.attrs.items():
        print(f"  Attribute - {key}: {value}")

with h5py.File(coords_path, 'r') as h5_file:
    print("Contents and Attributes in patch coords file:")
    h5_file.visititems(print_attrs)

"""
#### 3- Patch feature extraction with the UNI model

"""

from trident.patch_encoder_models import encoder_factory

PATCH_ENCODER = "uni_v1" # Visit the factory or check the README for a list of all available models

# a. Instantiate UNI model using the factory 
encoder = encoder_factory(PATCH_ENCODER)
encoder.eval()
encoder.to(DEVICE)

# b. Run UNI feature extraction
features_dir = os.path.join(OUTPUT_DIR, f"features_{PATCH_ENCODER}")
feats_path = slide.extract_patch_features(
    patch_encoder=encoder,
    coords_path=coords_path,
    save_features=features_dir,
    device=DEVICE
)

# c. Inspect h5 with patch features 
with h5py.File(feats_path, 'r') as h5_file:
    print("Contents and Attributes in feats file:")
    h5_file.visititems(print_attrs)




================================================
File: tutorials/2-Using-Trident-With-Your-Custom-Patch-Encoder.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
## Using Trident with Your Own Foundation Model 

As more and more groups design their own foundation model, we want to offer easy tools for custom integration. This is the idea of the `CustomInferenceEncoder` from the `patch_encoder_models` module. 
"""

from PIL import Image
import requests
import torch
import timm
import torchvision.transforms as transforms

from trident.patch_encoder_models import CustomInferenceEncoder

# Load your custom model (eg ViT pretrained on ImageNet)
model = timm.create_model('eva02_large_patch14_448.mim_m38m_ft_in22k_in1k', pretrained=True)
model = model.eval()
model.head = torch.nn.Identity()  

# Set precision
precision = torch.float16

# Set transforms
data_config = timm.data.resolve_model_data_config(model)
eval_transforms = timm.data.create_transform(**data_config, is_training=False)

# Create custom encoder
custom_patch_encoder = CustomInferenceEncoder(
    enc_name='my_custom_model',
    model=model,
    transforms=eval_transforms,
    precision=precision
)


# Integrate the above model into Trident "regular" pipeline, e.g., using the Processor
import os
import torch
from huggingface_hub import snapshot_download

from trident.Processor import Processor
from trident.segmentation_models import segmentation_model_factory

OUTPUT_DIR = "tutorial-2/"
DEVICE = f"cuda:0" if torch.cuda.is_available() else "cpu"
WSI_FNAME = '394140.svs'
os.makedirs(OUTPUT_DIR, exist_ok=True)
local_wsi_dir = snapshot_download(
    repo_id="MahmoodLab/unit-testing",
    repo_type='dataset',
    local_dir=os.path.join(OUTPUT_DIR, 'wsis'),
    allow_patterns=[WSI_FNAME]
)

# Create processor
processor = Processor(
    job_dir=OUTPUT_DIR,       # Directory to store outputs
    wsi_source=local_wsi_dir, # Directory containing WSI files
)

# Run tissue vs background segmentation
segmentation_model = segmentation_model_factory('hest')
processor.run_segmentation_job(
    segmentation_model,
    device=DEVICE
)

# Run tissue coordinate extraction (256x256 at 20x)
processor.run_patching_job(
    target_magnification=20,
    patch_size=256,
    overlap=0
)

# Run patch feature extraction using the custom encoder
processor.run_patch_feature_extraction_job(
    coords_dir=f'20x_256px_0px_overlap', # Make sure to change this if you changed the patching parameters
    patch_encoder=custom_patch_encoder,
    device=DEVICE,
    saveas='h5',
    batch_limit=32
)





================================================
File: tutorials/3-Training-a-WSI-Classification-Model-with-ABMIL-and-Heatmaps.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
### Tutorial 3: Training a WSI Classification Model with ABMIL

This tutorial will guide you step-by-step to train an attention-based multiple instance learning model using Trident patch embeddings. Then, we will generation attention heatmaps using the pretrained model. 

"""

"""
#### A- Installation and patch feature extraction using UNI

#### Step 1: Download a dataset of whole-slide images

You can use your own WSIs or download a publicly available dataset, e.g. from:

- **CPTAC CCRCC WSIs**: Download from the [TCIA Cancer Imaging Archive](https://www.cancerimagingarchive.net/collection/cptac-ccrcc/).
- **Store WSIs**: Save all WSIs into a local directory, e.g.,  
  ```bash
  ./CPTAC-CCRCC_v1/CCRCC
  ```

#### Step 2:  Run CONCH v1.5 feature extraction:

Navigate to the base directory of Trident and execute the following command:

```bash
python run_batch_of_slides.py --task all \
  --wsi_dir ./CPTAC-CCRCC_v1/CCRCC \
  --job_dir ./tutorial-3 \
  --patch_encoder conch_v15 \
  --mag 20 \
  --patch_size 512
```

"""

"""
#### B- Download labels with data splits

Here, we use Patho-Bench CPTAC-CCRCC labels for predicting BAP1 mutation. 
"""

import datasets
import pandas as pd

# Download labels as csv
datasets.load_dataset(
    'MahmoodLab/Patho-Bench', 
    cache_dir='./tutorial-3',
    dataset_to_download='cptac_ccrcc',     
    task_in_dataset='BAP1_mutation',           
    trust_remote_code=True
)

# Visualize my labels and splits
df = pd.read_csv('tutorial-3/cptac_ccrcc/BAP1_mutation/k=all.tsv', sep="\t")
df

# Check the label distribution
df_counts = df['BAP1_mutation'].value_counts().reset_index()
df_counts.columns = ['BAP1_mutation', 'Count']
df_counts


"""
#### C- Training an ABMIL model
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import h5py
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score

from trident.slide_encoder_models import ABMILSlideEncoder

# Set deterministic behavior
SEED = 1234
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


class BinaryClassificationModel(nn.Module):
    def __init__(self, input_feature_dim=768, n_heads=1, head_dim=512, dropout=0., gated=True, hidden_dim=256):
        super().__init__()
        self.feature_encoder = ABMILSlideEncoder(
            input_feature_dim=input_feature_dim, 
            n_heads=n_heads, 
            head_dim=head_dim, 
            dropout=dropout, 
            gated=gated
        )
        self.classifier = nn.Sequential(
            nn.Linear(input_feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x, return_raw_attention=False):
        if return_raw_attention:
            features, attn = self.feature_encoder(x, return_raw_attention=True)
        else:
            features = self.feature_encoder(x)
        logits = self.classifier(features).squeeze(1)
        
        if return_raw_attention:
            return logits, attn
        
        return logits

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BinaryClassificationModel().to(device)

# Custom dataset
class H5Dataset(Dataset):
    def __init__(self, feats_path, df, split, num_features=512):
        self.df = df[df["fold_0"] == split]
        self.feats_path = feats_path
        self.num_features = num_features
        self.split = split
    
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        with h5py.File(os.path.join(self.feats_path, row['slide_id'] + '.h5'), "r") as f:
            features = torch.from_numpy(f["features"][:])

        if self.split == 'train':
            num_available = features.shape[0]
            if num_available >= self.num_features:
                indices = torch.randperm(num_available, generator=torch.Generator().manual_seed(SEED))[:self.num_features]
            else:
                indices = torch.randint(num_available, (self.num_features,), generator=torch.Generator().manual_seed(SEED))  # Oversampling
            features = features[indices]

        label = torch.tensor(row["BAP1_mutation"], dtype=torch.float32)
        return features, label

# Create dataloaders
feats_path = './tutorial-3/cptac_ccrcc/20x_512px_0px_overlap/features_conch_v15'
batch_size = 8
train_loader = DataLoader(H5Dataset(feats_path, df, "train"), batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(SEED))
test_loader = DataLoader(H5Dataset(feats_path, df, "test"), batch_size=1, shuffle=False, worker_init_fn=lambda _: np.random.seed(SEED))

# Training setup
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=4e-4)

# Training loop
num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    total_loss = 0.
    for features, labels in train_loader:
        features, labels = {'features': features.to(device)}, labels.to(device)
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}")


"""
#### D- Evaluating the ABMIL model
"""

# Evaluation
model.eval()
all_labels, all_outputs = [], []
correct = 0
total = 0

with torch.no_grad():
    for features, labels in test_loader:
        features, labels = {'features': features.to(device)}, labels.to(device)
        outputs = model(features)
        
        # Convert logits to probabilities and binary predictions
        predicted = (outputs > 0).float()  # Since BCEWithLogitsLoss expects raw logits
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

        all_outputs.append(outputs.cpu().numpy())  
        all_labels.append(labels.cpu().numpy())

# Compute AUC
all_outputs = np.concatenate(all_outputs)
all_labels = np.concatenate(all_labels)
auc = roc_auc_score(all_labels, all_outputs)

# Compute accuracy
accuracy = correct / total
print(f"Test AUC: {auc:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")


"""
#### E- Extract attention heatmap for the freshly trained model
"""

from trident import OpenSlideWSI, visualize_heatmap
from trident.segmentation_models import segmentation_model_factory
from trident.patch_encoder_models import encoder_factory as patch_encoder_factory

# a. Load WSI to process
job_dir = './tutorial-3/heatmap_viz'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
slide = OpenSlideWSI(slide_path='./CPTAC-CCRCC_v1/CCRCC/C3L-00418-22.svs', lazy_init=False)

# b. Run segmentation 
segmentation_model = segmentation_model_factory("hest")
geojson_contours = slide.segment_tissue(segmentation_model=segmentation_model, job_dir=job_dir, device=device)

# c. Run patch coordinate extraction
coords_path = slide.extract_tissue_coords(
    target_mag=20,
    patch_size=512,
    save_coords=job_dir,
    overlap=256, 
)

# d. Run patch feature extraction
patch_encoder = patch_encoder_factory("conch_v15").eval().to(device)
patch_features_path = slide.extract_patch_features(
    patch_encoder=patch_encoder,
    coords_path=coords_path,
    save_features=os.path.join(job_dir, f"features_conch_v15"),
    device=device
)

#  e. Run inference 
with h5py.File(patch_features_path, 'r') as f:
    coords = f['coords'][:]
    patch_features = f['features'][:]
    coords_attrs = dict(f['coords'].attrs)

batch = {'features': torch.from_numpy(patch_features).float().to(device).unsqueeze(0)}
logits, attention = model(batch, return_raw_attention=True)

# f. generate heatmap
heatmap_save_path = visualize_heatmap(
    wsi=slide,
    scores=attention.cpu().numpy().squeeze(),  
    coords=coords,
    vis_level=1,
    patch_size_level0=coords_attrs['patch_size_level0'],
    normalize=True,
    num_top_patches_to_save=10,
    output_dir=job_dir
)





================================================
File: .github/ISSUE_TEMPLATE/bug_report.yml
================================================
name: Bug Report
description: Report something that isn't working as expected
title: "[Bug] "
labels: ["bug"]
body:
  - type: textarea
    id: what-happened
    attributes:
      label: What happened?
      description: Describe the issue and what you expected instead.
      placeholder: "The `--remove_artifacts` flag fails when I run on XYZ dataset..."
    validations:
      required: true

  - type: textarea
    id: how-to-reproduce
    attributes:
      label: Steps to Reproduce
      description: What commands did you run? What were the inputs?
      placeholder: |
        1. Installed Trident on workstation with 1 RTX 4090
        2. Ran `python run_single_slide.py --slide_path ...`
        3. Got error blablabla
    validations:
      required: false

  - type: input
    id: trident-version
    attributes:
      label: Trident version
      description: Commit hash or version (e.g., main branch or commit ID)
    validations:
      required: false

  - type: textarea
    id: logs
    attributes:
      label: Relevant logs or error messages
      description: Paste any error message or logs
      render: shell



================================================
File: .github/ISSUE_TEMPLATE/feature_request.yml
================================================
name: Feature Request
description: Suggest a new feature or improvement
title: "[Feature] "
labels: ["enhancement"]
body:
  - type: textarea
    id: motivation
    attributes:
      label: Motivation
      description: What problem are you trying to solve or improve?
      placeholder: "I want to extract features from JPEGs directly without using pyramidal images..."
    validations:
      required: true

  - type: textarea
    id: proposal
    attributes:
      label: Proposed Solution
      description: How do you imagine this could be solved or implemented?
      placeholder: "Add a JPEG loader class and map it in `wsi_reader_factory`..."
    validations:
      required: false

  - type: dropdown
    id: impact
    attributes:
      label: Who does this help?
      options:
        - Just me
        - Some users
        - Most Trident users



================================================
File: .github/ISSUE_TEMPLATE/question.yml
================================================
name: Question
description: Ask a general usage or implementation question
title: "[Question] "
labels: ["question"]
body:
  - type: textarea
    id: question
    attributes:
      label: What's your question?
      description: Ask anything related to usage, models, parameters, etc.
      placeholder: "Can I use `grandqc` segmenter on IHC-stained slides?"
    validations:
      required: true



================================================
File: .github/workflows/ci.yml
================================================
name: trident tests

on:
  pull_request:
    branches: [ "main" ]

permissions:
  contents: read

jobs:
  build:

    runs-on: ubuntu-latest
    env:
      HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
     
    - name: Install python dependencies
      run: |
        python -m pip install -e .
    - name: Install apt dependencies
      run: |
        sudo apt-get update
        sudo apt-get install libvips libvips-dev openslide-tools

    - name: Run tests
      run: |
        python tests/test_segmentation_models.py

